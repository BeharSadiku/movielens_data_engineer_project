[0m19:54:59.502918 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m19:54:59.516207 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m19:54:59.518699 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m19:54:59.519612 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m19:54:59.520156 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



  
  
      
    ) dbt_internal_test
[0m19:54:59.520784 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:00.857145 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c193-152a-8b60-b9c78af42cf2) - Created
[0m19:55:01.033197 [debug] [Thread-1 (]: SQL status: OK in 1.510 seconds
[0m19:55:01.035179 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c193-152a-8b60-b9c78af42cf2, command-id=01f09a38-c1a1-1c7e-a3b0-e8eb9c18eb6c) - Closing
[0m19:55:01.035828 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m19:55:01.036290 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c193-152a-8b60-b9c78af42cf2) - Closing
[0m19:55:01.147782 [info ] [Thread-1 (]: 24 of 55 PASS not_null_silver_links_imdb_id .................................... [[32mPASS[0m in 1.65s]
[0m19:55:01.148702 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m19:55:01.149234 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m19:55:01.149745 [info ] [Thread-1 (]: 25 of 55 START test not_null_silver_links_movie_id ............................. [RUN]
[0m19:55:01.150400 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d) - Creating connection
[0m19:55:01.150754 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d'
[0m19:55:01.151079 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m19:55:01.157023 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m19:55:01.158278 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m19:55:01.160918 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m19:55:01.161942 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m19:55:01.162467 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



  
  
      
    ) dbt_internal_test
[0m19:55:01.162856 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:01.571706 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c200-1f8f-bdc2-b7cad402592f) - Created
[0m19:55:01.750276 [debug] [Thread-1 (]: SQL status: OK in 0.590 seconds
[0m19:55:01.752731 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c200-1f8f-bdc2-b7cad402592f, command-id=01f09a38-c20f-1115-857d-3189e6ffc6bf) - Closing
[0m19:55:01.753387 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m19:55:01.753733 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c200-1f8f-bdc2-b7cad402592f) - Closing
[0m19:55:02.155177 [info ] [Thread-1 (]: 25 of 55 PASS not_null_silver_links_movie_id ................................... [[32mPASS[0m in 1.00s]
[0m19:55:02.156000 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m19:55:02.156357 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m19:55:02.156741 [info ] [Thread-1 (]: 26 of 55 START test not_null_silver_links_tmdb_id .............................. [RUN]
[0m19:55:02.157491 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f) - Creating connection
[0m19:55:02.157828 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f'
[0m19:55:02.158120 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m19:55:02.161835 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m19:55:02.162935 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m19:55:02.170805 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m19:55:02.171882 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m19:55:02.172411 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



  
  
      
    ) dbt_internal_test
[0m19:55:02.172752 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:02.510766 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c28f-1a07-8a97-1df395be2d4d) - Created
[0m19:55:02.693829 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m19:55:02.695743 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c28f-1a07-8a97-1df395be2d4d, command-id=01f09a38-c29e-1551-b1ab-049509b60d2b) - Closing
[0m19:55:02.696428 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m19:55:02.696775 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c28f-1a07-8a97-1df395be2d4d) - Closing
[0m19:55:02.815654 [info ] [Thread-1 (]: 26 of 55 PASS not_null_silver_links_tmdb_id .................................... [[32mPASS[0m in 0.66s]
[0m19:55:02.816885 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m19:55:02.817434 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m19:55:02.818342 [info ] [Thread-1 (]: 27 of 55 START test not_null_silver_movies_genres .............................. [RUN]
[0m19:55:02.819175 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63) - Creating connection
[0m19:55:02.819551 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63'
[0m19:55:02.819922 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m19:55:02.824743 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m19:55:02.826093 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m19:55:02.830455 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m19:55:02.831870 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m19:55:02.832459 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



  
  
      
    ) dbt_internal_test
[0m19:55:02.832820 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:03.624723 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c339-19ca-ac22-e3b0d39bb1f7) - Created
[0m19:55:03.808666 [debug] [Thread-1 (]: SQL status: OK in 0.980 seconds
[0m19:55:03.810733 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c339-19ca-ac22-e3b0d39bb1f7, command-id=01f09a38-c348-1474-b1c1-9135b6e842ee) - Closing
[0m19:55:03.811612 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m19:55:03.811973 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c339-19ca-ac22-e3b0d39bb1f7) - Closing
[0m19:55:03.930206 [info ] [Thread-1 (]: 27 of 55 PASS not_null_silver_movies_genres .................................... [[32mPASS[0m in 1.11s]
[0m19:55:03.931020 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m19:55:03.931450 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m19:55:03.931923 [info ] [Thread-1 (]: 28 of 55 START test not_null_silver_movies_movie_id ............................ [RUN]
[0m19:55:03.932564 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e) - Creating connection
[0m19:55:03.932945 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e'
[0m19:55:03.933235 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m19:55:03.936848 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m19:55:03.937932 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m19:55:03.941607 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m19:55:03.942510 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m19:55:03.943011 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



  
  
      
    ) dbt_internal_test
[0m19:55:03.943340 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:04.274497 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c39d-1627-ae93-0facea665f95) - Created
[0m19:55:04.836866 [debug] [Thread-1 (]: SQL status: OK in 0.890 seconds
[0m19:55:04.838766 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c39d-1627-ae93-0facea665f95, command-id=01f09a38-c3ab-11c2-b4c8-d09e91915fc9) - Closing
[0m19:55:04.839382 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m19:55:04.839700 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c39d-1627-ae93-0facea665f95) - Closing
[0m19:55:04.949747 [info ] [Thread-1 (]: 28 of 55 PASS not_null_silver_movies_movie_id .................................. [[32mPASS[0m in 1.02s]
[0m19:55:04.950618 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m19:55:04.950975 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m19:55:04.951357 [info ] [Thread-1 (]: 29 of 55 START test not_null_silver_movies_title ............................... [RUN]
[0m19:55:04.951918 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_movies_title.3a779bd2de) - Creating connection
[0m19:55:04.952251 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_movies_title.3a779bd2de'
[0m19:55:04.952591 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m19:55:04.956476 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m19:55:04.957609 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m19:55:04.961569 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m19:55:04.962526 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m19:55:04.963084 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



  
  
      
    ) dbt_internal_test
[0m19:55:04.963408 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:05.571607 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c439-1109-92c2-0ac353f4d17f) - Created
[0m19:55:05.756041 [debug] [Thread-1 (]: SQL status: OK in 0.790 seconds
[0m19:55:05.758687 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c439-1109-92c2-0ac353f4d17f, command-id=01f09a38-c470-1f54-9dba-ed05725db5c4) - Closing
[0m19:55:05.759566 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m19:55:05.760167 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c439-1109-92c2-0ac353f4d17f) - Closing
[0m19:55:05.880698 [info ] [Thread-1 (]: 29 of 55 PASS not_null_silver_movies_title ..................................... [[32mPASS[0m in 0.93s]
[0m19:55:05.881502 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m19:55:05.881884 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m19:55:05.882280 [info ] [Thread-1 (]: 30 of 55 START test not_null_silver_ratings_movie_id ........................... [RUN]
[0m19:55:05.882865 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36) - Creating connection
[0m19:55:05.883168 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36'
[0m19:55:05.883454 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m19:55:05.888792 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m19:55:05.889875 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m19:55:05.892068 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m19:55:05.892876 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m19:55:05.893382 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



  
  
      
    ) dbt_internal_test
[0m19:55:05.893701 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:06.263419 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c4cc-1c19-8532-07540d6f1eb5) - Created
[0m19:55:06.464673 [debug] [Thread-1 (]: SQL status: OK in 0.570 seconds
[0m19:55:06.466785 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c4cc-1c19-8532-07540d6f1eb5, command-id=01f09a38-c4da-19c6-80a0-f990266726d7) - Closing
[0m19:55:06.467451 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m19:55:06.467801 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c4cc-1c19-8532-07540d6f1eb5) - Closing
[0m19:55:06.576447 [info ] [Thread-1 (]: 30 of 55 PASS not_null_silver_ratings_movie_id ................................. [[32mPASS[0m in 0.69s]
[0m19:55:06.577251 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m19:55:06.577620 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m19:55:06.578012 [info ] [Thread-1 (]: 31 of 55 START test not_null_silver_ratings_rating ............................. [RUN]
[0m19:55:06.578548 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a) - Creating connection
[0m19:55:06.578849 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a'
[0m19:55:06.579147 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m19:55:06.584583 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m19:55:06.585622 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m19:55:06.587814 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m19:55:06.588657 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m19:55:06.589149 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



  
  
      
    ) dbt_internal_test
[0m19:55:06.589480 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:06.918680 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c531-13b1-bdac-c1c99f058683) - Created
[0m19:55:07.503444 [debug] [Thread-1 (]: SQL status: OK in 0.910 seconds
[0m19:55:07.505639 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c531-13b1-bdac-c1c99f058683, command-id=01f09a38-c57b-19f1-b263-6dd64f9c2252) - Closing
[0m19:55:07.506333 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m19:55:07.506807 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c531-13b1-bdac-c1c99f058683) - Closing
[0m19:55:07.622225 [info ] [Thread-1 (]: 31 of 55 PASS not_null_silver_ratings_rating ................................... [[32mPASS[0m in 1.04s]
[0m19:55:07.622993 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m19:55:07.623361 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m19:55:07.623809 [info ] [Thread-1 (]: 32 of 55 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m19:55:07.624604 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f) - Creating connection
[0m19:55:07.625002 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f'
[0m19:55:07.625350 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m19:55:07.630809 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m19:55:07.632121 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m19:55:07.634498 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m19:55:07.635526 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m19:55:07.636051 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



  
  
      
    ) dbt_internal_test
[0m19:55:07.636391 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:07.963271 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c5d0-16cd-9597-ce6f9ce91eea) - Created
[0m19:55:08.516024 [debug] [Thread-1 (]: SQL status: OK in 0.880 seconds
[0m19:55:08.517908 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c5d0-16cd-9597-ce6f9ce91eea, command-id=01f09a38-c5de-12da-ab63-fdb4d2f0cc57) - Closing
[0m19:55:08.518600 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m19:55:08.518952 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c5d0-16cd-9597-ce6f9ce91eea) - Closing
[0m19:55:08.621118 [info ] [Thread-1 (]: 32 of 55 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 1.00s]
[0m19:55:08.621908 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m19:55:08.622294 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m19:55:08.622696 [info ] [Thread-1 (]: 33 of 55 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m19:55:08.623314 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3) - Creating connection
[0m19:55:08.623688 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3'
[0m19:55:08.623985 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m19:55:08.627725 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m19:55:08.628861 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m19:55:08.631353 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m19:55:08.632298 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m19:55:08.632835 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m19:55:08.633159 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:08.967866 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c669-1ecc-a6f5-1b0e85d71814) - Created
[0m19:55:09.147730 [debug] [Thread-1 (]: SQL status: OK in 0.510 seconds
[0m19:55:09.149592 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c669-1ecc-a6f5-1b0e85d71814, command-id=01f09a38-c677-1dc4-9ca6-f6d9c7d20dfa) - Closing
[0m19:55:09.150196 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m19:55:09.150514 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c669-1ecc-a6f5-1b0e85d71814) - Closing
[0m19:55:09.255290 [info ] [Thread-1 (]: 33 of 55 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 0.63s]
[0m19:55:09.256123 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m19:55:09.256528 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m19:55:09.257078 [info ] [Thread-1 (]: 34 of 55 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m19:55:09.257951 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3) - Creating connection
[0m19:55:09.258345 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3'
[0m19:55:09.258666 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m19:55:09.262570 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m19:55:09.263706 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m19:55:09.267964 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m19:55:09.269230 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m19:55:09.269711 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



  
  
      
    ) dbt_internal_test
[0m19:55:09.270050 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:09.606614 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c6cb-10c8-b460-8c21d1b5e2e1) - Created
[0m19:55:09.817683 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m19:55:09.819755 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c6cb-10c8-b460-8c21d1b5e2e1, command-id=01f09a38-c6d8-1f1b-8bd4-142be50eeedc) - Closing
[0m19:55:09.820412 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m19:55:09.820739 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c6cb-10c8-b460-8c21d1b5e2e1) - Closing
[0m19:55:09.930931 [info ] [Thread-1 (]: 34 of 55 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 0.67s]
[0m19:55:09.931760 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m19:55:09.932131 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m19:55:09.932569 [info ] [Thread-1 (]: 35 of 55 START test not_null_silver_tags_tag ................................... [RUN]
[0m19:55:09.933248 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8) - Creating connection
[0m19:55:09.933646 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8'
[0m19:55:09.933955 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m19:55:09.938184 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m19:55:09.939593 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m19:55:09.944150 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m19:55:09.945285 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m19:55:09.945871 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



  
  
      
    ) dbt_internal_test
[0m19:55:09.946306 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:11.005118 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c7a0-17db-99d0-c282c6b189e1) - Created
[0m19:55:11.189101 [debug] [Thread-1 (]: SQL status: OK in 1.240 seconds
[0m19:55:11.190940 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c7a0-17db-99d0-c282c6b189e1, command-id=01f09a38-c7af-12e5-8bec-52fd0e6079ed) - Closing
[0m19:55:11.191521 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m19:55:11.191831 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c7a0-17db-99d0-c282c6b189e1) - Closing
[0m19:55:11.300455 [info ] [Thread-1 (]: 35 of 55 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 1.37s]
[0m19:55:11.301471 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m19:55:11.302341 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m19:55:11.302815 [info ] [Thread-1 (]: 36 of 55 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m19:55:11.303509 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86) - Creating connection
[0m19:55:11.303854 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86'
[0m19:55:11.304232 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m19:55:11.309682 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m19:55:11.310793 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m19:55:11.313870 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m19:55:11.314847 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m19:55:11.315373 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



  
  
      
    ) dbt_internal_test
[0m19:55:11.315684 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:11.640894 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c801-19d0-8c50-44f0ec1fad9d) - Created
[0m19:55:12.229155 [debug] [Thread-1 (]: SQL status: OK in 0.910 seconds
[0m19:55:12.231363 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c801-19d0-8c50-44f0ec1fad9d, command-id=01f09a38-c80e-1c28-8420-2f941a4231ac) - Closing
[0m19:55:12.232123 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m19:55:12.232485 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c801-19d0-8c50-44f0ec1fad9d) - Closing
[0m19:55:12.345101 [info ] [Thread-1 (]: 36 of 55 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 1.04s]
[0m19:55:12.346136 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m19:55:12.346569 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m19:55:12.347001 [info ] [Thread-1 (]: 37 of 55 START test not_null_silver_tags_user_id ............................... [RUN]
[0m19:55:12.347667 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3) - Creating connection
[0m19:55:12.348039 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3'
[0m19:55:12.348366 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m19:55:12.359892 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m19:55:12.361235 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m19:55:12.364162 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m19:55:12.365277 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m19:55:12.365770 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m19:55:12.366192 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:12.709689 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c8a4-1c43-824a-0b1c16508eba) - Created
[0m19:55:12.893662 [debug] [Thread-1 (]: SQL status: OK in 0.530 seconds
[0m19:55:12.895838 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c8a4-1c43-824a-0b1c16508eba, command-id=01f09a38-c8b2-1c4c-856a-394741411315) - Closing
[0m19:55:12.896919 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m19:55:12.897497 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c8a4-1c43-824a-0b1c16508eba) - Closing
[0m19:55:13.000080 [info ] [Thread-1 (]: 37 of 55 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 0.65s]
[0m19:55:13.001199 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m19:55:13.001794 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m19:55:13.002372 [info ] [Thread-1 (]: 38 of 55 START test not_null_user_activity_avg_user_rating ..................... [RUN]
[0m19:55:13.003287 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b) - Creating connection
[0m19:55:13.003783 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b'
[0m19:55:13.004257 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m19:55:13.013688 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m19:55:13.015345 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m19:55:13.018574 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m19:55:13.019734 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m19:55:13.020298 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select avg_user_rating
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating is null



  
  
      
    ) dbt_internal_test
[0m19:55:13.020813 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:13.365746 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c909-10fc-a7f5-d6b51c71a069) - Created
[0m19:55:13.542096 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m19:55:13.544637 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c909-10fc-a7f5-d6b51c71a069, command-id=01f09a38-c916-15d9-95ab-97cdb8432a14) - Closing
[0m19:55:13.545496 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: Close
[0m19:55:13.545934 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c909-10fc-a7f5-d6b51c71a069) - Closing
[0m19:55:13.648333 [info ] [Thread-1 (]: 38 of 55 PASS not_null_user_activity_avg_user_rating ........................... [[32mPASS[0m in 0.65s]
[0m19:55:13.649229 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m19:55:13.649660 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m19:55:13.650073 [info ] [Thread-1 (]: 39 of 55 START test not_null_user_activity_first_activity_date ................. [RUN]
[0m19:55:13.650663 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac) - Creating connection
[0m19:55:13.650996 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac'
[0m19:55:13.651289 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m19:55:13.656849 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m19:55:13.657969 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m19:55:13.660673 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m19:55:13.661679 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m19:55:13.662114 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select first_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where first_activity_date is null



  
  
      
    ) dbt_internal_test
[0m19:55:13.662510 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:14.001460 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c96a-1163-8d9c-e4a40f8fe89a) - Created
[0m19:55:14.177645 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m19:55:14.179920 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c96a-1163-8d9c-e4a40f8fe89a, command-id=01f09a38-c977-162d-9db9-e0febdbaf07d) - Closing
[0m19:55:14.180783 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: Close
[0m19:55:14.181168 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c96a-1163-8d9c-e4a40f8fe89a) - Closing
[0m19:55:14.280204 [info ] [Thread-1 (]: 39 of 55 PASS not_null_user_activity_first_activity_date ....................... [[32mPASS[0m in 0.63s]
[0m19:55:14.281274 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m19:55:14.281839 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m19:55:14.282386 [info ] [Thread-1 (]: 40 of 55 START test not_null_user_activity_influence_score ..................... [RUN]
[0m19:55:14.283287 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_user_activity_influence_score.758852d325) - Creating connection
[0m19:55:14.283856 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_user_activity_influence_score.758852d325'
[0m19:55:14.284565 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m19:55:14.290607 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m19:55:14.292954 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m19:55:14.300795 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m19:55:14.302253 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m19:55:14.302861 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select influence_score
from `workspace`.`movielens_volume`.`user_activity`
where influence_score is null



  
  
      
    ) dbt_internal_test
[0m19:55:14.303234 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:14.666403 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c9ce-1ca3-9bcc-f4d734950058) - Created
[0m19:55:14.842721 [debug] [Thread-1 (]: SQL status: OK in 0.540 seconds
[0m19:55:14.844735 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-c9ce-1ca3-9bcc-f4d734950058, command-id=01f09a38-c9dd-1327-83b6-c8f1359550aa) - Closing
[0m19:55:14.845385 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: Close
[0m19:55:14.845738 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-c9ce-1ca3-9bcc-f4d734950058) - Closing
[0m19:55:14.956147 [info ] [Thread-1 (]: 40 of 55 PASS not_null_user_activity_influence_score ........................... [[32mPASS[0m in 0.67s]
[0m19:55:14.957181 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m19:55:14.957748 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m19:55:14.958278 [info ] [Thread-1 (]: 41 of 55 START test not_null_user_activity_last_activity_date .................. [RUN]
[0m19:55:14.959419 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad) - Creating connection
[0m19:55:14.959947 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad'
[0m19:55:14.960290 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m19:55:14.965023 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m19:55:14.966315 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m19:55:14.972428 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m19:55:14.973810 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m19:55:14.974391 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select last_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where last_activity_date is null



  
  
      
    ) dbt_internal_test
[0m19:55:14.974837 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:15.360209 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-ca36-1d47-beb9-fd8493efd7e2) - Created
[0m19:55:15.544359 [debug] [Thread-1 (]: SQL status: OK in 0.570 seconds
[0m19:55:15.546588 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-ca36-1d47-beb9-fd8493efd7e2, command-id=01f09a38-ca47-186c-84e4-86db4258bdb1) - Closing
[0m19:55:15.547649 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: Close
[0m19:55:15.548429 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-ca36-1d47-beb9-fd8493efd7e2) - Closing
[0m19:55:15.688096 [info ] [Thread-1 (]: 41 of 55 PASS not_null_user_activity_last_activity_date ........................ [[32mPASS[0m in 0.73s]
[0m19:55:15.689166 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m19:55:15.689655 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m19:55:15.690139 [info ] [Thread-1 (]: 42 of 55 START test not_null_user_activity_top_genre ........................... [RUN]
[0m19:55:15.690765 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313) - Creating connection
[0m19:55:15.691080 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313'
[0m19:55:15.691389 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m19:55:15.696344 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m19:55:15.697461 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m19:55:15.702533 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m19:55:15.703797 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m19:55:15.704319 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select top_genre
from `workspace`.`movielens_volume`.`user_activity`
where top_genre is null



  
  
      
    ) dbt_internal_test
[0m19:55:15.704710 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:16.060800 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-caa4-12a9-8a8e-8f98fb7d7f68) - Created
[0m19:55:16.278908 [debug] [Thread-1 (]: SQL status: OK in 0.570 seconds
[0m19:55:16.281289 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-caa4-12a9-8a8e-8f98fb7d7f68, command-id=01f09a38-cab1-1c40-9f3b-7b8c69a30673) - Closing
[0m19:55:16.282074 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: Close
[0m19:55:16.282515 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-caa4-12a9-8a8e-8f98fb7d7f68) - Closing
[0m19:55:16.394865 [info ] [Thread-1 (]: 42 of 55 PASS not_null_user_activity_top_genre ................................. [[32mPASS[0m in 0.70s]
[0m19:55:16.395766 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m19:55:16.396146 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m19:55:16.396584 [info ] [Thread-1 (]: 43 of 55 START test not_null_user_activity_user_id ............................. [RUN]
[0m19:55:16.397247 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36) - Creating connection
[0m19:55:16.397575 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36'
[0m19:55:16.397866 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m19:55:16.404221 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m19:55:16.405614 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m19:55:16.408548 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m19:55:16.409959 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m19:55:16.410488 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `workspace`.`movielens_volume`.`user_activity`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m19:55:16.410830 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:16.753566 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cb0d-1cbf-b476-d710003e1217) - Created
[0m19:55:16.931814 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m19:55:16.934315 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-cb0d-1cbf-b476-d710003e1217, command-id=01f09a38-cb1b-1b82-8949-d03b85d7d88c) - Closing
[0m19:55:16.935145 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: Close
[0m19:55:16.935572 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cb0d-1cbf-b476-d710003e1217) - Closing
[0m19:55:17.043035 [info ] [Thread-1 (]: 43 of 55 PASS not_null_user_activity_user_id ................................... [[32mPASS[0m in 0.65s]
[0m19:55:17.043991 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m19:55:17.044513 [debug] [Thread-1 (]: Began running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m19:55:17.045056 [info ] [Thread-1 (]: 44 of 55 START test not_null_user_activity_user_rating_count ................... [RUN]
[0m19:55:17.045784 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e) - Creating connection
[0m19:55:17.046176 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e'
[0m19:55:17.046522 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m19:55:17.053959 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m19:55:17.055629 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m19:55:17.058727 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m19:55:17.059927 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m19:55:17.060454 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_rating_count
from `workspace`.`movielens_volume`.`user_activity`
where user_rating_count is null



  
  
      
    ) dbt_internal_test
[0m19:55:17.060799 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:17.405998 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cb71-1636-b2fb-adf73d55a89a) - Created
[0m19:55:17.584838 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m19:55:17.587264 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-cb71-1636-b2fb-adf73d55a89a, command-id=01f09a38-cb7e-1db0-b6d5-031ad81549c4) - Closing
[0m19:55:17.588040 [debug] [Thread-1 (]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: Close
[0m19:55:17.588456 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cb71-1636-b2fb-adf73d55a89a) - Closing
[0m19:55:17.692530 [info ] [Thread-1 (]: 44 of 55 PASS not_null_user_activity_user_rating_count ......................... [[32mPASS[0m in 0.65s]
[0m19:55:17.693711 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m19:55:17.694098 [debug] [Thread-1 (]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m19:55:17.694512 [info ] [Thread-1 (]: 45 of 55 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m19:55:17.695187 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1) - Creating connection
[0m19:55:17.695517 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1'
[0m19:55:17.695819 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m19:55:17.703206 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m19:55:17.704555 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m19:55:17.707428 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m19:55:17.708702 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m19:55:17.709230 [debug] [Thread-1 (]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m19:55:17.709661 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:18.058582 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cbd4-1d6d-82fa-286142b6298a) - Created
[0m19:55:18.251701 [debug] [Thread-1 (]: SQL status: OK in 0.540 seconds
[0m19:55:18.253888 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-cbd4-1d6d-82fa-286142b6298a, command-id=01f09a38-cbe2-11f8-ab9e-fa2605d84e7b) - Closing
[0m19:55:18.254648 [debug] [Thread-1 (]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m19:55:18.255029 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cbd4-1d6d-82fa-286142b6298a) - Closing
[0m19:55:18.358494 [info ] [Thread-1 (]: 45 of 55 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.66s]
[0m19:55:18.359465 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m19:55:18.360031 [debug] [Thread-1 (]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m19:55:18.360618 [info ] [Thread-1 (]: 46 of 55 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m19:55:18.361387 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d) - Creating connection
[0m19:55:18.361942 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d'
[0m19:55:18.362436 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m19:55:18.371818 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m19:55:18.373178 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m19:55:18.377356 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m19:55:18.378584 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m19:55:18.379202 [debug] [Thread-1 (]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m19:55:18.379595 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:18.722453 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cc3a-1344-a8a5-2b70f3eba62c) - Created
[0m19:55:18.906884 [debug] [Thread-1 (]: SQL status: OK in 0.530 seconds
[0m19:55:18.909448 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-cc3a-1344-a8a5-2b70f3eba62c, command-id=01f09a38-cc47-1391-bed0-b628d454e7ce) - Closing
[0m19:55:18.910239 [debug] [Thread-1 (]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m19:55:18.910595 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cc3a-1344-a8a5-2b70f3eba62c) - Closing
[0m19:55:19.016809 [info ] [Thread-1 (]: 46 of 55 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.66s]
[0m19:55:19.017737 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m19:55:19.018185 [debug] [Thread-1 (]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m19:55:19.018664 [info ] [Thread-1 (]: 47 of 55 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m19:55:19.019326 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32) - Creating connection
[0m19:55:19.019659 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32'
[0m19:55:19.019960 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m19:55:19.025599 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m19:55:19.026996 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m19:55:19.037291 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m19:55:19.038627 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m19:55:19.039177 [debug] [Thread-1 (]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m19:55:19.039625 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:19.691672 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cccd-1efe-bac1-7ee6cc829e7f) - Created
[0m19:55:19.885063 [debug] [Thread-1 (]: SQL status: OK in 0.850 seconds
[0m19:55:19.888154 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-cccd-1efe-bac1-7ee6cc829e7f, command-id=01f09a38-ccdc-14b7-946e-0913c8954d0b) - Closing
[0m19:55:19.889254 [debug] [Thread-1 (]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m19:55:19.889784 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cccd-1efe-bac1-7ee6cc829e7f) - Closing
[0m19:55:19.997835 [info ] [Thread-1 (]: 47 of 55 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.98s]
[0m19:55:19.998768 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m19:55:19.999268 [debug] [Thread-1 (]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m19:55:19.999763 [info ] [Thread-1 (]: 48 of 55 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m19:55:20.000513 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd) - Creating connection
[0m19:55:20.000860 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd'
[0m19:55:20.001181 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m19:55:20.006477 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m19:55:20.007735 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m19:55:20.013188 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m19:55:20.014610 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m19:55:20.015250 [debug] [Thread-1 (]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m19:55:20.015731 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:20.665813 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cd62-1257-a5de-f3d4c1dc242b) - Created
[0m19:55:20.851365 [debug] [Thread-1 (]: SQL status: OK in 0.840 seconds
[0m19:55:20.854985 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-cd62-1257-a5de-f3d4c1dc242b, command-id=01f09a38-cd70-12e0-8f3a-2554e840a593) - Closing
[0m19:55:20.856446 [debug] [Thread-1 (]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m19:55:20.857118 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cd62-1257-a5de-f3d4c1dc242b) - Closing
[0m19:55:20.963716 [info ] [Thread-1 (]: 48 of 55 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.96s]
[0m19:55:20.965147 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m19:55:20.965768 [debug] [Thread-1 (]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m19:55:20.966440 [info ] [Thread-1 (]: 49 of 55 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m19:55:20.967386 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719) - Creating connection
[0m19:55:20.967993 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719'
[0m19:55:20.968489 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m19:55:20.975997 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m19:55:20.977596 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m19:55:20.984429 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m19:55:20.986539 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m19:55:20.987225 [debug] [Thread-1 (]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m19:55:20.987742 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:21.358257 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cdcc-170a-b582-b99dec373132) - Created
[0m19:55:21.552108 [debug] [Thread-1 (]: SQL status: OK in 0.560 seconds
[0m19:55:21.554116 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-cdcc-170a-b582-b99dec373132, command-id=01f09a38-cdd9-1b1c-ab9c-696bbc09bf85) - Closing
[0m19:55:21.555024 [debug] [Thread-1 (]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m19:55:21.555505 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cdcc-170a-b582-b99dec373132) - Closing
[0m19:55:21.664454 [info ] [Thread-1 (]: 49 of 55 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 0.70s]
[0m19:55:21.665306 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m19:55:21.665809 [debug] [Thread-1 (]: Began running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m19:55:21.666510 [info ] [Thread-1 (]: 50 of 55 START test unique_genre_popularity_genres ............................. [RUN]
[0m19:55:21.667398 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075) - Creating connection
[0m19:55:21.667878 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075'
[0m19:55:21.668214 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m19:55:21.677293 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m19:55:21.678400 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m19:55:21.681484 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m19:55:21.682590 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m19:55:21.683085 [debug] [Thread-1 (]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    genres as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`genre_popularity`
where genres is not null
group by genres
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:55:21.683419 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:22.018687 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-ce30-1c54-b6c0-223549bba870) - Created
[0m19:55:22.201920 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m19:55:22.203951 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-ce30-1c54-b6c0-223549bba870, command-id=01f09a38-ce3f-1022-b2e3-3e8ce0300693) - Closing
[0m19:55:22.204589 [debug] [Thread-1 (]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: Close
[0m19:55:22.204914 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-ce30-1c54-b6c0-223549bba870) - Closing
[0m19:55:22.305334 [info ] [Thread-1 (]: 50 of 55 PASS unique_genre_popularity_genres ................................... [[32mPASS[0m in 0.64s]
[0m19:55:22.306571 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m19:55:22.307230 [debug] [Thread-1 (]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m19:55:22.307824 [info ] [Thread-1 (]: 51 of 55 START test unique_movie_enriched_movie_id ............................. [RUN]
[0m19:55:22.308533 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a) - Creating connection
[0m19:55:22.308941 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a'
[0m19:55:22.309308 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m19:55:22.315463 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m19:55:22.316845 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m19:55:22.319845 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m19:55:22.320955 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m19:55:22.321367 [debug] [Thread-1 (]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:55:22.321694 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:22.741494 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-ce9f-106c-9893-b3f34206dd7e) - Created
[0m19:55:22.934170 [debug] [Thread-1 (]: SQL status: OK in 0.610 seconds
[0m19:55:22.936566 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-ce9f-106c-9893-b3f34206dd7e, command-id=01f09a38-cead-170d-85c1-a20cee4dd063) - Closing
[0m19:55:22.937409 [debug] [Thread-1 (]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m19:55:22.937776 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-ce9f-106c-9893-b3f34206dd7e) - Closing
[0m19:55:23.041165 [info ] [Thread-1 (]: 51 of 55 PASS unique_movie_enriched_movie_id ................................... [[32mPASS[0m in 0.73s]
[0m19:55:23.042222 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m19:55:23.042722 [debug] [Thread-1 (]: Began running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m19:55:23.043168 [info ] [Thread-1 (]: 52 of 55 START test unique_movie_performance_movie_id .......................... [RUN]
[0m19:55:23.044079 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22) - Creating connection
[0m19:55:23.044501 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22'
[0m19:55:23.045088 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m19:55:23.055744 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m19:55:23.057721 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m19:55:23.062241 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m19:55:23.064305 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m19:55:23.065211 [debug] [Thread-1 (]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is not null
group by movie_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:55:23.065913 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:23.423653 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cf07-1225-a2ca-92f90fbef284) - Created
[0m19:55:23.603224 [debug] [Thread-1 (]: SQL status: OK in 0.540 seconds
[0m19:55:23.605619 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-cf07-1225-a2ca-92f90fbef284, command-id=01f09a38-cf15-13ca-accd-686bb5eebc2f) - Closing
[0m19:55:23.606305 [debug] [Thread-1 (]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: Close
[0m19:55:23.606652 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cf07-1225-a2ca-92f90fbef284) - Closing
[0m19:55:23.722493 [info ] [Thread-1 (]: 52 of 55 PASS unique_movie_performance_movie_id ................................ [[32mPASS[0m in 0.68s]
[0m19:55:23.723444 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m19:55:23.723852 [debug] [Thread-1 (]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m19:55:23.724276 [info ] [Thread-1 (]: 53 of 55 START test unique_silver_links_movie_id ............................... [RUN]
[0m19:55:23.724968 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.unique_silver_links_movie_id.c422609054) - Creating connection
[0m19:55:23.725298 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.unique_silver_links_movie_id.c422609054'
[0m19:55:23.725604 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m19:55:23.732622 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m19:55:23.733843 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m19:55:23.736974 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m19:55:23.738334 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m19:55:23.738962 [debug] [Thread-1 (]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:55:23.739489 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:55:23.876276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a94043550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a9266e100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a9266e850>]}


============================== 17:55:23.884188 | 3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1 ==============================
[0m17:55:23.884188 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:55:23.885213 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/opt/airflow/dbt/logs', 'debug': 'False', 'profiles_dir': '/opt/airflow/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m19:55:24.076575 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cf6a-1fcc-8277-f80396369078) - Created
[0m19:55:24.263265 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m19:55:24.265560 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-cf6a-1fcc-8277-f80396369078, command-id=01f09a38-cf79-1626-ac7b-f554759dcdbd) - Closing
[0m19:55:24.266286 [debug] [Thread-1 (]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m19:55:24.266643 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-cf6a-1fcc-8277-f80396369078) - Closing
[0m19:55:24.372646 [info ] [Thread-1 (]: 53 of 55 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 0.65s]
[0m19:55:24.373528 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m19:55:24.374097 [debug] [Thread-1 (]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m19:55:24.374612 [info ] [Thread-1 (]: 54 of 55 START test unique_silver_movies_movie_id .............................. [RUN]
[0m19:55:24.375348 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085) - Creating connection
[0m19:55:24.375763 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085'
[0m19:55:24.376161 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m19:55:24.380214 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m19:55:24.381666 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m19:55:24.384799 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m19:55:24.385893 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m19:55:24.386317 [debug] [Thread-1 (]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:55:24.386748 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:55:24.986457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a9266e0a0>]}
[0m17:55:25.019976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a6731b8b0>]}
[0m17:55:25.021082 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m19:55:25.110553 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-d008-1ca6-9303-24d6d490b166) - Created
[0m17:55:25.069025 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:55:25.212276 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m17:55:25.213463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a6735de50>]}
[0m19:55:25.307421 [debug] [Thread-1 (]: SQL status: OK in 0.920 seconds
[0m19:55:25.309913 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-d008-1ca6-9303-24d6d490b166, command-id=01f09a38-d016-1a6a-b2f4-4ee6c9447163) - Closing
[0m19:55:25.310732 [debug] [Thread-1 (]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m19:55:25.311203 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-d008-1ca6-9303-24d6d490b166) - Closing
[0m19:55:25.423138 [info ] [Thread-1 (]: 54 of 55 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 1.05s]
[0m19:55:25.424229 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m19:55:25.424731 [debug] [Thread-1 (]: Began running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m19:55:25.425328 [info ] [Thread-1 (]: 55 of 55 START test unique_user_activity_user_id ............................... [RUN]
[0m19:55:25.426187 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda) - Creating connection
[0m19:55:25.426667 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda'
[0m19:55:25.427142 [debug] [Thread-1 (]: Began compiling node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m19:55:25.432490 [debug] [Thread-1 (]: Writing injected SQL for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m19:55:25.433793 [debug] [Thread-1 (]: Began executing node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m19:55:25.439120 [debug] [Thread-1 (]: Writing runtime sql for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m19:55:25.440270 [debug] [Thread-1 (]: Using databricks connection "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m19:55:25.440747 [debug] [Thread-1 (]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`user_activity`
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m19:55:25.441198 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:55:25.790234 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-d071-1164-b176-6b3bac02c1fa) - Created
[0m19:55:25.974897 [debug] [Thread-1 (]: SQL status: OK in 0.530 seconds
[0m19:55:25.977531 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09a38-d071-1164-b176-6b3bac02c1fa, command-id=01f09a38-d07e-12c1-bb63-38eba2769203) - Closing
[0m19:55:25.978457 [debug] [Thread-1 (]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: Close
[0m19:55:25.978927 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09a38-d071-1164-b176-6b3bac02c1fa) - Closing
[0m19:55:26.085487 [info ] [Thread-1 (]: 55 of 55 PASS unique_user_activity_user_id ..................................... [[32mPASS[0m in 0.66s]
[0m19:55:26.086440 [debug] [Thread-1 (]: Finished running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m19:55:26.088061 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:55:26.088463 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:55:26.088999 [info ] [MainThread]: 
[0m19:55:26.089520 [info ] [MainThread]: Finished running 55 data tests in 0 hours 0 minutes and 50.98 seconds (50.98s).
[0m19:55:26.101204 [debug] [MainThread]: Command end result
[0m19:55:26.171879 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\38349\projects\end_to_end_de_project\dbt\target\manifest.json
[0m19:55:26.177221 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\38349\projects\end_to_end_de_project\dbt\target\semantic_manifest.json
[0m19:55:26.192999 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\38349\projects\end_to_end_de_project\dbt\target\run_results.json
[0m19:55:26.193563 [info ] [MainThread]: 
[0m19:55:26.194237 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:55:26.194772 [info ] [MainThread]: 
[0m19:55:26.195304 [info ] [MainThread]: Done. PASS=55 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=55
[0m19:55:26.196142 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 6 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m19:55:26.197450 [debug] [MainThread]: Command `dbt test` succeeded at 19:55:26.197307 after 54.80 seconds
[0m19:55:26.197975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F085BEFC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F07310FF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F07311310>]}
[0m19:55:26.198453 [debug] [MainThread]: Flushing usage events
[0m19:55:27.077600 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:55:28.099675 [debug] [MainThread]: 1603: static parser failed on gold/genre_popularity.sql
[0m17:55:28.124738 [debug] [MainThread]: 1602: parser fallback to jinja rendering on gold/genre_popularity.sql
[0m17:55:28.130742 [debug] [MainThread]: 1699: static parser successfully parsed gold/movie_enriched.sql
[0m17:55:28.137206 [debug] [MainThread]: 1603: static parser failed on gold/movie_performance.sql
[0m17:55:28.146854 [debug] [MainThread]: 1602: parser fallback to jinja rendering on gold/movie_performance.sql
[0m17:55:28.150103 [debug] [MainThread]: 1603: static parser failed on gold/user_activity.sql
[0m17:55:28.159019 [debug] [MainThread]: 1602: parser fallback to jinja rendering on gold/user_activity.sql
[0m17:55:28.161961 [debug] [MainThread]: 1603: static parser failed on gold/user_preferences.sql
[0m17:55:28.170552 [debug] [MainThread]: 1602: parser fallback to jinja rendering on gold/user_preferences.sql
[0m17:55:28.173997 [debug] [MainThread]: 1699: static parser successfully parsed silver/silver_links.sql
[0m17:55:28.179686 [debug] [MainThread]: 1699: static parser successfully parsed silver/silver_movies.sql
[0m17:55:28.186951 [debug] [MainThread]: 1699: static parser successfully parsed silver/silver_ratings.sql
[0m17:55:28.192512 [debug] [MainThread]: 1699: static parser successfully parsed silver/silver_tags.sql
[0m17:55:28.649591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a67062220>]}
[0m17:55:28.686990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a6726c6a0>]}
[0m17:55:28.687926 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m17:55:28.688667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a672114c0>]}
[0m17:55:28.691451 [info ] [MainThread]: 
[0m17:55:28.692831 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:55:28.695172 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:55:28.695822 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:55:28.696482 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m17:55:28.697432 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:55:29.772487 [debug] [ThreadPool]: SQL status: OK in 1.0800000429153442 seconds
[0m17:55:29.774613 [debug] [ThreadPool]: On list_workspace: Close
[0m17:55:29.894920 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m17:55:29.896487 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m17:55:29.912652 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:55:29.913445 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m17:55:29.914310 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m17:55:29.915078 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:55:30.813603 [debug] [ThreadPool]: SQL status: OK in 0.8999999761581421 seconds
[0m17:55:30.815328 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m17:55:30.816012 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m17:55:30.816812 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:55:30.817450 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m17:55:30.930005 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m17:55:30.939191 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m17:55:30.940012 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m17:55:30.940649 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:55:31.577984 [debug] [ThreadPool]: SQL status: OK in 0.6399999856948853 seconds
[0m17:55:31.581483 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m17:55:31.696237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a657b0fa0>]}
[0m17:55:31.697251 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:55:31.697805 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:55:31.698927 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:55:31.699876 [info ] [MainThread]: 
[0m17:55:31.715944 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_links
[0m17:55:31.716955 [info ] [Thread-4  ]: 1 of 4 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m17:55:31.718186 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.silver_links)
[0m17:55:31.718864 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_links
[0m17:55:31.725779 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m17:55:31.735348 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (compile): 17:55:31.719379 => 17:55:31.734782
[0m17:55:31.736220 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_links
[0m17:55:31.756519 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:55:31.757414 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m17:55:31.758085 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

      describe extended `workspace`.`movielens_volume`.`silver_links`
  
[0m17:55:31.758797 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:55:32.435032 [debug] [Thread-4  ]: SQL status: OK in 0.6800000071525574 seconds
[0m17:55:32.500514 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m17:55:32.509414 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m17:55:32.510340 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
    using delta
      
      
      
      
      
      
      as
      

with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m17:55:34.502632 [debug] [Thread-4  ]: SQL status: OK in 1.9900000095367432 seconds
[0m17:55:34.537753 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (execute): 17:55:31.736738 => 17:55:34.537495
[0m17:55:34.538637 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: ROLLBACK
[0m17:55:34.539306 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:55:34.540073 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: Close
[0m17:55:34.659273 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a644fd9d0>]}
[0m17:55:34.660475 [info ] [Thread-4  ]: 1 of 4 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 2.94s]
[0m17:55:34.661761 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_links
[0m17:55:34.662681 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_movies
[0m17:55:34.663484 [info ] [Thread-4  ]: 2 of 4 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m17:55:34.664967 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_links, now model.my_dbt_project.silver_movies)
[0m17:55:34.665902 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_movies
[0m17:55:34.671443 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m17:55:34.680599 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (compile): 17:55:34.666412 => 17:55:34.680146
[0m17:55:34.681443 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_movies
[0m17:55:34.688014 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:55:34.688739 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m17:55:34.689511 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

      describe extended `workspace`.`movielens_volume`.`silver_movies`
  
[0m17:55:34.690246 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:55:35.391802 [debug] [Thread-4  ]: SQL status: OK in 0.699999988079071 seconds
[0m17:55:35.398666 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m17:55:35.407878 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m17:55:35.408617 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m17:55:37.210416 [debug] [Thread-4  ]: SQL status: OK in 1.7999999523162842 seconds
[0m17:55:37.214599 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (execute): 17:55:34.681987 => 17:55:37.214318
[0m17:55:37.215556 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: ROLLBACK
[0m17:55:37.216267 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:55:37.217010 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: Close
[0m17:55:37.338312 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a644fbf10>]}
[0m17:55:37.339578 [info ] [Thread-4  ]: 2 of 4 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 2.67s]
[0m17:55:37.340878 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_movies
[0m17:55:37.341716 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_ratings
[0m17:55:37.342719 [info ] [Thread-4  ]: 3 of 4 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m17:55:37.344044 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_movies, now model.my_dbt_project.silver_ratings)
[0m17:55:37.344905 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_ratings
[0m17:55:37.350692 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m17:55:37.360226 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (compile): 17:55:37.345503 => 17:55:37.359809
[0m17:55:37.360972 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_ratings
[0m17:55:37.372748 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:55:37.373583 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m17:55:37.374288 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

      describe extended `workspace`.`movielens_volume`.`silver_ratings`
  
[0m17:55:37.374962 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:55:37.992034 [debug] [Thread-4  ]: SQL status: OK in 0.6200000047683716 seconds
[0m17:55:37.998694 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m17:55:38.008066 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m17:55:38.008956 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m17:55:40.990956 [debug] [Thread-4  ]: SQL status: OK in 2.9800000190734863 seconds
[0m17:55:40.994804 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (execute): 17:55:37.361507 => 17:55:40.994452
[0m17:55:40.995541 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: ROLLBACK
[0m17:55:40.996193 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:55:40.996874 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: Close
[0m17:55:41.106104 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a64552c40>]}
[0m17:55:41.107393 [info ] [Thread-4  ]: 3 of 4 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 3.76s]
[0m17:55:41.108698 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_ratings
[0m17:55:41.109584 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_tags
[0m17:55:41.110507 [info ] [Thread-4  ]: 4 of 4 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m17:55:41.111807 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_ratings, now model.my_dbt_project.silver_tags)
[0m17:55:41.112542 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_tags
[0m17:55:41.118926 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m17:55:41.128404 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (compile): 17:55:41.113177 => 17:55:41.127945
[0m17:55:41.129199 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_tags
[0m17:55:41.136041 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:55:41.136808 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m17:55:41.137417 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

      describe extended `workspace`.`movielens_volume`.`silver_tags`
  
[0m17:55:41.138184 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:55:41.769840 [debug] [Thread-4  ]: SQL status: OK in 0.6299999952316284 seconds
[0m17:55:41.777907 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m17:55:41.787227 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m17:55:41.787962 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m17:55:44.227327 [debug] [Thread-4  ]: SQL status: OK in 2.440000057220459 seconds
[0m17:55:44.231155 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (execute): 17:55:41.129726 => 17:55:44.230803
[0m17:55:44.232051 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: ROLLBACK
[0m17:55:44.232712 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:55:44.233359 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: Close
[0m17:55:44.343828 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d4ef104-84d2-4cd3-af18-c88dfb9e5bf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a6452f850>]}
[0m17:55:44.345334 [info ] [Thread-4  ]: 4 of 4 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 3.23s]
[0m17:55:44.346837 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_tags
[0m17:55:44.349421 [debug] [MainThread]: On master: ROLLBACK
[0m17:55:44.350176 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:55:44.708304 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:55:44.709426 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:55:44.710516 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:55:44.711188 [debug] [MainThread]: On master: ROLLBACK
[0m17:55:44.711864 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:55:44.712512 [debug] [MainThread]: On master: Close
[0m17:55:44.819817 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:55:44.820651 [debug] [MainThread]: Connection 'model.my_dbt_project.silver_tags' was properly closed.
[0m17:55:44.824072 [info ] [MainThread]: 
[0m17:55:44.825697 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 16.13 seconds (16.13s).
[0m17:55:44.827885 [debug] [MainThread]: Command end result
[0m17:55:44.954057 [info ] [MainThread]: 
[0m17:55:44.955319 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:55:44.956245 [info ] [MainThread]: 
[0m17:55:44.957097 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m17:55:44.958216 [debug] [MainThread]: Command `dbt run` succeeded at 17:55:44.958071 after 21.11 seconds
[0m17:55:44.958862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a94043550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a6456e250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a6726c6a0>]}
[0m17:55:44.959539 [debug] [MainThread]: Flushing usage events
[0m17:55:49.388571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6368847640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6366db0250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6366db0970>]}


============================== 17:55:49.396185 | e54cdca7-65f4-4346-b823-003c28d81365 ==============================
[0m17:55:49.396185 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:55:49.397160 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:55:50.277991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e54cdca7-65f4-4346-b823-003c28d81365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6366db01f0>]}
[0m17:55:50.308813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e54cdca7-65f4-4346-b823-003c28d81365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633bad8b80>]}
[0m17:55:50.309779 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m17:55:50.355453 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:55:51.637308 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:55:51.638032 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:55:51.650213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e54cdca7-65f4-4346-b823-003c28d81365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633b840400>]}
[0m17:55:51.688528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e54cdca7-65f4-4346-b823-003c28d81365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633b9ae430>]}
[0m17:55:51.689619 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m17:55:51.690485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e54cdca7-65f4-4346-b823-003c28d81365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633b9ae490>]}
[0m17:55:51.693126 [info ] [MainThread]: 
[0m17:55:51.694994 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:55:51.697258 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:55:51.698089 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:55:51.698897 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m17:55:51.699658 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:55:52.234077 [debug] [ThreadPool]: SQL status: OK in 0.5299999713897705 seconds
[0m17:55:52.236190 [debug] [ThreadPool]: On list_workspace: Close
[0m17:55:52.347036 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m17:55:52.348390 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m17:55:52.362541 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:55:52.363413 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m17:55:52.364107 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m17:55:52.364722 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:55:53.344723 [debug] [ThreadPool]: SQL status: OK in 0.9800000190734863 seconds
[0m17:55:53.346468 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m17:55:53.347124 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m17:55:53.347634 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:55:53.348166 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m17:55:53.457454 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m17:55:53.467898 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m17:55:53.468765 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m17:55:53.469398 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:55:54.073386 [debug] [ThreadPool]: SQL status: OK in 0.6000000238418579 seconds
[0m17:55:54.076908 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m17:55:54.194987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e54cdca7-65f4-4346-b823-003c28d81365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633b9ae160>]}
[0m17:55:54.196091 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:55:54.196800 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:55:54.197836 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:55:54.198606 [info ] [MainThread]: 
[0m17:55:54.219154 [debug] [Thread-4  ]: Began running node model.my_dbt_project.genre_popularity
[0m17:55:54.220616 [info ] [Thread-4  ]: 1 of 5 START sql incremental model movielens_volume.genre_popularity ........... [RUN]
[0m17:55:54.222804 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.genre_popularity)
[0m17:55:54.224122 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.genre_popularity
[0m17:55:54.252783 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.genre_popularity"
[0m17:55:54.263512 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (compile): 17:55:54.224872 => 17:55:54.262723
[0m17:55:54.264348 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.genre_popularity
[0m17:55:54.315662 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:55:54.316459 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:55:54.317067 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m17:55:54.317951 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:55:55.209760 [debug] [Thread-4  ]: SQL status: OK in 0.8899999856948853 seconds
[0m17:55:55.237985 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:55:55.238932 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

  
    create temporary view `genre_popularity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings with Genres
-- Join silver ratings with silver movies to attach genres
-- Prepare rating timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        r.rating,
        r.rating_date as rating_timestamp,  
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Aggregate Genre Metrics
-- For each genre, calculate total ratings, avg rating,
-- and distribution insights
-- =======================================================
genre_stats as (
    select
        genres,  -- genres already formatted with spacing
        count(*) as rating_count,
        round(avg(rating), 2) as avg_rating,  
        to_date(min(rating_timestamp)) as first_rating_date,  
        to_date(max(rating_timestamp)) as last_rating_date   
    from ratings_base
    group by genres
),

-- =======================================================
-- Step 3: Ranking Genres
-- Rank genres by popularity (# of ratings)
-- and quality (avg rating)
-- =======================================================
ranked_genres as (
    select
        genres,
        rating_count,
        avg_rating,
        first_rating_date,
        last_rating_date,
        rank() over (order by rating_count desc) as popularity_rank,
        rank() over (order by avg_rating desc) as quality_rank
    from genre_stats
)

-- =======================================================
-- Step 4: Final Selection
-- One row per genre with all metrics
-- =======================================================
select
    genres,
    rating_count,
    avg_rating,
    popularity_rank,
    quality_rank,
    first_rating_date,
    last_rating_date
from ranked_genres


  -- Only insert new genres on incremental runs
  where genres not in (select genres from `workspace`.`movielens_volume`.`genre_popularity`)

  
[0m17:55:55.553817 [debug] [Thread-4  ]: SQL status: OK in 0.3100000023841858 seconds
[0m17:55:55.581029 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:55:55.581855 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m17:55:55.796649 [debug] [Thread-4  ]: SQL status: OK in 0.20999999344348907 seconds
[0m17:55:55.810823 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.genre_popularity"
[0m17:55:55.819621 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:55:55.820706 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`genre_popularity` as DBT_INTERNAL_DEST
      using `genre_popularity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.genres = DBT_INTERNAL_DEST.genres
          

      when matched then update set
         * 

      when not matched then insert *

[0m17:56:00.987875 [debug] [Thread-4  ]: SQL status: OK in 5.170000076293945 seconds
[0m17:56:01.022921 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (execute): 17:55:54.264909 => 17:56:01.022612
[0m17:56:01.023999 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: ROLLBACK
[0m17:56:01.024855 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:01.025482 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: Close
[0m17:56:01.142622 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e54cdca7-65f4-4346-b823-003c28d81365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633a64eac0>]}
[0m17:56:01.144043 [info ] [Thread-4  ]: 1 of 5 OK created sql incremental model movielens_volume.genre_popularity ...... [[32mOK[0m in 6.92s]
[0m17:56:01.145452 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.genre_popularity
[0m17:56:01.146328 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_enriched
[0m17:56:01.147461 [info ] [Thread-4  ]: 2 of 5 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m17:56:01.148951 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.genre_popularity, now model.my_dbt_project.movie_enriched)
[0m17:56:01.149699 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_enriched
[0m17:56:01.155282 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m17:56:01.162758 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (compile): 17:56:01.150213 => 17:56:01.162345
[0m17:56:01.163640 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_enriched
[0m17:56:01.180513 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:01.181238 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m17:56:01.181742 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

      describe extended `workspace`.`movielens_volume`.`movie_enriched`
  
[0m17:56:01.182390 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:56:02.106559 [debug] [Thread-4  ]: SQL status: OK in 0.9200000166893005 seconds
[0m17:56:02.153702 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m17:56:02.163148 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m17:56:02.163929 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m17:56:04.501058 [debug] [Thread-4  ]: SQL status: OK in 2.3399999141693115 seconds
[0m17:56:04.508848 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (execute): 17:56:01.164159 => 17:56:04.508589
[0m17:56:04.509592 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: ROLLBACK
[0m17:56:04.510190 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:04.510763 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: Close
[0m17:56:04.632682 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e54cdca7-65f4-4346-b823-003c28d81365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633a73d370>]}
[0m17:56:04.633915 [info ] [Thread-4  ]: 2 of 5 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 3.48s]
[0m17:56:04.635266 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_enriched
[0m17:56:04.636206 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_performance
[0m17:56:04.637106 [info ] [Thread-4  ]: 3 of 5 START sql incremental model movielens_volume.movie_performance .......... [RUN]
[0m17:56:04.638624 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_enriched, now model.my_dbt_project.movie_performance)
[0m17:56:04.639395 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_performance
[0m17:56:04.646714 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_performance"
[0m17:56:04.655794 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (compile): 17:56:04.639845 => 17:56:04.655203
[0m17:56:04.656589 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_performance
[0m17:56:04.667760 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:04.668512 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:56:04.669145 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m17:56:04.669873 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:56:05.608085 [debug] [Thread-4  ]: SQL status: OK in 0.9399999976158142 seconds
[0m17:56:05.613438 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:56:05.614501 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

  
    create temporary view `movie_performance__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings Join
-- Attach movie metadata (title, genres) to ratings
-- =======================================================
with ratings_base as (
    select
        r.movie_id,
        m.title,
        regexp_replace(m.genres, '\\|', ' | ') as genres,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_date
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Weighted Rating
-- Compute avg rating, rating count, weighted avg
-- =======================================================
weighted_rating as (
    select
        movie_id,
        title,
        genres,
        round(avg(rating), 2) as avg_rating,
        count(*) as rating_count,
        round(sum(rating) / count(*), 2) as weighted_avg_rating
    from ratings_base
    group by movie_id, title, genres
),

-- =======================================================
-- Step 3: Polarization Score
-- Standard deviation of ratings per movie
-- =======================================================
polarization as (
    select
        movie_id,
        round(coalesce(stddev(rating), 0.0), 2) as polarization_score
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 4: Top Rating Percentage
-- Percent of ratings >= 4.5
-- =======================================================
top_rating_pct as (
    select
        movie_id,
        round(100.0 * sum(case when rating >= 4.5 then 1 else 0 end) / count(*), 2) as top_rating_percentage
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 5: Genre Ranking
-- Rank movies within each genre by weighted_avg_rating
-- =======================================================
genre_rank as (
    select
        movie_id,
        genres,
        weighted_avg_rating,
        rank() over (partition by genres order by weighted_avg_rating desc) as genre_rank
    from weighted_rating
),

-- =======================================================
-- Step 6: Tag Count
-- Count distinct tags per movie
-- =======================================================
tag_count as (
    select
        movie_id,
        count(distinct tag) as tag_count
    from `workspace`.`movielens_volume`.`silver_tags`
    group by movie_id
)

-- =======================================================
-- Step 7: Final Selection
-- Combine all metrics into the gold table
-- =======================================================
select
    w.movie_id,
    w.title,
    w.genres,
    w.weighted_avg_rating,
    w.rating_count,
    g.genre_rank,
    p.polarization_score,
    tr.top_rating_percentage,
    coalesce(tc.tag_count, 0) as tag_count
from weighted_rating w
join genre_rank g
    on w.movie_id = g.movie_id
join polarization p
    on w.movie_id = p.movie_id
join top_rating_pct tr
    on w.movie_id = tr.movie_id
left join tag_count tc
    on w.movie_id = tc.movie_id


-- Use NOT EXISTS for safer incremental inserts
where not exists (
    select 1 
    from `workspace`.`movielens_volume`.`movie_performance` t
    where t.movie_id = w.movie_id
)

  
[0m17:56:05.915236 [debug] [Thread-4  ]: SQL status: OK in 0.30000001192092896 seconds
[0m17:56:05.920646 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:56:05.921354 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m17:56:06.155785 [debug] [Thread-4  ]: SQL status: OK in 0.23000000417232513 seconds
[0m17:56:06.160675 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_performance"
[0m17:56:06.168589 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:56:06.169376 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`movie_performance` as DBT_INTERNAL_DEST
      using `movie_performance__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.movie_id = DBT_INTERNAL_DEST.movie_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m17:56:10.575721 [debug] [Thread-4  ]: SQL status: OK in 4.409999847412109 seconds
[0m17:56:10.578872 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (execute): 17:56:04.657129 => 17:56:10.578580
[0m17:56:10.579589 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: ROLLBACK
[0m17:56:10.580192 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:10.580852 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: Close
[0m17:56:10.697004 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e54cdca7-65f4-4346-b823-003c28d81365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633a62f910>]}
[0m17:56:10.698138 [info ] [Thread-4  ]: 3 of 5 OK created sql incremental model movielens_volume.movie_performance ..... [[32mOK[0m in 6.06s]
[0m17:56:10.699828 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_performance
[0m17:56:10.700670 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_activity
[0m17:56:10.701537 [info ] [Thread-4  ]: 4 of 5 START sql incremental model movielens_volume.user_activity .............. [RUN]
[0m17:56:10.702997 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_performance, now model.my_dbt_project.user_activity)
[0m17:56:10.703596 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_activity
[0m17:56:10.710424 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_activity"
[0m17:56:10.717884 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (compile): 17:56:10.704063 => 17:56:10.717432
[0m17:56:10.718632 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_activity
[0m17:56:10.725370 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:10.726085 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:56:10.726765 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m17:56:10.727360 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:56:11.358047 [debug] [Thread-4  ]: SQL status: OK in 0.6299999952316284 seconds
[0m17:56:11.363463 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:56:11.364525 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

  
    create temporary view `user_activity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Join ratings with movies to attach genres and prepare timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_timestamp,
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: User Activity (General)
-- Calculate total ratings, average rating, first and last activity dates per user
-- =======================================================
user_activity as (
    select
        user_id,
        count(*) as user_rating_count,
        round(avg(rating), 2) as avg_user_rating,
        min(rating_timestamp) as first_activity_date,
        max(rating_timestamp) as last_activity_date
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 3: User Influence Score
-- Compute influence based on total ratings (more ratings = higher influence)
-- =======================================================
user_influence as (
    select
        user_id,
        user_rating_count as influence_score
    from user_activity
),

-- =======================================================
-- Step 4: Active Users by Genre
-- Count ratings per genre and rank genres per user
-- =======================================================
user_genre_activity as (
    select
        user_id,
        regexp_replace(genres, '\\|', ' | ') as genres,
        count(*) as genre_rating_count,
        rank() over (partition by user_id order by count(*) desc) as genre_rank
    from ratings_base
    group by user_id, genres
),

-- =======================================================
-- Step 5: New vs Loyal Users
-- Categorize users as new if first activity < 30 days ago, else loyal
-- =======================================================
user_type as (
    select
        user_id,
        case
            when min(rating_timestamp) >= date_sub(current_date(), 30)
                then 'new_user'
            else 'loyal_user'
        end as user_category
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 6: Top Genre per User
-- Identify the most active genre for each user
-- =======================================================
top_genre as (
    select
        user_id,
        max(genres) as top_genre
    from user_genre_activity
    group by user_id
)

-- =======================================================
-- Final Selection
-- Combine general activity, influence, type, and top genre per user
-- =======================================================
select
    ua.user_id,
    ua.user_rating_count,
    ua.avg_user_rating,
    ua.first_activity_date,
    ua.last_activity_date,
    ui.influence_score,
    ut.user_category,
    tg.top_genre
from user_activity ua
join user_influence ui on ua.user_id = ui.user_id
join user_type ut on ua.user_id = ut.user_id
left join top_genre tg on ua.user_id = tg.user_id


  where ua.user_id not in (select user_id from `workspace`.`movielens_volume`.`user_activity`)

  
[0m17:56:11.692246 [debug] [Thread-4  ]: SQL status: OK in 0.33000001311302185 seconds
[0m17:56:11.702253 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:56:11.703059 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m17:56:11.933057 [debug] [Thread-4  ]: SQL status: OK in 0.23000000417232513 seconds
[0m17:56:11.938594 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_activity"
[0m17:56:11.948518 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:56:11.949254 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_activity` as DBT_INTERNAL_DEST
      using `user_activity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m17:56:18.872394 [debug] [Thread-4  ]: SQL status: OK in 6.920000076293945 seconds
[0m17:56:19.000767 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (execute): 17:56:10.719116 => 17:56:19.000458
[0m17:56:19.001657 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: ROLLBACK
[0m17:56:19.002492 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:19.003226 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: Close
[0m17:56:19.110920 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e54cdca7-65f4-4346-b823-003c28d81365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633a6fbfd0>]}
[0m17:56:19.112428 [info ] [Thread-4  ]: 4 of 5 OK created sql incremental model movielens_volume.user_activity ......... [[32mOK[0m in 8.41s]
[0m17:56:19.114074 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_activity
[0m17:56:19.114912 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_preferences
[0m17:56:19.115809 [info ] [Thread-4  ]: 5 of 5 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m17:56:19.117331 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.user_activity, now model.my_dbt_project.user_preferences)
[0m17:56:19.118289 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_preferences
[0m17:56:19.126299 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m17:56:19.133689 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (compile): 17:56:19.118919 => 17:56:19.133265
[0m17:56:19.134409 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_preferences
[0m17:56:19.141391 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:19.142096 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m17:56:19.142602 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m17:56:19.143210 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:56:19.623451 [debug] [Thread-4  ]: SQL status: OK in 0.47999998927116394 seconds
[0m17:56:19.628991 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m17:56:19.630015 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create temporary view `user_preferences__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score,  -- influence based on total ratings
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )

  
[0m17:56:19.904263 [debug] [Thread-4  ]: SQL status: OK in 0.27000001072883606 seconds
[0m17:56:19.910307 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m17:56:19.911146 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m17:56:20.121696 [debug] [Thread-4  ]: SQL status: OK in 0.20999999344348907 seconds
[0m17:56:20.126433 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m17:56:20.135532 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m17:56:20.136250 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
      using `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m17:56:25.752443 [debug] [Thread-4  ]: SQL status: OK in 5.619999885559082 seconds
[0m17:56:25.887244 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (execute): 17:56:19.134907 => 17:56:25.886978
[0m17:56:25.888032 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: ROLLBACK
[0m17:56:25.888788 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:25.889596 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: Close
[0m17:56:26.004895 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e54cdca7-65f4-4346-b823-003c28d81365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633a6fbcd0>]}
[0m17:56:26.006426 [info ] [Thread-4  ]: 5 of 5 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 6.89s]
[0m17:56:26.007856 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_preferences
[0m17:56:26.010336 [debug] [MainThread]: On master: ROLLBACK
[0m17:56:26.010956 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:56:26.370521 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:56:26.371344 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:26.371983 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:56:26.372554 [debug] [MainThread]: On master: ROLLBACK
[0m17:56:26.373198 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:56:26.373740 [debug] [MainThread]: On master: Close
[0m17:56:26.478249 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:56:26.479035 [debug] [MainThread]: Connection 'model.my_dbt_project.user_preferences' was properly closed.
[0m17:56:26.481398 [info ] [MainThread]: 
[0m17:56:26.482760 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 34.79 seconds (34.79s).
[0m17:56:26.485626 [debug] [MainThread]: Command end result
[0m17:56:26.520644 [info ] [MainThread]: 
[0m17:56:26.521943 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:56:26.522937 [info ] [MainThread]: 
[0m17:56:26.523993 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m17:56:26.525983 [debug] [MainThread]: Command `dbt run` succeeded at 17:56:26.525707 after 37.40 seconds
[0m17:56:26.526928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6368847640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633b7d0640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633b83b4c0>]}
[0m17:56:26.527948 [debug] [MainThread]: Flushing usage events
[0m17:56:31.735903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0630836520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f062ed70190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f062ed708b0>]}


============================== 17:56:31.743996 | 7ba10f44-b4df-404b-8d54-120940ca16d1 ==============================
[0m17:56:31.743996 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:56:31.745284 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/opt/airflow/dbt', 'log_path': '/opt/airflow/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:56:32.804481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7ba10f44-b4df-404b-8d54-120940ca16d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f062ed70130>]}
[0m17:56:32.835414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7ba10f44-b4df-404b-8d54-120940ca16d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0603a97910>]}
[0m17:56:32.836349 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m17:56:32.882928 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:56:34.139821 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:56:34.140538 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:56:34.151484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7ba10f44-b4df-404b-8d54-120940ca16d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0603a4d370>]}
[0m17:56:34.185752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7ba10f44-b4df-404b-8d54-120940ca16d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f06038733a0>]}
[0m17:56:34.186709 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m17:56:34.187670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7ba10f44-b4df-404b-8d54-120940ca16d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0603873400>]}
[0m17:56:34.191474 [info ] [MainThread]: 
[0m17:56:34.192922 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:56:34.195736 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m17:56:34.202702 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m17:56:34.203522 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m17:56:34.204217 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:56:35.123020 [debug] [ThreadPool]: SQL status: OK in 0.9200000166893005 seconds
[0m17:56:35.126339 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m17:56:35.245121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7ba10f44-b4df-404b-8d54-120940ca16d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0603873070>]}
[0m17:56:35.246097 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:35.246745 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:56:35.247863 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:56:35.248677 [info ] [MainThread]: 
[0m17:56:35.264970 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m17:56:35.265801 [info ] [Thread-2  ]: 1 of 22 START test accepted_rating_range_silver_ratings_rating__5__0 ........... [RUN]
[0m17:56:35.266894 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff)
[0m17:56:35.267527 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m17:56:35.279129 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m17:56:35.288707 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (compile): 17:56:35.268032 => 17:56:35.288384
[0m17:56:35.289437 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m17:56:35.321940 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m17:56:35.331474 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:35.332145 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m17:56:35.332815 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`silver_ratings`
where rating < 0
   or rating > 5


      
    ) dbt_internal_test
[0m17:56:35.333593 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:36.596476 [debug] [Thread-2  ]: SQL status: OK in 1.2599999904632568 seconds
[0m17:56:36.604989 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (execute): 17:56:35.290048 => 17:56:36.604666
[0m17:56:36.605830 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: ROLLBACK
[0m17:56:36.606560 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:36.607254 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: Close
[0m17:56:36.720637 [info ] [Thread-2  ]: 1 of 22 PASS accepted_rating_range_silver_ratings_rating__5__0 ................. [[32mPASS[0m in 1.45s]
[0m17:56:36.722061 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m17:56:36.722983 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m17:56:36.723769 [info ] [Thread-2  ]: 2 of 22 START test not_null_silver_links_imdb_id ............................... [RUN]
[0m17:56:36.725132 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff, now test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51)
[0m17:56:36.725875 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m17:56:36.736180 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m17:56:36.745457 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (compile): 17:56:36.726401 => 17:56:36.745125
[0m17:56:36.746112 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m17:56:36.750076 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m17:56:36.759465 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:36.760044 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m17:56:36.760617 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



      
    ) dbt_internal_test
[0m17:56:36.761313 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:37.880549 [debug] [Thread-2  ]: SQL status: OK in 1.1200000047683716 seconds
[0m17:56:37.884709 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (execute): 17:56:36.746579 => 17:56:37.884307
[0m17:56:37.885515 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: ROLLBACK
[0m17:56:37.886127 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:37.886727 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m17:56:38.007484 [info ] [Thread-2  ]: 2 of 22 PASS not_null_silver_links_imdb_id ..................................... [[32mPASS[0m in 1.28s]
[0m17:56:38.008842 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m17:56:38.009618 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m17:56:38.010404 [info ] [Thread-2  ]: 3 of 22 START test not_null_silver_links_movie_id .............................. [RUN]
[0m17:56:38.011946 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51, now test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d)
[0m17:56:38.012611 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m17:56:38.022081 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m17:56:38.029537 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (compile): 17:56:38.013092 => 17:56:38.029174
[0m17:56:38.030207 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m17:56:38.034031 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m17:56:38.041642 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:38.042164 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m17:56:38.042703 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



      
    ) dbt_internal_test
[0m17:56:38.043413 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:38.791882 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m17:56:38.797471 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (execute): 17:56:38.030685 => 17:56:38.797059
[0m17:56:38.798531 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: ROLLBACK
[0m17:56:38.799321 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:38.800002 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m17:56:38.914036 [info ] [Thread-2  ]: 3 of 22 PASS not_null_silver_links_movie_id .................................... [[32mPASS[0m in 0.90s]
[0m17:56:38.915495 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m17:56:38.916320 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m17:56:38.917098 [info ] [Thread-2  ]: 4 of 22 START test not_null_silver_links_tmdb_id ............................... [RUN]
[0m17:56:38.918454 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d, now test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f)
[0m17:56:38.919332 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m17:56:38.931798 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m17:56:38.942020 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (compile): 17:56:38.919932 => 17:56:38.941569
[0m17:56:38.942827 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m17:56:38.947222 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m17:56:38.957936 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:38.958621 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m17:56:38.959171 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



      
    ) dbt_internal_test
[0m17:56:38.959787 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:40.107916 [debug] [Thread-2  ]: SQL status: OK in 1.149999976158142 seconds
[0m17:56:40.111802 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (execute): 17:56:38.943344 => 17:56:40.111501
[0m17:56:40.112520 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: ROLLBACK
[0m17:56:40.113282 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:40.113861 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m17:56:40.227554 [info ] [Thread-2  ]: 4 of 22 PASS not_null_silver_links_tmdb_id ..................................... [[32mPASS[0m in 1.31s]
[0m17:56:40.228925 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m17:56:40.229838 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m17:56:40.230649 [info ] [Thread-2  ]: 5 of 22 START test not_null_silver_movies_genres ............................... [RUN]
[0m17:56:40.232003 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f, now test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63)
[0m17:56:40.232855 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m17:56:40.241722 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m17:56:40.251064 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (compile): 17:56:40.233568 => 17:56:40.250705
[0m17:56:40.251770 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m17:56:40.255941 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m17:56:40.265336 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:40.266140 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m17:56:40.266857 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



      
    ) dbt_internal_test
[0m17:56:40.267587 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:41.175898 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m17:56:41.180112 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (execute): 17:56:40.252340 => 17:56:41.179763
[0m17:56:41.180959 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: ROLLBACK
[0m17:56:41.181726 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:41.182432 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m17:56:41.290596 [info ] [Thread-2  ]: 5 of 22 PASS not_null_silver_movies_genres ..................................... [[32mPASS[0m in 1.06s]
[0m17:56:41.292099 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m17:56:41.292996 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m17:56:41.293828 [info ] [Thread-2  ]: 6 of 22 START test not_null_silver_movies_movie_id ............................. [RUN]
[0m17:56:41.295271 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63, now test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e)
[0m17:56:41.296041 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m17:56:41.302769 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m17:56:41.312605 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (compile): 17:56:41.296549 => 17:56:41.312129
[0m17:56:41.313392 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m17:56:41.317864 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m17:56:41.327337 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:41.328054 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m17:56:41.328655 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



      
    ) dbt_internal_test
[0m17:56:41.329262 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:42.017688 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m17:56:42.021725 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (execute): 17:56:41.313938 => 17:56:42.021400
[0m17:56:42.022473 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: ROLLBACK
[0m17:56:42.023138 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:42.023759 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m17:56:42.137976 [info ] [Thread-2  ]: 6 of 22 PASS not_null_silver_movies_movie_id ................................... [[32mPASS[0m in 0.84s]
[0m17:56:42.139277 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m17:56:42.140092 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m17:56:42.140934 [info ] [Thread-2  ]: 7 of 22 START test not_null_silver_movies_title ................................ [RUN]
[0m17:56:42.142443 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e, now test.my_dbt_project.not_null_silver_movies_title.3a779bd2de)
[0m17:56:42.143200 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m17:56:42.156294 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m17:56:42.166649 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (compile): 17:56:42.143762 => 17:56:42.166211
[0m17:56:42.167464 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m17:56:42.171743 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m17:56:42.181199 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:42.181965 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m17:56:42.182633 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



      
    ) dbt_internal_test
[0m17:56:42.183300 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:43.082831 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m17:56:43.087255 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (execute): 17:56:42.167958 => 17:56:43.086928
[0m17:56:43.088036 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: ROLLBACK
[0m17:56:43.088718 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:43.089377 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m17:56:43.203636 [info ] [Thread-2  ]: 7 of 22 PASS not_null_silver_movies_title ...................................... [[32mPASS[0m in 1.06s]
[0m17:56:43.205210 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m17:56:43.206082 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m17:56:43.206804 [info ] [Thread-2  ]: 8 of 22 START test not_null_silver_ratings_movie_id ............................ [RUN]
[0m17:56:43.208147 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_title.3a779bd2de, now test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36)
[0m17:56:43.209022 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m17:56:43.215589 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m17:56:43.224502 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (compile): 17:56:43.209653 => 17:56:43.223965
[0m17:56:43.225476 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m17:56:43.229911 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m17:56:43.237659 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:43.238266 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m17:56:43.238872 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



      
    ) dbt_internal_test
[0m17:56:43.239491 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:44.356696 [debug] [Thread-2  ]: SQL status: OK in 1.1200000047683716 seconds
[0m17:56:44.361668 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (execute): 17:56:43.225955 => 17:56:44.361242
[0m17:56:44.362539 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: ROLLBACK
[0m17:56:44.363259 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:44.365922 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m17:56:44.473293 [info ] [Thread-2  ]: 8 of 22 PASS not_null_silver_ratings_movie_id .................................. [[32mPASS[0m in 1.27s]
[0m17:56:44.474901 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m17:56:44.475939 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m17:56:44.476941 [info ] [Thread-2  ]: 9 of 22 START test not_null_silver_ratings_rating .............................. [RUN]
[0m17:56:44.478364 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36, now test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a)
[0m17:56:44.479114 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m17:56:44.485723 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m17:56:44.496096 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (compile): 17:56:44.479585 => 17:56:44.495675
[0m17:56:44.496943 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m17:56:44.501393 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m17:56:44.511108 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:44.511685 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m17:56:44.512297 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



      
    ) dbt_internal_test
[0m17:56:44.512957 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:45.235283 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m17:56:45.239852 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (execute): 17:56:44.497509 => 17:56:45.239466
[0m17:56:45.240720 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: ROLLBACK
[0m17:56:45.241381 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:45.242083 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m17:56:45.358799 [info ] [Thread-2  ]: 9 of 22 PASS not_null_silver_ratings_rating .................................... [[32mPASS[0m in 0.88s]
[0m17:56:45.360302 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m17:56:45.361223 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m17:56:45.362152 [info ] [Thread-2  ]: 10 of 22 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m17:56:45.363378 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a, now test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f)
[0m17:56:45.364006 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m17:56:45.380261 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m17:56:45.390840 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (compile): 17:56:45.364555 => 17:56:45.390388
[0m17:56:45.391653 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m17:56:45.396019 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m17:56:45.406577 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:45.407302 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m17:56:45.407969 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



      
    ) dbt_internal_test
[0m17:56:45.408940 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:46.750861 [debug] [Thread-2  ]: SQL status: OK in 1.340000033378601 seconds
[0m17:56:46.755080 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (execute): 17:56:45.392153 => 17:56:46.754741
[0m17:56:46.755813 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: ROLLBACK
[0m17:56:46.756504 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:46.757178 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m17:56:46.865938 [info ] [Thread-2  ]: 10 of 22 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 1.50s]
[0m17:56:46.867309 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m17:56:46.868215 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m17:56:46.868873 [info ] [Thread-2  ]: 11 of 22 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m17:56:46.870223 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f, now test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3)
[0m17:56:46.870831 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m17:56:46.879151 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m17:56:46.887252 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (compile): 17:56:46.871290 => 17:56:46.886765
[0m17:56:46.888043 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m17:56:46.892173 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m17:56:46.901258 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:46.901988 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m17:56:46.902646 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



      
    ) dbt_internal_test
[0m17:56:46.903394 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:47.712181 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m17:56:47.717381 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (execute): 17:56:46.888509 => 17:56:47.716956
[0m17:56:47.718299 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: ROLLBACK
[0m17:56:47.719062 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:47.721012 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m17:56:47.829069 [info ] [Thread-2  ]: 11 of 22 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 0.96s]
[0m17:56:47.830530 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m17:56:47.831320 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m17:56:47.832049 [info ] [Thread-2  ]: 12 of 22 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m17:56:47.833403 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3, now test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3)
[0m17:56:47.834078 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m17:56:47.842920 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m17:56:47.853313 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (compile): 17:56:47.834680 => 17:56:47.852880
[0m17:56:47.854081 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m17:56:47.858857 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m17:56:47.868922 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:47.869710 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m17:56:47.870499 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



      
    ) dbt_internal_test
[0m17:56:47.871360 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:48.208363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f36656834f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3663bf0160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3663bf0880>]}


============================== 17:56:48.216538 | e596726c-23c7-4aae-92fe-c7f0043278f5 ==============================
[0m17:56:48.216538 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:56:48.217747 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/opt/airflow/dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:56:49.179450 [debug] [Thread-2  ]: SQL status: OK in 1.309999942779541 seconds
[0m17:56:49.187406 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (execute): 17:56:47.854664 => 17:56:49.186797
[0m17:56:49.188706 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: ROLLBACK
[0m17:56:49.189575 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:49.190322 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m17:56:49.312879 [info ] [Thread-2  ]: 12 of 22 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 1.48s]
[0m17:56:49.314405 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m17:56:49.315360 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m17:56:49.316322 [info ] [Thread-2  ]: 13 of 22 START test not_null_silver_tags_tag ................................... [RUN]
[0m17:56:49.317881 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3, now test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8)
[0m17:56:49.318677 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m17:56:49.332692 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m17:56:49.343412 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (compile): 17:56:49.319319 => 17:56:49.342971
[0m17:56:49.344436 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m17:56:49.349115 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m17:56:49.360225 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:49.361147 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m17:56:49.362034 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



      
    ) dbt_internal_test
[0m17:56:49.362785 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:49.437377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e596726c-23c7-4aae-92fe-c7f0043278f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3663bf0100>]}
[0m17:56:49.472059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e596726c-23c7-4aae-92fe-c7f0043278f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f363c95da90>]}
[0m17:56:49.473271 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m17:56:49.406104 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:56:49.970702 [debug] [Thread-2  ]: SQL status: OK in 0.6100000143051147 seconds
[0m17:56:49.976519 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (execute): 17:56:49.345017 => 17:56:49.976078
[0m17:56:49.977377 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: ROLLBACK
[0m17:56:49.978187 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:49.978900 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m17:56:50.093716 [info ] [Thread-2  ]: 13 of 22 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 0.78s]
[0m17:56:50.095302 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m17:56:50.096243 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m17:56:50.097229 [info ] [Thread-2  ]: 14 of 22 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m17:56:50.098691 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8, now test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86)
[0m17:56:50.099479 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m17:56:50.107206 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m17:56:50.118460 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (compile): 17:56:50.099976 => 17:56:50.117971
[0m17:56:50.119516 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m17:56:50.124577 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m17:56:50.137109 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:50.137962 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m17:56:50.138637 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



      
    ) dbt_internal_test
[0m17:56:50.139472 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:50.787301 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:56:50.788186 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:56:50.799831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e596726c-23c7-4aae-92fe-c7f0043278f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f363c6c5340>]}
[0m17:56:50.834990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e596726c-23c7-4aae-92fe-c7f0043278f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f363c82f370>]}
[0m17:56:50.836119 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m17:56:50.838864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e596726c-23c7-4aae-92fe-c7f0043278f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f363c82f3d0>]}
[0m17:56:50.843414 [info ] [MainThread]: 
[0m17:56:50.845004 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:56:50.845671 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m17:56:50.849162 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:56:50.850086 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:56:50.850805 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m17:56:50.850997 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (execute): 17:56:50.120117 => 17:56:50.850610
[0m17:56:50.851391 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:56:50.851895 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: ROLLBACK
[0m17:56:50.852853 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:50.853713 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m17:56:50.956986 [info ] [Thread-2  ]: 14 of 22 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 0.86s]
[0m17:56:50.958703 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m17:56:50.959844 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m17:56:50.960634 [info ] [Thread-2  ]: 15 of 22 START test not_null_silver_tags_user_id ............................... [RUN]
[0m17:56:50.962284 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86, now test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3)
[0m17:56:50.962973 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m17:56:50.969953 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m17:56:50.980653 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (compile): 17:56:50.963581 => 17:56:50.980180
[0m17:56:50.981557 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m17:56:50.985869 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m17:56:50.996184 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:50.996885 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m17:56:50.997489 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



      
    ) dbt_internal_test
[0m17:56:50.998236 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:51.405303 [debug] [ThreadPool]: SQL status: OK in 0.550000011920929 seconds
[0m17:56:51.407626 [debug] [ThreadPool]: On list_workspace: Close
[0m17:56:51.516127 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m17:56:51.517443 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m17:56:51.531745 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:51.532519 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m17:56:51.533116 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m17:56:51.533669 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:56:51.980292 [debug] [Thread-2  ]: SQL status: OK in 0.9800000190734863 seconds
[0m17:56:51.984140 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (execute): 17:56:50.982007 => 17:56:51.983837
[0m17:56:51.984875 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: ROLLBACK
[0m17:56:51.985745 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:51.986362 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m17:56:52.095056 [info ] [Thread-2  ]: 15 of 22 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 1.13s]
[0m17:56:52.096442 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m17:56:52.097437 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:56:52.098275 [info ] [Thread-2  ]: 16 of 22 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:56:52.099489 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m17:56:52.100191 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:56:52.120276 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:56:52.130286 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 17:56:52.100783 => 17:56:52.129867
[0m17:56:52.131042 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:56:52.135125 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:56:52.143929 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:52.144564 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:56:52.145207 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:56:52.145807 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:52.238032 [debug] [ThreadPool]: SQL status: OK in 0.699999988079071 seconds
[0m17:56:52.239635 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m17:56:52.240327 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m17:56:52.240920 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:56:52.241554 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m17:56:52.348084 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m17:56:52.357172 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m17:56:52.358128 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m17:56:52.358724 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:56:53.060177 [debug] [ThreadPool]: SQL status: OK in 0.699999988079071 seconds
[0m17:56:53.064350 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m17:56:53.173553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e596726c-23c7-4aae-92fe-c7f0043278f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f363c82f0a0>]}
[0m17:56:53.174840 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:53.175766 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:56:53.176857 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:56:53.177731 [info ] [MainThread]: 
[0m17:56:53.194305 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_links
[0m17:56:53.195485 [info ] [Thread-4  ]: 1 of 4 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m17:56:53.196980 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.silver_links)
[0m17:56:53.197641 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_links
[0m17:56:53.209155 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m17:56:53.218149 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (compile): 17:56:53.198193 => 17:56:53.217722
[0m17:56:53.218920 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_links
[0m17:56:53.241295 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:53.242425 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m17:56:53.243522 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

      describe extended `workspace`.`movielens_volume`.`silver_links`
  
[0m17:56:53.244350 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:56:53.306737 [debug] [Thread-2  ]: SQL status: OK in 1.159999966621399 seconds
[0m17:56:53.310625 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 17:56:52.131569 => 17:56:53.310212
[0m17:56:53.311674 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m17:56:53.312484 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:53.313244 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m17:56:53.429570 [info ] [Thread-2  ]: 16 of 22 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.33s]
[0m17:56:53.431106 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:56:53.432004 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:56:53.432807 [info ] [Thread-2  ]: 17 of 22 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:56:53.434167 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m17:56:53.435115 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:56:53.444174 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:56:53.454588 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 17:56:53.435736 => 17:56:53.454168
[0m17:56:53.455607 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:56:53.459899 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:56:53.470729 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:53.471623 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:56:53.472261 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:56:53.473072 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:53.872261 [debug] [Thread-4  ]: SQL status: OK in 0.6299999952316284 seconds
[0m17:56:53.937590 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m17:56:53.946558 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m17:56:53.947303 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
    using delta
      
      
      
      
      
      
      as
      

with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m17:56:54.411217 [debug] [Thread-2  ]: SQL status: OK in 0.9399999976158142 seconds
[0m17:56:54.415551 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 17:56:53.456300 => 17:56:54.415220
[0m17:56:54.416364 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m17:56:54.417146 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:54.417937 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m17:56:54.532800 [info ] [Thread-2  ]: 17 of 22 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.10s]
[0m17:56:54.534118 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:56:54.534946 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m17:56:54.535726 [info ] [Thread-2  ]: 18 of 22 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:56:54.536939 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32)
[0m17:56:54.537542 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m17:56:54.545138 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m17:56:54.554436 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (compile): 17:56:54.538004 => 17:56:54.554027
[0m17:56:54.555205 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m17:56:54.559108 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m17:56:54.568636 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:54.569328 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m17:56:54.570020 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:56:54.570764 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:55.562662 [debug] [Thread-2  ]: SQL status: OK in 0.9900000095367432 seconds
[0m17:56:55.567044 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (execute): 17:56:54.555675 => 17:56:55.566697
[0m17:56:55.567835 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: ROLLBACK
[0m17:56:55.568708 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:55.569268 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m17:56:55.681177 [info ] [Thread-2  ]: 18 of 22 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.14s]
[0m17:56:55.682610 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m17:56:55.683397 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m17:56:55.684128 [info ] [Thread-2  ]: 19 of 22 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:56:55.685363 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32, now test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd)
[0m17:56:55.686072 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m17:56:55.702809 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m17:56:55.712439 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (compile): 17:56:55.686649 => 17:56:55.712019
[0m17:56:55.713189 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m17:56:55.717192 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m17:56:55.726387 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:55.727086 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m17:56:55.727696 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:56:55.728516 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:56.175257 [debug] [Thread-4  ]: SQL status: OK in 2.2300000190734863 seconds
[0m17:56:56.210685 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (execute): 17:56:53.219378 => 17:56:56.210421
[0m17:56:56.211518 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: ROLLBACK
[0m17:56:56.212291 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:56.213017 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: Close
[0m17:56:56.321038 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e596726c-23c7-4aae-92fe-c7f0043278f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f363c4d1f40>]}
[0m17:56:56.322156 [info ] [Thread-4  ]: 1 of 4 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 3.12s]
[0m17:56:56.323213 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_links
[0m17:56:56.323969 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_movies
[0m17:56:56.324748 [info ] [Thread-4  ]: 2 of 4 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m17:56:56.326112 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_links, now model.my_dbt_project.silver_movies)
[0m17:56:56.326858 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_movies
[0m17:56:56.331858 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m17:56:56.340362 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (compile): 17:56:56.327324 => 17:56:56.339989
[0m17:56:56.341177 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_movies
[0m17:56:56.350852 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:56.351676 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m17:56:56.352224 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

      describe extended `workspace`.`movielens_volume`.`silver_movies`
  
[0m17:56:56.352902 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:56:57.087781 [debug] [Thread-4  ]: SQL status: OK in 0.7300000190734863 seconds
[0m17:56:57.094991 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m17:56:57.105091 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m17:56:57.106088 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m17:56:57.439941 [debug] [Thread-2  ]: SQL status: OK in 1.7100000381469727 seconds
[0m17:56:57.444715 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (execute): 17:56:55.713787 => 17:56:57.444387
[0m17:56:57.445574 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: ROLLBACK
[0m17:56:57.446413 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:57.447220 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m17:56:57.552158 [info ] [Thread-2  ]: 19 of 22 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.87s]
[0m17:56:57.553592 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m17:56:57.554400 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m17:56:57.555276 [info ] [Thread-2  ]: 20 of 22 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:56:57.556681 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd, now test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719)
[0m17:56:57.557352 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m17:56:57.565226 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m17:56:57.575503 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (compile): 17:56:57.557842 => 17:56:57.574951
[0m17:56:57.576249 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m17:56:57.580187 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m17:56:57.589855 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:57.590467 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m17:56:57.591032 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:56:57.591776 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:56:58.879094 [debug] [Thread-4  ]: SQL status: OK in 1.7699999809265137 seconds
[0m17:56:58.882947 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (execute): 17:56:56.341774 => 17:56:58.882680
[0m17:56:58.883708 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: ROLLBACK
[0m17:56:58.884427 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:56:58.885167 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: Close
[0m17:56:59.296395 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e596726c-23c7-4aae-92fe-c7f0043278f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f363c4e0df0>]}
[0m17:56:59.297700 [info ] [Thread-4  ]: 2 of 4 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 2.97s]
[0m17:56:59.299188 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_movies
[0m17:56:59.300196 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_ratings
[0m17:56:59.301061 [info ] [Thread-4  ]: 3 of 4 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m17:56:59.302415 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_movies, now model.my_dbt_project.silver_ratings)
[0m17:56:59.303061 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_ratings
[0m17:56:59.308723 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m17:56:59.317552 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (compile): 17:56:59.303596 => 17:56:59.317221
[0m17:56:59.318187 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_ratings
[0m17:56:59.324242 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:56:59.324776 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m17:56:59.325337 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

      describe extended `workspace`.`movielens_volume`.`silver_ratings`
  
[0m17:56:59.326044 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:56:59.970579 [debug] [Thread-4  ]: SQL status: OK in 0.6399999856948853 seconds
[0m17:56:59.976827 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m17:56:59.985526 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m17:56:59.986292 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m17:57:00.968247 [debug] [Thread-2  ]: SQL status: OK in 3.380000114440918 seconds
[0m17:57:00.972121 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (execute): 17:56:57.576718 => 17:57:00.971794
[0m17:57:00.972956 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: ROLLBACK
[0m17:57:00.973699 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:00.974427 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m17:57:01.094874 [info ] [Thread-2  ]: 20 of 22 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 3.54s]
[0m17:57:01.096172 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m17:57:01.097067 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m17:57:01.097859 [info ] [Thread-2  ]: 21 of 22 START test unique_silver_links_movie_id ............................... [RUN]
[0m17:57:01.099199 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719, now test.my_dbt_project.unique_silver_links_movie_id.c422609054)
[0m17:57:01.099886 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m17:57:01.111361 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m17:57:01.122197 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (compile): 17:57:01.100372 => 17:57:01.121754
[0m17:57:01.123040 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m17:57:01.127405 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m17:57:01.137173 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:01.137815 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m17:57:01.138678 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:57:01.139466 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:02.297122 [debug] [Thread-2  ]: SQL status: OK in 1.159999966621399 seconds
[0m17:57:02.301578 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (execute): 17:57:01.123823 => 17:57:02.301159
[0m17:57:02.302462 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: ROLLBACK
[0m17:57:02.303220 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:02.303846 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m17:57:02.416786 [info ] [Thread-2  ]: 21 of 22 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 1.32s]
[0m17:57:02.418291 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m17:57:02.419215 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m17:57:02.420037 [info ] [Thread-2  ]: 22 of 22 START test unique_silver_movies_movie_id .............................. [RUN]
[0m17:57:02.421391 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_silver_links_movie_id.c422609054, now test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085)
[0m17:57:02.422127 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m17:57:02.428505 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m17:57:02.438355 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (compile): 17:57:02.422723 => 17:57:02.438045
[0m17:57:02.438987 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m17:57:02.451457 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m17:57:02.460645 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:02.461344 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m17:57:02.461984 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:57:02.462630 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:03.768682 [debug] [Thread-4  ]: SQL status: OK in 3.7799999713897705 seconds
[0m17:57:03.772718 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (execute): 17:56:59.318652 => 17:57:03.772446
[0m17:57:03.773612 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: ROLLBACK
[0m17:57:03.774328 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:03.775020 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: Close
[0m17:57:03.889485 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e596726c-23c7-4aae-92fe-c7f0043278f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f363c4e03a0>]}
[0m17:57:03.890732 [info ] [Thread-4  ]: 3 of 4 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 4.59s]
[0m17:57:03.892265 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_ratings
[0m17:57:03.893007 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_tags
[0m17:57:03.893810 [info ] [Thread-4  ]: 4 of 4 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m17:57:03.895069 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_ratings, now model.my_dbt_project.silver_tags)
[0m17:57:03.895745 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_tags
[0m17:57:03.907006 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m17:57:03.916098 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (compile): 17:57:03.896288 => 17:57:03.915639
[0m17:57:03.916856 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_tags
[0m17:57:03.924045 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:03.924741 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m17:57:03.925348 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

      describe extended `workspace`.`movielens_volume`.`silver_tags`
  
[0m17:57:03.926052 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:57:03.977353 [debug] [Thread-2  ]: SQL status: OK in 1.5099999904632568 seconds
[0m17:57:03.985286 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (execute): 17:57:02.439446 => 17:57:03.984618
[0m17:57:03.986807 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: ROLLBACK
[0m17:57:03.987814 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:03.988484 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m17:57:04.096057 [info ] [Thread-2  ]: 22 of 22 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 1.67s]
[0m17:57:04.097373 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m17:57:04.100145 [debug] [MainThread]: On master: ROLLBACK
[0m17:57:04.100742 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:57:04.591686 [debug] [Thread-4  ]: SQL status: OK in 0.6700000166893005 seconds
[0m17:57:04.598253 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m17:57:04.607364 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m17:57:04.608151 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m17:57:04.750872 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:57:04.751873 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:04.752490 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:57:04.753377 [debug] [MainThread]: On master: ROLLBACK
[0m17:57:04.754068 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:57:04.754770 [debug] [MainThread]: On master: Close
[0m17:57:04.865107 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:57:04.865935 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085' was properly closed.
[0m17:57:04.868367 [info ] [MainThread]: 
[0m17:57:04.869840 [info ] [MainThread]: Finished running 22 tests in 0 hours 0 minutes and 30.68 seconds (30.68s).
[0m17:57:04.875605 [debug] [MainThread]: Command end result
[0m17:57:04.906144 [info ] [MainThread]: 
[0m17:57:04.907325 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:57:04.908191 [info ] [MainThread]: 
[0m17:57:04.909009 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=0 SKIP=0 TOTAL=22
[0m17:57:04.910385 [debug] [MainThread]: Command `dbt test` succeeded at 17:57:04.910238 after 33.33 seconds
[0m17:57:04.911140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0630836520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0603a97910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0603aa9490>]}
[0m17:57:04.911971 [debug] [MainThread]: Flushing usage events
[0m17:57:06.974406 [debug] [Thread-4  ]: SQL status: OK in 2.369999885559082 seconds
[0m17:57:06.978532 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (execute): 17:57:03.917394 => 17:57:06.978184
[0m17:57:06.979519 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: ROLLBACK
[0m17:57:06.980291 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:06.981220 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: Close
[0m17:57:07.107949 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e596726c-23c7-4aae-92fe-c7f0043278f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f363c4e0d60>]}
[0m17:57:07.109739 [info ] [Thread-4  ]: 4 of 4 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 3.21s]
[0m17:57:07.111510 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_tags
[0m17:57:07.115653 [debug] [MainThread]: On master: ROLLBACK
[0m17:57:07.117098 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:57:07.789746 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:57:07.790646 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:07.791223 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:57:07.791842 [debug] [MainThread]: On master: ROLLBACK
[0m17:57:07.792406 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:57:07.792973 [debug] [MainThread]: On master: Close
[0m17:57:07.907775 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:57:07.909166 [debug] [MainThread]: Connection 'model.my_dbt_project.silver_tags' was properly closed.
[0m17:57:07.914035 [info ] [MainThread]: 
[0m17:57:07.918858 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 17.07 seconds (17.07s).
[0m17:57:07.923522 [debug] [MainThread]: Command end result
[0m17:57:07.970710 [info ] [MainThread]: 
[0m17:57:07.972895 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:57:07.974405 [info ] [MainThread]: 
[0m17:57:07.976076 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m17:57:07.979781 [debug] [MainThread]: Command `dbt run` succeeded at 17:57:07.978542 after 19.92 seconds
[0m17:57:07.981317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f36656834f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f363ca93430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f363c972640>]}
[0m17:57:07.982543 [debug] [MainThread]: Flushing usage events
[0m17:57:09.861798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f477af43550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f47794ef1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f47794ef8e0>]}


============================== 17:57:09.870423 | 2c26c548-121a-48d8-9108-6c6c45192d97 ==============================
[0m17:57:09.870423 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:57:09.871417 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'debug': 'False', 'profiles_dir': '/opt/airflow/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:57:10.976404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2c26c548-121a-48d8-9108-6c6c45192d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f47794ef160>]}
[0m17:57:11.019226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2c26c548-121a-48d8-9108-6c6c45192d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f474e224850>]}
[0m17:57:11.020346 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m17:57:11.078577 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:57:12.403065 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:57:12.403870 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:57:12.415603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c26c548-121a-48d8-9108-6c6c45192d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f474e1db3a0>]}
[0m17:57:12.454988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c26c548-121a-48d8-9108-6c6c45192d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f474e1093d0>]}
[0m17:57:12.455964 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m17:57:12.456858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c26c548-121a-48d8-9108-6c6c45192d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f474e109430>]}
[0m17:57:12.462481 [info ] [MainThread]: 
[0m17:57:12.464077 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:57:12.467509 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m17:57:12.474942 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m17:57:12.475771 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m17:57:12.476343 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:57:13.491828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f810c236670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f810a76e280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f810a76e9d0>]}


============================== 17:57:13.500550 | d8f64344-1ea5-4316-96c4-e6f622528483 ==============================
[0m17:57:13.500550 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:57:13.501808 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:57:13.729209 [debug] [ThreadPool]: SQL status: OK in 1.25 seconds
[0m17:57:13.733259 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m17:57:13.850252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c26c548-121a-48d8-9108-6c6c45192d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f474e109040>]}
[0m17:57:13.851437 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:13.852128 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:57:13.853282 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:57:13.854036 [info ] [MainThread]: 
[0m17:57:13.873349 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m17:57:13.874308 [info ] [Thread-2  ]: 1 of 35 START test accepted_rating_range_genre_popularity_avg_rating__5__0 ..... [RUN]
[0m17:57:13.875630 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413)
[0m17:57:13.876397 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m17:57:13.897735 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m17:57:13.909676 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (compile): 17:57:13.876994 => 17:57:13.909219
[0m17:57:13.910458 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m17:57:13.939828 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m17:57:13.951569 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:13.952304 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m17:57:13.952912 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m17:57:13.953706 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:14.661488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd8f64344-1ea5-4316-96c4-e6f622528483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f810a76e220>]}
[0m17:57:14.696241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd8f64344-1ea5-4316-96c4-e6f622528483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80df4f9a60>]}
[0m17:57:14.697371 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m17:57:14.747598 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:57:15.046765 [debug] [Thread-2  ]: SQL status: OK in 1.090000033378601 seconds
[0m17:57:15.054555 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (execute): 17:57:13.910969 => 17:57:15.054191
[0m17:57:15.055315 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: ROLLBACK
[0m17:57:15.056105 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:15.056807 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: Close
[0m17:57:15.173326 [info ] [Thread-2  ]: 1 of 35 PASS accepted_rating_range_genre_popularity_avg_rating__5__0 ........... [[32mPASS[0m in 1.30s]
[0m17:57:15.174671 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m17:57:15.175530 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m17:57:15.176350 [info ] [Thread-2  ]: 2 of 35 START test accepted_rating_range_movie_enriched_avg_rating__5__0 ....... [RUN]
[0m17:57:15.177745 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413, now test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8)
[0m17:57:15.178593 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m17:57:15.185065 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m17:57:15.196932 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (compile): 17:57:15.179127 => 17:57:15.196397
[0m17:57:15.197778 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m17:57:15.202669 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m17:57:15.213040 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:15.214226 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m17:57:15.215047 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m17:57:15.215903 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:16.031026 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:57:16.031796 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:57:16.043301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd8f64344-1ea5-4316-96c4-e6f622528483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80df4854c0>]}
[0m17:57:16.078304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd8f64344-1ea5-4316-96c4-e6f622528483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80df3b44c0>]}
[0m17:57:16.079494 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m17:57:16.080817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8f64344-1ea5-4316-96c4-e6f622528483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80df3b4490>]}
[0m17:57:16.083891 [info ] [MainThread]: 
[0m17:57:16.085385 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:57:16.087890 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:57:16.088761 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:57:16.089336 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m17:57:16.089931 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:57:16.150674 [debug] [Thread-2  ]: SQL status: OK in 0.9300000071525574 seconds
[0m17:57:16.154852 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (execute): 17:57:15.198355 => 17:57:16.154548
[0m17:57:16.155581 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: ROLLBACK
[0m17:57:16.156552 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:16.157362 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: Close
[0m17:57:16.265701 [info ] [Thread-2  ]: 2 of 35 PASS accepted_rating_range_movie_enriched_avg_rating__5__0 ............. [[32mPASS[0m in 1.09s]
[0m17:57:16.267169 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m17:57:16.267991 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m17:57:16.268755 [info ] [Thread-2  ]: 3 of 35 START test accepted_rating_range_movie_performance_weighted_avg_rating__5__0  [RUN]
[0m17:57:16.270117 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8, now test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083)
[0m17:57:16.271025 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m17:57:16.280899 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m17:57:16.291254 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (compile): 17:57:16.271656 => 17:57:16.290766
[0m17:57:16.292303 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m17:57:16.296465 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m17:57:16.306913 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:16.307650 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m17:57:16.308271 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating < 0
   or weighted_avg_rating > 5


      
    ) dbt_internal_test
[0m17:57:16.308948 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:16.621939 [debug] [ThreadPool]: SQL status: OK in 0.5299999713897705 seconds
[0m17:57:16.624018 [debug] [ThreadPool]: On list_workspace: Close
[0m17:57:16.732161 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m17:57:16.733679 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m17:57:16.747185 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:16.748086 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m17:57:16.748849 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m17:57:16.749423 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:57:17.016625 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m17:57:17.021152 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (execute): 17:57:16.292799 => 17:57:17.020738
[0m17:57:17.022155 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: ROLLBACK
[0m17:57:17.023008 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:17.023790 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: Close
[0m17:57:17.129798 [info ] [Thread-2  ]: 3 of 35 PASS accepted_rating_range_movie_performance_weighted_avg_rating__5__0 . [[32mPASS[0m in 0.86s]
[0m17:57:17.131211 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m17:57:17.132177 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m17:57:17.133022 [info ] [Thread-2  ]: 4 of 35 START test accepted_rating_range_user_activity_avg_user_rating__5__0 ... [RUN]
[0m17:57:17.134386 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083, now test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a)
[0m17:57:17.135293 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m17:57:17.147468 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m17:57:17.158190 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (compile): 17:57:17.135903 => 17:57:17.157756
[0m17:57:17.158970 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m17:57:17.163092 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m17:57:17.174559 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:17.175508 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m17:57:17.176166 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating < 0
   or avg_user_rating > 5


      
    ) dbt_internal_test
[0m17:57:17.177063 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:17.400446 [debug] [ThreadPool]: SQL status: OK in 0.6499999761581421 seconds
[0m17:57:17.402514 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m17:57:17.403548 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m17:57:17.404285 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:57:17.404887 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m17:57:17.523724 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m17:57:17.534778 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m17:57:17.535839 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m17:57:17.536632 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:57:17.914256 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m17:57:17.919050 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (execute): 17:57:17.159443 => 17:57:17.918611
[0m17:57:17.919985 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: ROLLBACK
[0m17:57:17.920776 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:17.921491 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: Close
[0m17:57:18.029370 [info ] [Thread-2  ]: 4 of 35 PASS accepted_rating_range_user_activity_avg_user_rating__5__0 ......... [[32mPASS[0m in 0.90s]
[0m17:57:18.030849 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m17:57:18.031712 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m17:57:18.032452 [info ] [Thread-2  ]: 5 of 35 START test not_null_genre_popularity_avg_rating ........................ [RUN]
[0m17:57:18.033770 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a, now test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9)
[0m17:57:18.034441 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m17:57:18.044416 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m17:57:18.054712 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (compile): 17:57:18.035039 => 17:57:18.054286
[0m17:57:18.055514 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m17:57:18.059977 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m17:57:18.071266 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:18.072036 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m17:57:18.072722 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating is null



      
    ) dbt_internal_test
[0m17:57:18.073378 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:18.738512 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m17:57:18.742298 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m17:57:18.832023 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m17:57:18.836773 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (execute): 17:57:18.056039 => 17:57:18.836104
[0m17:57:18.837754 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: ROLLBACK
[0m17:57:18.838485 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:18.839163 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: Close
[0m17:57:18.850960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8f64344-1ea5-4316-96c4-e6f622528483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80df3bb9d0>]}
[0m17:57:18.852057 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:18.852613 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:57:18.853585 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:57:18.854276 [info ] [MainThread]: 
[0m17:57:18.871183 [debug] [Thread-4  ]: Began running node model.my_dbt_project.genre_popularity
[0m17:57:18.872448 [info ] [Thread-4  ]: 1 of 5 START sql incremental model movielens_volume.genre_popularity ........... [RUN]
[0m17:57:18.873810 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.genre_popularity)
[0m17:57:18.874438 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.genre_popularity
[0m17:57:18.893777 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.genre_popularity"
[0m17:57:18.902948 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (compile): 17:57:18.874958 => 17:57:18.902511
[0m17:57:18.903710 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.genre_popularity
[0m17:57:18.951163 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:18.951955 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:57:18.952549 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m17:57:18.952478 [info ] [Thread-2  ]: 5 of 35 PASS not_null_genre_popularity_avg_rating .............................. [[32mPASS[0m in 0.92s]
[0m17:57:18.953328 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:57:18.953715 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m17:57:18.954534 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m17:57:18.955307 [info ] [Thread-2  ]: 6 of 35 START test not_null_genre_popularity_first_rating_date ................. [RUN]
[0m17:57:18.956813 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9, now test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32)
[0m17:57:18.957564 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m17:57:18.964302 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m17:57:18.974706 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (compile): 17:57:18.958089 => 17:57:18.974260
[0m17:57:18.975480 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m17:57:18.979969 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m17:57:18.990761 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:18.991575 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m17:57:18.992586 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where first_rating_date is null



      
    ) dbt_internal_test
[0m17:57:18.993290 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:19.551938 [debug] [Thread-4  ]: SQL status: OK in 0.6000000238418579 seconds
[0m17:57:19.580312 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:57:19.581241 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

  
    create temporary view `genre_popularity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings with Genres
-- Join silver ratings with silver movies to attach genres
-- Prepare rating timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        r.rating,
        r.rating_date as rating_timestamp,  
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Aggregate Genre Metrics
-- For each genre, calculate total ratings, avg rating,
-- and distribution insights
-- =======================================================
genre_stats as (
    select
        genres,  -- genres already formatted with spacing
        count(*) as rating_count,
        round(avg(rating), 2) as avg_rating,  
        to_date(min(rating_timestamp)) as first_rating_date,  
        to_date(max(rating_timestamp)) as last_rating_date   
    from ratings_base
    group by genres
),

-- =======================================================
-- Step 3: Ranking Genres
-- Rank genres by popularity (# of ratings)
-- and quality (avg rating)
-- =======================================================
ranked_genres as (
    select
        genres,
        rating_count,
        avg_rating,
        first_rating_date,
        last_rating_date,
        rank() over (order by rating_count desc) as popularity_rank,
        rank() over (order by avg_rating desc) as quality_rank
    from genre_stats
)

-- =======================================================
-- Step 4: Final Selection
-- One row per genre with all metrics
-- =======================================================
select
    genres,
    rating_count,
    avg_rating,
    popularity_rank,
    quality_rank,
    first_rating_date,
    last_rating_date
from ranked_genres


  -- Only insert new genres on incremental runs
  where genres not in (select genres from `workspace`.`movielens_volume`.`genre_popularity`)

  
[0m17:57:19.627282 [debug] [Thread-2  ]: SQL status: OK in 0.6299999952316284 seconds
[0m17:57:19.631869 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (execute): 17:57:18.975958 => 17:57:19.631505
[0m17:57:19.632723 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: ROLLBACK
[0m17:57:19.633388 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:19.633986 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: Close
[0m17:57:19.740666 [info ] [Thread-2  ]: 6 of 35 PASS not_null_genre_popularity_first_rating_date ....................... [[32mPASS[0m in 0.78s]
[0m17:57:19.742068 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m17:57:19.742981 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m17:57:19.743718 [info ] [Thread-2  ]: 7 of 35 START test not_null_genre_popularity_genres ............................ [RUN]
[0m17:57:19.745159 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32, now test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa)
[0m17:57:19.745899 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m17:57:19.760512 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m17:57:19.770869 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (compile): 17:57:19.746417 => 17:57:19.770360
[0m17:57:19.771715 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m17:57:19.776070 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m17:57:19.785759 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:19.786424 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m17:57:19.786968 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`genre_popularity`
where genres is null



      
    ) dbt_internal_test
[0m17:57:19.787571 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:19.864660 [debug] [Thread-4  ]: SQL status: OK in 0.2800000011920929 seconds
[0m17:57:19.894106 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:57:19.894963 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m17:57:20.100232 [debug] [Thread-4  ]: SQL status: OK in 0.20000000298023224 seconds
[0m17:57:20.112157 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.genre_popularity"
[0m17:57:20.120851 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:57:20.121621 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`genre_popularity` as DBT_INTERNAL_DEST
      using `genre_popularity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.genres = DBT_INTERNAL_DEST.genres
          

      when matched then update set
         * 

      when not matched then insert *

[0m17:57:20.611084 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m17:57:20.615164 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (execute): 17:57:19.772382 => 17:57:20.614814
[0m17:57:20.615963 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: ROLLBACK
[0m17:57:20.616698 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:20.617379 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: Close
[0m17:57:20.724743 [info ] [Thread-2  ]: 7 of 35 PASS not_null_genre_popularity_genres .................................. [[32mPASS[0m in 0.98s]
[0m17:57:20.726123 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m17:57:20.726964 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m17:57:20.727779 [info ] [Thread-2  ]: 8 of 35 START test not_null_genre_popularity_last_rating_date .................. [RUN]
[0m17:57:20.729176 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa, now test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432)
[0m17:57:20.729892 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m17:57:20.738501 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m17:57:20.748974 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (compile): 17:57:20.730370 => 17:57:20.748507
[0m17:57:20.749689 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m17:57:20.753502 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m17:57:20.762711 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:20.763393 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m17:57:20.763923 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where last_rating_date is null



      
    ) dbt_internal_test
[0m17:57:20.764502 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:21.445826 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m17:57:21.450386 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (execute): 17:57:20.750121 => 17:57:21.450005
[0m17:57:21.451284 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: ROLLBACK
[0m17:57:21.452150 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:21.452873 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: Close
[0m17:57:21.593209 [info ] [Thread-2  ]: 8 of 35 PASS not_null_genre_popularity_last_rating_date ........................ [[32mPASS[0m in 0.86s]
[0m17:57:21.594634 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m17:57:21.595457 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m17:57:21.596172 [info ] [Thread-2  ]: 9 of 35 START test not_null_genre_popularity_popularity_rank ................... [RUN]
[0m17:57:21.597585 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432, now test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc)
[0m17:57:21.598488 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m17:57:21.607194 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m17:57:21.617923 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (compile): 17:57:21.599089 => 17:57:21.617163
[0m17:57:21.618840 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m17:57:21.622603 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m17:57:21.632264 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:21.632949 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m17:57:21.633593 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select popularity_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where popularity_rank is null



      
    ) dbt_internal_test
[0m17:57:21.634238 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:22.486877 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m17:57:22.490936 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (execute): 17:57:21.619309 => 17:57:22.490601
[0m17:57:22.491740 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: ROLLBACK
[0m17:57:22.492631 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:22.493486 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: Close
[0m17:57:22.592705 [info ] [Thread-2  ]: 9 of 35 PASS not_null_genre_popularity_popularity_rank ......................... [[32mPASS[0m in 1.00s]
[0m17:57:22.594315 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m17:57:22.595204 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m17:57:22.596038 [info ] [Thread-2  ]: 10 of 35 START test not_null_genre_popularity_quality_rank ..................... [RUN]
[0m17:57:22.597310 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc, now test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a)
[0m17:57:22.598021 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m17:57:22.608279 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m17:57:22.618007 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (compile): 17:57:22.598673 => 17:57:22.617555
[0m17:57:22.618702 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m17:57:22.623565 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m17:57:22.633311 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:22.633953 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m17:57:22.634483 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select quality_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where quality_rank is null



      
    ) dbt_internal_test
[0m17:57:22.635177 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:23.413000 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m17:57:23.417362 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (execute): 17:57:22.619204 => 17:57:23.416955
[0m17:57:23.418141 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: ROLLBACK
[0m17:57:23.418791 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:23.419362 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: Close
[0m17:57:23.529219 [info ] [Thread-2  ]: 10 of 35 PASS not_null_genre_popularity_quality_rank ........................... [[32mPASS[0m in 0.93s]
[0m17:57:23.530661 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m17:57:23.531615 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m17:57:23.532498 [info ] [Thread-2  ]: 11 of 35 START test not_null_genre_popularity_rating_count ..................... [RUN]
[0m17:57:23.534245 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a, now test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856)
[0m17:57:23.534965 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m17:57:23.541490 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m17:57:23.549853 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (compile): 17:57:23.535420 => 17:57:23.549427
[0m17:57:23.550577 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m17:57:23.554587 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m17:57:23.562302 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:23.562937 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m17:57:23.563500 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`genre_popularity`
where rating_count is null



      
    ) dbt_internal_test
[0m17:57:23.564175 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:24.282659 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m17:57:24.288589 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (execute): 17:57:23.551083 => 17:57:24.288066
[0m17:57:24.289769 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: ROLLBACK
[0m17:57:24.290706 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:24.291431 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: Close
[0m17:57:24.406847 [info ] [Thread-2  ]: 11 of 35 PASS not_null_genre_popularity_rating_count ........................... [[32mPASS[0m in 0.87s]
[0m17:57:24.408218 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m17:57:24.409048 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m17:57:24.409883 [info ] [Thread-2  ]: 12 of 35 START test not_null_movie_enriched_avg_rating ......................... [RUN]
[0m17:57:24.411160 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856, now test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd)
[0m17:57:24.411822 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m17:57:24.418292 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m17:57:24.429062 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (compile): 17:57:24.412376 => 17:57:24.428617
[0m17:57:24.429826 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m17:57:24.434846 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m17:57:24.445706 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:24.446434 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m17:57:24.447068 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating is null



      
    ) dbt_internal_test
[0m17:57:24.447842 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:25.176714 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m17:57:25.180925 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (execute): 17:57:24.430331 => 17:57:25.180577
[0m17:57:25.181788 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: ROLLBACK
[0m17:57:25.182752 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:25.183582 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: Close
[0m17:57:25.288116 [info ] [Thread-2  ]: 12 of 35 PASS not_null_movie_enriched_avg_rating ............................... [[32mPASS[0m in 0.88s]
[0m17:57:25.289754 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m17:57:25.290751 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m17:57:25.291570 [info ] [Thread-2  ]: 13 of 35 START test not_null_movie_enriched_genres ............................. [RUN]
[0m17:57:25.292956 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd, now test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27)
[0m17:57:25.293762 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m17:57:25.300396 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m17:57:25.310727 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (compile): 17:57:25.294314 => 17:57:25.310302
[0m17:57:25.311674 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m17:57:25.326517 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m17:57:25.336491 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:25.337392 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m17:57:25.338059 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`movie_enriched`
where genres is null



      
    ) dbt_internal_test
[0m17:57:25.338680 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:26.253435 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m17:57:26.258281 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (execute): 17:57:25.312387 => 17:57:26.257925
[0m17:57:26.259145 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: ROLLBACK
[0m17:57:26.259973 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:26.260618 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: Close
[0m17:57:26.660737 [info ] [Thread-2  ]: 13 of 35 PASS not_null_movie_enriched_genres ................................... [[32mPASS[0m in 1.37s]
[0m17:57:26.662198 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m17:57:26.663171 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m17:57:26.664198 [info ] [Thread-2  ]: 14 of 35 START test not_null_movie_enriched_movie_id ........................... [RUN]
[0m17:57:26.665563 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27, now test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86)
[0m17:57:26.666342 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m17:57:26.673017 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m17:57:26.683801 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (compile): 17:57:26.666888 => 17:57:26.683311
[0m17:57:26.684944 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m17:57:26.689333 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m17:57:26.699658 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:26.700388 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m17:57:26.700984 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is null



      
    ) dbt_internal_test
[0m17:57:26.701631 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:27.480923 [debug] [Thread-4  ]: SQL status: OK in 7.360000133514404 seconds
[0m17:57:27.649051 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (execute): 17:57:18.904166 => 17:57:27.648784
[0m17:57:27.649902 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: ROLLBACK
[0m17:57:27.650527 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:27.651090 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: Close
[0m17:57:27.762286 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8f64344-1ea5-4316-96c4-e6f622528483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80de103430>]}
[0m17:57:27.763485 [info ] [Thread-4  ]: 1 of 5 OK created sql incremental model movielens_volume.genre_popularity ...... [[32mOK[0m in 8.89s]
[0m17:57:27.764718 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.genre_popularity
[0m17:57:27.765859 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_enriched
[0m17:57:27.766899 [info ] [Thread-4  ]: 2 of 5 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m17:57:27.768332 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.genre_popularity, now model.my_dbt_project.movie_enriched)
[0m17:57:27.769160 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_enriched
[0m17:57:27.775262 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m17:57:27.783944 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (compile): 17:57:27.769833 => 17:57:27.783539
[0m17:57:27.784686 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_enriched
[0m17:57:27.801200 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:27.801947 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m17:57:27.802546 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

      describe extended `workspace`.`movielens_volume`.`movie_enriched`
  
[0m17:57:27.803305 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:57:28.044398 [debug] [Thread-2  ]: SQL status: OK in 1.340000033378601 seconds
[0m17:57:28.048959 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (execute): 17:57:26.685804 => 17:57:28.048628
[0m17:57:28.049776 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: ROLLBACK
[0m17:57:28.050774 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:28.051575 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: Close
[0m17:57:28.173833 [info ] [Thread-2  ]: 14 of 35 PASS not_null_movie_enriched_movie_id ................................. [[32mPASS[0m in 1.51s]
[0m17:57:28.175426 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m17:57:28.176483 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m17:57:28.177261 [info ] [Thread-2  ]: 15 of 35 START test not_null_movie_enriched_title .............................. [RUN]
[0m17:57:28.178451 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86, now test.my_dbt_project.not_null_movie_enriched_title.d20f00c292)
[0m17:57:28.179100 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m17:57:28.185646 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m17:57:28.196177 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (compile): 17:57:28.179567 => 17:57:28.195634
[0m17:57:28.197197 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m17:57:28.201318 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m17:57:28.211514 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:28.212381 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m17:57:28.213060 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`movie_enriched`
where title is null



      
    ) dbt_internal_test
[0m17:57:28.213894 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:28.875063 [debug] [Thread-4  ]: SQL status: OK in 1.0700000524520874 seconds
[0m17:57:28.923728 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m17:57:28.933451 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m17:57:28.934396 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m17:57:29.385982 [debug] [Thread-2  ]: SQL status: OK in 1.1699999570846558 seconds
[0m17:57:29.390253 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (execute): 17:57:28.197716 => 17:57:29.389933
[0m17:57:29.391089 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: ROLLBACK
[0m17:57:29.391883 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:29.392623 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: Close
[0m17:57:29.504449 [info ] [Thread-2  ]: 15 of 35 PASS not_null_movie_enriched_title .................................... [[32mPASS[0m in 1.33s]
[0m17:57:29.506204 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m17:57:29.506990 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m17:57:29.507730 [info ] [Thread-2  ]: 16 of 35 START test not_null_movie_enriched_total_ratings ...................... [RUN]
[0m17:57:29.508960 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_title.d20f00c292, now test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04)
[0m17:57:29.509538 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m17:57:29.515756 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m17:57:29.526358 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (compile): 17:57:29.509993 => 17:57:29.525856
[0m17:57:29.527193 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m17:57:29.542409 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m17:57:29.553250 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:29.554230 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m17:57:29.554920 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select total_ratings
from `workspace`.`movielens_volume`.`movie_enriched`
where total_ratings is null



      
    ) dbt_internal_test
[0m17:57:29.555672 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:30.359311 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m17:57:30.363730 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (execute): 17:57:29.527703 => 17:57:30.363372
[0m17:57:30.364567 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: ROLLBACK
[0m17:57:30.365350 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:30.365926 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: Close
[0m17:57:30.474011 [info ] [Thread-2  ]: 16 of 35 PASS not_null_movie_enriched_total_ratings ............................ [[32mPASS[0m in 0.97s]
[0m17:57:30.475458 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m17:57:30.476259 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m17:57:30.477149 [info ] [Thread-2  ]: 17 of 35 START test not_null_movie_performance_genre_rank ...................... [RUN]
[0m17:57:30.478434 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04, now test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e)
[0m17:57:30.479403 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m17:57:30.489894 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m17:57:30.500180 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (compile): 17:57:30.480157 => 17:57:30.499641
[0m17:57:30.501105 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m17:57:30.505826 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m17:57:30.516297 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:30.516969 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m17:57:30.517543 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genre_rank
from `workspace`.`movielens_volume`.`movie_performance`
where genre_rank is null



      
    ) dbt_internal_test
[0m17:57:30.518144 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:31.606767 [debug] [Thread-2  ]: SQL status: OK in 1.090000033378601 seconds
[0m17:57:31.611014 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (execute): 17:57:30.501659 => 17:57:31.610650
[0m17:57:31.611978 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: ROLLBACK
[0m17:57:31.612682 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:31.613311 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: Close
[0m17:57:31.731630 [info ] [Thread-2  ]: 17 of 35 PASS not_null_movie_performance_genre_rank ............................ [[32mPASS[0m in 1.25s]
[0m17:57:31.732971 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m17:57:31.733889 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m17:57:31.734972 [info ] [Thread-2  ]: 18 of 35 START test not_null_movie_performance_movie_id ........................ [RUN]
[0m17:57:31.736490 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e, now test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52)
[0m17:57:31.737349 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m17:57:31.747287 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m17:57:31.758761 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (compile): 17:57:31.737869 => 17:57:31.758303
[0m17:57:31.759578 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m17:57:31.765018 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m17:57:31.775301 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:31.776037 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m17:57:31.776608 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is null



      
    ) dbt_internal_test
[0m17:57:31.777439 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:32.603595 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m17:57:32.607615 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (execute): 17:57:31.760262 => 17:57:32.607321
[0m17:57:32.608374 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: ROLLBACK
[0m17:57:32.609170 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:32.609775 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: Close
[0m17:57:32.721895 [info ] [Thread-2  ]: 18 of 35 PASS not_null_movie_performance_movie_id .............................. [[32mPASS[0m in 0.99s]
[0m17:57:32.723344 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m17:57:32.724140 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m17:57:32.724956 [info ] [Thread-2  ]: 19 of 35 START test not_null_movie_performance_polarization_score .............. [RUN]
[0m17:57:32.726104 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52, now test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6)
[0m17:57:32.726900 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m17:57:32.735996 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m17:57:32.756243 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (compile): 17:57:32.727581 => 17:57:32.755701
[0m17:57:32.757015 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m17:57:32.762553 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m17:57:32.772748 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:32.773519 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m17:57:32.774175 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select polarization_score
from `workspace`.`movielens_volume`.`movie_performance`
where polarization_score is null



      
    ) dbt_internal_test
[0m17:57:32.774906 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:33.143597 [debug] [Thread-4  ]: SQL status: OK in 4.210000038146973 seconds
[0m17:57:33.151867 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (execute): 17:57:27.785243 => 17:57:33.151602
[0m17:57:33.152692 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: ROLLBACK
[0m17:57:33.153440 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:33.154159 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: Close
[0m17:57:33.273375 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8f64344-1ea5-4316-96c4-e6f622528483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80df2aedf0>]}
[0m17:57:33.274872 [info ] [Thread-4  ]: 2 of 5 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 5.51s]
[0m17:57:33.276120 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_enriched
[0m17:57:33.276928 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_performance
[0m17:57:33.277760 [info ] [Thread-4  ]: 3 of 5 START sql incremental model movielens_volume.movie_performance .......... [RUN]
[0m17:57:33.278997 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_enriched, now model.my_dbt_project.movie_performance)
[0m17:57:33.279821 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_performance
[0m17:57:33.288103 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_performance"
[0m17:57:33.297791 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (compile): 17:57:33.280395 => 17:57:33.297242
[0m17:57:33.298790 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_performance
[0m17:57:33.311098 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:33.311863 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:57:33.312522 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m17:57:33.313075 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:57:33.528147 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m17:57:33.533177 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (execute): 17:57:32.757490 => 17:57:33.532605
[0m17:57:33.534121 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: ROLLBACK
[0m17:57:33.534873 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:33.535642 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: Close
[0m17:57:33.650770 [info ] [Thread-2  ]: 19 of 35 PASS not_null_movie_performance_polarization_score .................... [[32mPASS[0m in 0.92s]
[0m17:57:33.652443 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m17:57:33.653263 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m17:57:33.653955 [info ] [Thread-2  ]: 20 of 35 START test not_null_movie_performance_rating_count .................... [RUN]
[0m17:57:33.655115 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6, now test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac)
[0m17:57:33.655746 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m17:57:33.664704 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m17:57:33.675149 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (compile): 17:57:33.656262 => 17:57:33.674569
[0m17:57:33.675890 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m17:57:33.681262 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m17:57:33.692471 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:33.693258 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m17:57:33.693870 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`movie_performance`
where rating_count is null



      
    ) dbt_internal_test
[0m17:57:33.694525 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:33.923587 [debug] [Thread-4  ]: SQL status: OK in 0.6100000143051147 seconds
[0m17:57:33.928593 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:57:33.929548 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

  
    create temporary view `movie_performance__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings Join
-- Attach movie metadata (title, genres) to ratings
-- =======================================================
with ratings_base as (
    select
        r.movie_id,
        m.title,
        regexp_replace(m.genres, '\\|', ' | ') as genres,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_date
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Weighted Rating
-- Compute avg rating, rating count, weighted avg
-- =======================================================
weighted_rating as (
    select
        movie_id,
        title,
        genres,
        round(avg(rating), 2) as avg_rating,
        count(*) as rating_count,
        round(sum(rating) / count(*), 2) as weighted_avg_rating
    from ratings_base
    group by movie_id, title, genres
),

-- =======================================================
-- Step 3: Polarization Score
-- Standard deviation of ratings per movie
-- =======================================================
polarization as (
    select
        movie_id,
        round(coalesce(stddev(rating), 0.0), 2) as polarization_score
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 4: Top Rating Percentage
-- Percent of ratings >= 4.5
-- =======================================================
top_rating_pct as (
    select
        movie_id,
        round(100.0 * sum(case when rating >= 4.5 then 1 else 0 end) / count(*), 2) as top_rating_percentage
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 5: Genre Ranking
-- Rank movies within each genre by weighted_avg_rating
-- =======================================================
genre_rank as (
    select
        movie_id,
        genres,
        weighted_avg_rating,
        rank() over (partition by genres order by weighted_avg_rating desc) as genre_rank
    from weighted_rating
),

-- =======================================================
-- Step 6: Tag Count
-- Count distinct tags per movie
-- =======================================================
tag_count as (
    select
        movie_id,
        count(distinct tag) as tag_count
    from `workspace`.`movielens_volume`.`silver_tags`
    group by movie_id
)

-- =======================================================
-- Step 7: Final Selection
-- Combine all metrics into the gold table
-- =======================================================
select
    w.movie_id,
    w.title,
    w.genres,
    w.weighted_avg_rating,
    w.rating_count,
    g.genre_rank,
    p.polarization_score,
    tr.top_rating_percentage,
    coalesce(tc.tag_count, 0) as tag_count
from weighted_rating w
join genre_rank g
    on w.movie_id = g.movie_id
join polarization p
    on w.movie_id = p.movie_id
join top_rating_pct tr
    on w.movie_id = tr.movie_id
left join tag_count tc
    on w.movie_id = tc.movie_id


-- Use NOT EXISTS for safer incremental inserts
where not exists (
    select 1 
    from `workspace`.`movielens_volume`.`movie_performance` t
    where t.movie_id = w.movie_id
)

  
[0m17:57:34.284289 [debug] [Thread-4  ]: SQL status: OK in 0.3499999940395355 seconds
[0m17:57:34.291386 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:57:34.292423 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m17:57:34.460926 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m17:57:34.465332 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (execute): 17:57:33.676355 => 17:57:34.464977
[0m17:57:34.466171 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: ROLLBACK
[0m17:57:34.466910 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:34.467541 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: Close
[0m17:57:34.539995 [debug] [Thread-4  ]: SQL status: OK in 0.25 seconds
[0m17:57:34.544607 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_performance"
[0m17:57:34.553794 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:57:34.554761 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`movie_performance` as DBT_INTERNAL_DEST
      using `movie_performance__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.movie_id = DBT_INTERNAL_DEST.movie_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m17:57:34.589163 [info ] [Thread-2  ]: 20 of 35 PASS not_null_movie_performance_rating_count .......................... [[32mPASS[0m in 0.93s]
[0m17:57:34.590602 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m17:57:34.591388 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m17:57:34.592219 [info ] [Thread-2  ]: 21 of 35 START test not_null_movie_performance_top_rating_percentage ........... [RUN]
[0m17:57:34.593630 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac, now test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb)
[0m17:57:34.594276 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m17:57:34.602355 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m17:57:34.613203 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (compile): 17:57:34.594891 => 17:57:34.612745
[0m17:57:34.614089 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m17:57:34.618501 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m17:57:34.628698 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:34.629501 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m17:57:34.630191 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_rating_percentage
from `workspace`.`movielens_volume`.`movie_performance`
where top_rating_percentage is null



      
    ) dbt_internal_test
[0m17:57:34.630964 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:35.821799 [debug] [Thread-2  ]: SQL status: OK in 1.190000057220459 seconds
[0m17:57:35.825911 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (execute): 17:57:34.614754 => 17:57:35.825567
[0m17:57:35.826722 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: ROLLBACK
[0m17:57:35.827441 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:35.828186 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: Close
[0m17:57:35.937656 [info ] [Thread-2  ]: 21 of 35 PASS not_null_movie_performance_top_rating_percentage ................. [[32mPASS[0m in 1.34s]
[0m17:57:35.939017 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m17:57:35.939805 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m17:57:35.940568 [info ] [Thread-2  ]: 22 of 35 START test not_null_movie_performance_weighted_avg_rating ............. [RUN]
[0m17:57:35.941884 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb, now test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5)
[0m17:57:35.942600 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m17:57:35.949615 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m17:57:35.959673 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (compile): 17:57:35.943147 => 17:57:35.959222
[0m17:57:35.960431 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m17:57:35.972473 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m17:57:35.982993 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:35.983698 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m17:57:35.984450 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select weighted_avg_rating
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating is null



      
    ) dbt_internal_test
[0m17:57:35.985173 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:37.466757 [debug] [Thread-2  ]: SQL status: OK in 1.4800000190734863 seconds
[0m17:57:37.471271 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (execute): 17:57:35.961117 => 17:57:37.470877
[0m17:57:37.472163 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: ROLLBACK
[0m17:57:37.472971 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:37.473780 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: Close
[0m17:57:37.590019 [info ] [Thread-2  ]: 22 of 35 PASS not_null_movie_performance_weighted_avg_rating ................... [[32mPASS[0m in 1.65s]
[0m17:57:37.591497 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m17:57:37.592325 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m17:57:37.593130 [info ] [Thread-2  ]: 23 of 35 START test not_null_user_activity_avg_user_rating ..................... [RUN]
[0m17:57:37.594510 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5, now test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b)
[0m17:57:37.595460 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m17:57:37.602008 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m17:57:37.611851 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (compile): 17:57:37.596183 => 17:57:37.611422
[0m17:57:37.612610 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m17:57:37.616717 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m17:57:37.626698 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:37.627367 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m17:57:37.627929 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_user_rating
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating is null



      
    ) dbt_internal_test
[0m17:57:37.628535 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:38.061970 [debug] [Thread-4  ]: SQL status: OK in 3.509999990463257 seconds
[0m17:57:38.065731 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (execute): 17:57:33.299460 => 17:57:38.065354
[0m17:57:38.066556 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: ROLLBACK
[0m17:57:38.067284 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:38.068036 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: Close
[0m17:57:38.177590 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8f64344-1ea5-4316-96c4-e6f622528483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80de152e50>]}
[0m17:57:38.178860 [info ] [Thread-4  ]: 3 of 5 OK created sql incremental model movielens_volume.movie_performance ..... [[32mOK[0m in 4.90s]
[0m17:57:38.180075 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_performance
[0m17:57:38.180818 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_activity
[0m17:57:38.181740 [info ] [Thread-4  ]: 4 of 5 START sql incremental model movielens_volume.user_activity .............. [RUN]
[0m17:57:38.183142 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_performance, now model.my_dbt_project.user_activity)
[0m17:57:38.183890 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_activity
[0m17:57:38.191209 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_activity"
[0m17:57:38.200251 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (compile): 17:57:38.184362 => 17:57:38.199795
[0m17:57:38.201091 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_activity
[0m17:57:38.208181 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:38.208969 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:57:38.209780 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m17:57:38.210448 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:57:38.353077 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m17:57:38.357888 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (execute): 17:57:37.613092 => 17:57:38.357404
[0m17:57:38.358749 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: ROLLBACK
[0m17:57:38.359377 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:38.360279 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: Close
[0m17:57:38.471912 [info ] [Thread-2  ]: 23 of 35 PASS not_null_user_activity_avg_user_rating ........................... [[32mPASS[0m in 0.88s]
[0m17:57:38.474078 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m17:57:38.475171 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m17:57:38.475899 [info ] [Thread-2  ]: 24 of 35 START test not_null_user_activity_first_activity_date ................. [RUN]
[0m17:57:38.477152 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b, now test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac)
[0m17:57:38.477803 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m17:57:38.484915 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m17:57:38.495311 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (compile): 17:57:38.478304 => 17:57:38.494821
[0m17:57:38.496044 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m17:57:38.500127 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m17:57:38.510500 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:38.511265 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m17:57:38.511919 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where first_activity_date is null



      
    ) dbt_internal_test
[0m17:57:38.512727 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:38.834419 [debug] [Thread-4  ]: SQL status: OK in 0.6200000047683716 seconds
[0m17:57:38.839979 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:57:38.841055 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

  
    create temporary view `user_activity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Join ratings with movies to attach genres and prepare timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_timestamp,
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: User Activity (General)
-- Calculate total ratings, average rating, first and last activity dates per user
-- =======================================================
user_activity as (
    select
        user_id,
        count(*) as user_rating_count,
        round(avg(rating), 2) as avg_user_rating,
        min(rating_timestamp) as first_activity_date,
        max(rating_timestamp) as last_activity_date
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 3: User Influence Score
-- Compute influence based on total ratings (more ratings = higher influence)
-- =======================================================
user_influence as (
    select
        user_id,
        user_rating_count as influence_score
    from user_activity
),

-- =======================================================
-- Step 4: Active Users by Genre
-- Count ratings per genre and rank genres per user
-- =======================================================
user_genre_activity as (
    select
        user_id,
        regexp_replace(genres, '\\|', ' | ') as genres,
        count(*) as genre_rating_count,
        rank() over (partition by user_id order by count(*) desc) as genre_rank
    from ratings_base
    group by user_id, genres
),

-- =======================================================
-- Step 5: New vs Loyal Users
-- Categorize users as new if first activity < 30 days ago, else loyal
-- =======================================================
user_type as (
    select
        user_id,
        case
            when min(rating_timestamp) >= date_sub(current_date(), 30)
                then 'new_user'
            else 'loyal_user'
        end as user_category
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 6: Top Genre per User
-- Identify the most active genre for each user
-- =======================================================
top_genre as (
    select
        user_id,
        max(genres) as top_genre
    from user_genre_activity
    group by user_id
)

-- =======================================================
-- Final Selection
-- Combine general activity, influence, type, and top genre per user
-- =======================================================
select
    ua.user_id,
    ua.user_rating_count,
    ua.avg_user_rating,
    ua.first_activity_date,
    ua.last_activity_date,
    ui.influence_score,
    ut.user_category,
    tg.top_genre
from user_activity ua
join user_influence ui on ua.user_id = ui.user_id
join user_type ut on ua.user_id = ut.user_id
left join top_genre tg on ua.user_id = tg.user_id


  where ua.user_id not in (select user_id from `workspace`.`movielens_volume`.`user_activity`)

  
[0m17:57:39.177266 [debug] [Thread-4  ]: SQL status: OK in 0.3400000035762787 seconds
[0m17:57:39.186974 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:57:39.187806 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m17:57:39.321314 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m17:57:39.326046 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (execute): 17:57:38.496502 => 17:57:39.325682
[0m17:57:39.326926 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: ROLLBACK
[0m17:57:39.327637 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:39.328284 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: Close
[0m17:57:39.437516 [info ] [Thread-2  ]: 24 of 35 PASS not_null_user_activity_first_activity_date ....................... [[32mPASS[0m in 0.96s]
[0m17:57:39.439035 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m17:57:39.440038 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m17:57:39.441154 [info ] [Thread-2  ]: 25 of 35 START test not_null_user_activity_influence_score ..................... [RUN]
[0m17:57:39.442600 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac, now test.my_dbt_project.not_null_user_activity_influence_score.758852d325)
[0m17:57:39.443369 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m17:57:39.450110 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m17:57:39.460579 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (compile): 17:57:39.443906 => 17:57:39.460116
[0m17:57:39.461415 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m17:57:39.463072 [debug] [Thread-4  ]: SQL status: OK in 0.27000001072883606 seconds
[0m17:57:39.467717 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_activity"
[0m17:57:39.475663 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m17:57:39.476847 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:57:39.477814 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_activity` as DBT_INTERNAL_DEST
      using `user_activity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m17:57:39.486255 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:39.486935 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m17:57:39.487497 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select influence_score
from `workspace`.`movielens_volume`.`user_activity`
where influence_score is null



      
    ) dbt_internal_test
[0m17:57:39.488158 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:40.506118 [debug] [Thread-2  ]: SQL status: OK in 1.0199999809265137 seconds
[0m17:57:40.510348 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (execute): 17:57:39.462095 => 17:57:40.509951
[0m17:57:40.511269 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: ROLLBACK
[0m17:57:40.512080 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:40.512794 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: Close
[0m17:57:40.627067 [info ] [Thread-2  ]: 25 of 35 PASS not_null_user_activity_influence_score ........................... [[32mPASS[0m in 1.18s]
[0m17:57:40.628779 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m17:57:40.629800 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m17:57:40.630637 [info ] [Thread-2  ]: 26 of 35 START test not_null_user_activity_last_activity_date .................. [RUN]
[0m17:57:40.631978 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_influence_score.758852d325, now test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad)
[0m17:57:40.632642 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m17:57:40.641237 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m17:57:40.651968 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (compile): 17:57:40.633149 => 17:57:40.651481
[0m17:57:40.652755 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m17:57:40.657035 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m17:57:40.666583 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:40.667269 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m17:57:40.667914 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where last_activity_date is null



      
    ) dbt_internal_test
[0m17:57:40.668532 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:41.715093 [debug] [Thread-2  ]: SQL status: OK in 1.0499999523162842 seconds
[0m17:57:41.719110 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (execute): 17:57:40.653294 => 17:57:41.718758
[0m17:57:41.719868 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: ROLLBACK
[0m17:57:41.720538 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:41.721179 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: Close
[0m17:57:41.830254 [info ] [Thread-2  ]: 26 of 35 PASS not_null_user_activity_last_activity_date ........................ [[32mPASS[0m in 1.20s]
[0m17:57:41.831670 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m17:57:41.832463 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m17:57:41.833254 [info ] [Thread-2  ]: 27 of 35 START test not_null_user_activity_top_genre ........................... [RUN]
[0m17:57:41.834645 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad, now test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313)
[0m17:57:41.835455 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m17:57:41.844731 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m17:57:41.854727 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (compile): 17:57:41.836027 => 17:57:41.854257
[0m17:57:41.855593 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m17:57:41.859924 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m17:57:41.869683 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:41.870397 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m17:57:41.871069 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_genre
from `workspace`.`movielens_volume`.`user_activity`
where top_genre is null



      
    ) dbt_internal_test
[0m17:57:41.871676 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:42.757733 [debug] [Thread-2  ]: SQL status: OK in 0.8899999856948853 seconds
[0m17:57:42.761977 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (execute): 17:57:41.856055 => 17:57:42.761634
[0m17:57:42.762789 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: ROLLBACK
[0m17:57:42.763566 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:42.764248 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: Close
[0m17:57:42.874914 [info ] [Thread-2  ]: 27 of 35 PASS not_null_user_activity_top_genre ................................. [[32mPASS[0m in 1.04s]
[0m17:57:42.876319 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m17:57:42.877347 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m17:57:42.878238 [info ] [Thread-2  ]: 28 of 35 START test not_null_user_activity_user_id ............................. [RUN]
[0m17:57:42.879768 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313, now test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36)
[0m17:57:42.880586 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m17:57:42.890262 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m17:57:42.900876 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (compile): 17:57:42.881229 => 17:57:42.900431
[0m17:57:42.901591 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m17:57:42.907267 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m17:57:42.917415 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:42.918175 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m17:57:42.919058 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`user_activity`
where user_id is null



      
    ) dbt_internal_test
[0m17:57:42.919898 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:43.122990 [debug] [Thread-4  ]: SQL status: OK in 3.640000104904175 seconds
[0m17:57:43.126747 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (execute): 17:57:38.201648 => 17:57:43.126455
[0m17:57:43.127495 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: ROLLBACK
[0m17:57:43.128166 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:43.128800 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: Close
[0m17:57:43.233699 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8f64344-1ea5-4316-96c4-e6f622528483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80de025250>]}
[0m17:57:43.235020 [info ] [Thread-4  ]: 4 of 5 OK created sql incremental model movielens_volume.user_activity ......... [[32mOK[0m in 5.05s]
[0m17:57:43.236393 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_activity
[0m17:57:43.237295 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_preferences
[0m17:57:43.238248 [info ] [Thread-4  ]: 5 of 5 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m17:57:43.239513 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.user_activity, now model.my_dbt_project.user_preferences)
[0m17:57:43.240187 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_preferences
[0m17:57:43.247878 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m17:57:43.256300 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (compile): 17:57:43.240735 => 17:57:43.255847
[0m17:57:43.257343 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_preferences
[0m17:57:43.264362 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:43.265325 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m17:57:43.265892 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m17:57:43.266564 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:57:43.656026 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m17:57:43.660441 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (execute): 17:57:42.902045 => 17:57:43.660062
[0m17:57:43.661356 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: ROLLBACK
[0m17:57:43.662215 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:43.662902 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: Close
[0m17:57:43.774079 [info ] [Thread-2  ]: 28 of 35 PASS not_null_user_activity_user_id ................................... [[32mPASS[0m in 0.89s]
[0m17:57:43.775541 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m17:57:43.776379 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m17:57:43.777326 [info ] [Thread-2  ]: 29 of 35 START test not_null_user_activity_user_rating_count ................... [RUN]
[0m17:57:43.778740 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36, now test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e)
[0m17:57:43.779401 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m17:57:43.789077 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m17:57:43.799487 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (compile): 17:57:43.779957 => 17:57:43.798992
[0m17:57:43.800306 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m17:57:43.804104 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m17:57:43.813765 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:43.814549 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m17:57:43.815119 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_rating_count
from `workspace`.`movielens_volume`.`user_activity`
where user_rating_count is null



      
    ) dbt_internal_test
[0m17:57:43.815731 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:43.949445 [debug] [Thread-4  ]: SQL status: OK in 0.6800000071525574 seconds
[0m17:57:43.955609 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m17:57:43.956635 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create temporary view `user_preferences__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score,  -- influence based on total ratings
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )

  
[0m17:57:44.348429 [debug] [Thread-4  ]: SQL status: OK in 0.38999998569488525 seconds
[0m17:57:44.353258 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m17:57:44.354265 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m17:57:44.662952 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m17:57:44.667492 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (execute): 17:57:43.800744 => 17:57:44.667077
[0m17:57:44.668266 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: ROLLBACK
[0m17:57:44.668949 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:44.669731 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: Close
[0m17:57:44.715391 [debug] [Thread-4  ]: SQL status: OK in 0.36000001430511475 seconds
[0m17:57:44.720621 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m17:57:44.729723 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m17:57:44.730663 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
      using `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m17:57:44.785972 [info ] [Thread-2  ]: 29 of 35 PASS not_null_user_activity_user_rating_count ......................... [[32mPASS[0m in 1.01s]
[0m17:57:44.787377 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m17:57:44.788298 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:57:44.789038 [info ] [Thread-2  ]: 30 of 35 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:57:44.790868 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m17:57:44.791926 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:57:44.804418 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:57:44.814403 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 17:57:44.792413 => 17:57:44.813948
[0m17:57:44.815281 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:57:44.819891 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:57:44.830365 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:44.831116 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:57:44.831833 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:57:44.832667 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:46.599065 [debug] [Thread-2  ]: SQL status: OK in 1.7699999809265137 seconds
[0m17:57:46.603421 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 17:57:44.815874 => 17:57:46.603084
[0m17:57:46.604257 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m17:57:46.605156 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:46.605819 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m17:57:46.719374 [info ] [Thread-2  ]: 30 of 35 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.93s]
[0m17:57:46.720803 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:57:46.721681 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:57:46.722493 [info ] [Thread-2  ]: 31 of 35 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:57:46.723828 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m17:57:46.724614 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:57:46.733082 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:57:46.742969 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 17:57:46.725234 => 17:57:46.742553
[0m17:57:46.743668 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:57:46.747887 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:57:46.757777 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:46.758432 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:57:46.759106 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:57:46.759888 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:47.633985 [debug] [Thread-2  ]: SQL status: OK in 0.8700000047683716 seconds
[0m17:57:47.638115 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 17:57:46.744122 => 17:57:47.637802
[0m17:57:47.638839 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m17:57:47.639550 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:47.640208 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m17:57:47.754155 [info ] [Thread-2  ]: 31 of 35 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.03s]
[0m17:57:47.755518 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:57:47.756335 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m17:57:47.757054 [info ] [Thread-2  ]: 32 of 35 START test unique_genre_popularity_genres ............................. [RUN]
[0m17:57:47.758388 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075)
[0m17:57:47.759219 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m17:57:47.769274 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m17:57:47.780429 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (compile): 17:57:47.759753 => 17:57:47.779968
[0m17:57:47.781291 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m17:57:47.785376 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m17:57:47.795560 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:47.796318 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m17:57:47.796924 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    genres as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`genre_popularity`
where genres is not null
group by genres
having count(*) > 1



      
    ) dbt_internal_test
[0m17:57:47.797551 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:48.603377 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m17:57:48.607324 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (execute): 17:57:47.781783 => 17:57:48.606980
[0m17:57:48.608066 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: ROLLBACK
[0m17:57:48.608871 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:48.609719 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: Close
[0m17:57:48.729859 [info ] [Thread-2  ]: 32 of 35 PASS unique_genre_popularity_genres ................................... [[32mPASS[0m in 0.97s]
[0m17:57:48.731496 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m17:57:48.732350 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m17:57:48.733168 [info ] [Thread-2  ]: 33 of 35 START test unique_movie_enriched_movie_id ............................. [RUN]
[0m17:57:48.734574 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075, now test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a)
[0m17:57:48.735383 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m17:57:48.742600 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m17:57:48.753730 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (compile): 17:57:48.736125 => 17:57:48.753305
[0m17:57:48.754509 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m17:57:48.758643 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m17:57:48.768351 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:48.769064 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m17:57:48.769784 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:57:48.770457 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:49.425237 [debug] [Thread-2  ]: SQL status: OK in 0.6499999761581421 seconds
[0m17:57:49.429872 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (execute): 17:57:48.754976 => 17:57:49.429521
[0m17:57:49.430640 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: ROLLBACK
[0m17:57:49.431314 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:49.431959 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m17:57:49.556087 [info ] [Thread-2  ]: 33 of 35 PASS unique_movie_enriched_movie_id ................................... [[32mPASS[0m in 0.82s]
[0m17:57:49.557486 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m17:57:49.558350 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m17:57:49.559029 [info ] [Thread-2  ]: 34 of 35 START test unique_movie_performance_movie_id .......................... [RUN]
[0m17:57:49.560351 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a, now test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22)
[0m17:57:49.561190 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m17:57:49.567473 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m17:57:49.577676 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (compile): 17:57:49.561719 => 17:57:49.577168
[0m17:57:49.578454 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m17:57:49.583025 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m17:57:49.593410 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:49.594220 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m17:57:49.594838 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:57:49.595770 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:50.590298 [debug] [Thread-2  ]: SQL status: OK in 0.9900000095367432 seconds
[0m17:57:50.601127 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (execute): 17:57:49.579137 => 17:57:50.598950
[0m17:57:50.602067 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: ROLLBACK
[0m17:57:50.602749 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:50.603373 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: Close
[0m17:57:50.709636 [info ] [Thread-2  ]: 34 of 35 PASS unique_movie_performance_movie_id ................................ [[32mPASS[0m in 1.15s]
[0m17:57:50.710971 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m17:57:50.711745 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m17:57:50.712516 [info ] [Thread-2  ]: 35 of 35 START test unique_user_activity_user_id ............................... [RUN]
[0m17:57:50.713823 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22, now test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda)
[0m17:57:50.714783 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m17:57:50.720988 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m17:57:50.730030 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (compile): 17:57:50.715511 => 17:57:50.729587
[0m17:57:50.730780 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m17:57:50.735153 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m17:57:50.744842 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:50.745688 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m17:57:50.746265 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`user_activity`
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:57:50.746885 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:57:51.537770 [debug] [Thread-2  ]: SQL status: OK in 0.7900000214576721 seconds
[0m17:57:51.542189 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (execute): 17:57:50.731222 => 17:57:51.541866
[0m17:57:51.542900 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: ROLLBACK
[0m17:57:51.543759 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:51.544614 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: Close
[0m17:57:51.648529 [info ] [Thread-2  ]: 35 of 35 PASS unique_user_activity_user_id ..................................... [[32mPASS[0m in 0.93s]
[0m17:57:51.649928 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m17:57:51.652695 [debug] [MainThread]: On master: ROLLBACK
[0m17:57:51.653526 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:57:51.921444 [debug] [Thread-4  ]: SQL status: OK in 7.190000057220459 seconds
[0m17:57:51.993187 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:57:51.994277 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:51.995098 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:57:51.995833 [debug] [MainThread]: On master: ROLLBACK
[0m17:57:51.996495 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:57:51.997137 [debug] [MainThread]: On master: Close
[0m17:57:52.042030 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (execute): 17:57:43.257906 => 17:57:52.041745
[0m17:57:52.043031 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: ROLLBACK
[0m17:57:52.043794 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:57:52.044506 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: Close
[0m17:57:52.098229 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:57:52.099103 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda' was properly closed.
[0m17:57:52.101506 [info ] [MainThread]: 
[0m17:57:52.103262 [info ] [MainThread]: Finished running 35 tests in 0 hours 0 minutes and 39.64 seconds (39.64s).
[0m17:57:52.110966 [debug] [MainThread]: Command end result
[0m17:57:52.140755 [info ] [MainThread]: 
[0m17:57:52.141694 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:57:52.142447 [info ] [MainThread]: 
[0m17:57:52.143351 [info ] [MainThread]: Done. PASS=35 WARN=0 ERROR=0 SKIP=0 TOTAL=35
[0m17:57:52.144761 [debug] [MainThread]: Command `dbt test` succeeded at 17:57:52.144599 after 42.55 seconds
[0m17:57:52.145502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f477af43550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f474e224850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f474def05e0>]}
[0m17:57:52.146157 [debug] [MainThread]: Flushing usage events
[0m17:57:52.160938 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8f64344-1ea5-4316-96c4-e6f622528483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80de083c70>]}
[0m17:57:52.162133 [info ] [Thread-4  ]: 5 of 5 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 8.92s]
[0m17:57:52.163391 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_preferences
[0m17:57:52.165990 [debug] [MainThread]: On master: ROLLBACK
[0m17:57:52.166697 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:57:52.511669 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:57:52.512495 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:57:52.513221 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:57:52.513867 [debug] [MainThread]: On master: ROLLBACK
[0m17:57:52.514451 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:57:52.515104 [debug] [MainThread]: On master: Close
[0m17:57:52.625363 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:57:52.626138 [debug] [MainThread]: Connection 'model.my_dbt_project.user_preferences' was properly closed.
[0m17:57:52.628656 [info ] [MainThread]: 
[0m17:57:52.630333 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 36.54 seconds (36.54s).
[0m17:57:52.632456 [debug] [MainThread]: Command end result
[0m17:57:52.660488 [info ] [MainThread]: 
[0m17:57:52.661346 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:57:52.662089 [info ] [MainThread]: 
[0m17:57:52.662883 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m17:57:52.664089 [debug] [MainThread]: Command `dbt run` succeeded at 17:57:52.663916 after 39.44 seconds
[0m17:57:52.664703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f810c236670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80df2ae520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80de0a0ee0>]}
[0m17:57:52.665375 [debug] [MainThread]: Flushing usage events
[0m17:57:57.418147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50ac0835b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50aa5ef220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50aa5ef940>]}


============================== 17:57:57.425727 | d9682cd8-ca6b-4474-8e16-0f6337053fd4 ==============================
[0m17:57:57.425727 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:57:57.427015 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:57:58.462883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd9682cd8-ca6b-4474-8e16-0f6337053fd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50aa5ef1c0>]}
[0m17:57:58.494460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd9682cd8-ca6b-4474-8e16-0f6337053fd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f507f31ac40>]}
[0m17:57:58.495632 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m17:57:58.541522 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:57:59.872931 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:57:59.873944 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:57:59.886019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd9682cd8-ca6b-4474-8e16-0f6337053fd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f507f2d0400>]}
[0m17:57:59.920704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd9682cd8-ca6b-4474-8e16-0f6337053fd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f507f200430>]}
[0m17:57:59.921735 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m17:57:59.922600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9682cd8-ca6b-4474-8e16-0f6337053fd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f507f200490>]}
[0m17:57:59.926213 [info ] [MainThread]: 
[0m17:57:59.927683 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:57:59.930195 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m17:57:59.939738 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m17:57:59.940759 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m17:57:59.941418 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:58:00.542613 [debug] [ThreadPool]: SQL status: OK in 0.6000000238418579 seconds
[0m17:58:00.546173 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m17:58:00.664208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9682cd8-ca6b-4474-8e16-0f6337053fd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f507f200310>]}
[0m17:58:00.665182 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:00.665877 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:58:00.666872 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:58:00.667680 [info ] [MainThread]: 
[0m17:58:00.682374 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m17:58:00.683182 [info ] [Thread-2  ]: 1 of 22 START test accepted_rating_range_silver_ratings_rating__5__0 ........... [RUN]
[0m17:58:00.684459 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff)
[0m17:58:00.685145 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m17:58:00.696435 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m17:58:00.705697 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (compile): 17:58:00.685668 => 17:58:00.705346
[0m17:58:00.706430 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m17:58:00.736773 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m17:58:00.746933 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:00.747622 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m17:58:00.748176 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`silver_ratings`
where rating < 0
   or rating > 5


      
    ) dbt_internal_test
[0m17:58:00.748764 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:01.483958 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m17:58:01.493164 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (execute): 17:58:00.706957 => 17:58:01.492804
[0m17:58:01.493904 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: ROLLBACK
[0m17:58:01.494568 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:01.495280 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: Close
[0m17:58:01.609237 [info ] [Thread-2  ]: 1 of 22 PASS accepted_rating_range_silver_ratings_rating__5__0 ................. [[32mPASS[0m in 0.92s]
[0m17:58:01.610694 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m17:58:01.611538 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m17:58:01.612251 [info ] [Thread-2  ]: 2 of 22 START test not_null_silver_links_imdb_id ............................... [RUN]
[0m17:58:01.613537 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff, now test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51)
[0m17:58:01.614178 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m17:58:01.624317 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m17:58:01.634303 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (compile): 17:58:01.614669 => 17:58:01.633760
[0m17:58:01.635136 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m17:58:01.639218 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m17:58:01.648781 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:01.649468 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m17:58:01.650071 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



      
    ) dbt_internal_test
[0m17:58:01.650834 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:02.419156 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m17:58:02.423242 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (execute): 17:58:01.635588 => 17:58:02.422906
[0m17:58:02.423985 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: ROLLBACK
[0m17:58:02.424750 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:02.425529 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m17:58:02.535506 [info ] [Thread-2  ]: 2 of 22 PASS not_null_silver_links_imdb_id ..................................... [[32mPASS[0m in 0.92s]
[0m17:58:02.536939 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m17:58:02.537815 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m17:58:02.538705 [info ] [Thread-2  ]: 3 of 22 START test not_null_silver_links_movie_id .............................. [RUN]
[0m17:58:02.540017 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51, now test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d)
[0m17:58:02.540800 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m17:58:02.549710 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m17:58:02.557261 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (compile): 17:58:02.541363 => 17:58:02.556907
[0m17:58:02.557915 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m17:58:02.561826 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m17:58:02.569429 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:02.570016 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m17:58:02.570534 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



      
    ) dbt_internal_test
[0m17:58:02.571191 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:03.290425 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m17:58:03.294735 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (execute): 17:58:02.558378 => 17:58:03.294413
[0m17:58:03.295600 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: ROLLBACK
[0m17:58:03.296348 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:03.297043 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m17:58:03.412212 [info ] [Thread-2  ]: 3 of 22 PASS not_null_silver_links_movie_id .................................... [[32mPASS[0m in 0.87s]
[0m17:58:03.414045 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m17:58:03.415211 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m17:58:03.416056 [info ] [Thread-2  ]: 4 of 22 START test not_null_silver_links_tmdb_id ............................... [RUN]
[0m17:58:03.417467 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d, now test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f)
[0m17:58:03.418349 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m17:58:03.430855 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m17:58:03.438665 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (compile): 17:58:03.419023 => 17:58:03.438311
[0m17:58:03.439330 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m17:58:03.443293 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m17:58:03.450287 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:03.450863 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m17:58:03.451422 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



      
    ) dbt_internal_test
[0m17:58:03.452036 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:04.170750 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m17:58:04.174836 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (execute): 17:58:03.439801 => 17:58:04.174510
[0m17:58:04.175695 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: ROLLBACK
[0m17:58:04.176580 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:04.177323 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m17:58:04.292038 [info ] [Thread-2  ]: 4 of 22 PASS not_null_silver_links_tmdb_id ..................................... [[32mPASS[0m in 0.87s]
[0m17:58:04.293362 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m17:58:04.294149 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m17:58:04.295001 [info ] [Thread-2  ]: 5 of 22 START test not_null_silver_movies_genres ............................... [RUN]
[0m17:58:04.296321 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f, now test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63)
[0m17:58:04.297002 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m17:58:04.307655 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m17:58:04.317874 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (compile): 17:58:04.297479 => 17:58:04.317431
[0m17:58:04.318655 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m17:58:04.323005 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m17:58:04.334918 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:04.335688 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m17:58:04.336228 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



      
    ) dbt_internal_test
[0m17:58:04.336914 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:05.047339 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m17:58:05.051785 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (execute): 17:58:04.319184 => 17:58:05.051436
[0m17:58:05.052687 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: ROLLBACK
[0m17:58:05.053471 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:05.054219 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m17:58:05.165120 [info ] [Thread-2  ]: 5 of 22 PASS not_null_silver_movies_genres ..................................... [[32mPASS[0m in 0.87s]
[0m17:58:05.166437 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m17:58:05.167368 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m17:58:05.168145 [info ] [Thread-2  ]: 6 of 22 START test not_null_silver_movies_movie_id ............................. [RUN]
[0m17:58:05.169391 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63, now test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e)
[0m17:58:05.170057 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m17:58:05.176561 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m17:58:05.186887 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (compile): 17:58:05.170595 => 17:58:05.186457
[0m17:58:05.187635 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m17:58:05.191412 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m17:58:05.200333 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:05.200935 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m17:58:05.201576 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



      
    ) dbt_internal_test
[0m17:58:05.202336 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:05.908579 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m17:58:05.912817 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (execute): 17:58:05.188096 => 17:58:05.912476
[0m17:58:05.913631 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: ROLLBACK
[0m17:58:05.914437 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:05.915279 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m17:58:06.033742 [info ] [Thread-2  ]: 6 of 22 PASS not_null_silver_movies_movie_id ................................... [[32mPASS[0m in 0.86s]
[0m17:58:06.035320 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m17:58:06.036171 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m17:58:06.037132 [info ] [Thread-2  ]: 7 of 22 START test not_null_silver_movies_title ................................ [RUN]
[0m17:58:06.038431 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e, now test.my_dbt_project.not_null_silver_movies_title.3a779bd2de)
[0m17:58:06.039150 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m17:58:06.051636 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m17:58:06.060430 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (compile): 17:58:06.039814 => 17:58:06.060072
[0m17:58:06.061094 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m17:58:06.065128 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m17:58:06.074526 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:06.075255 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m17:58:06.075830 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



      
    ) dbt_internal_test
[0m17:58:06.076580 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:06.784442 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m17:58:06.788824 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (execute): 17:58:06.061536 => 17:58:06.788485
[0m17:58:06.789717 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: ROLLBACK
[0m17:58:06.790651 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:06.791372 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m17:58:06.905803 [info ] [Thread-2  ]: 7 of 22 PASS not_null_silver_movies_title ...................................... [[32mPASS[0m in 0.87s]
[0m17:58:06.907247 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m17:58:06.908039 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m17:58:06.908818 [info ] [Thread-2  ]: 8 of 22 START test not_null_silver_ratings_movie_id ............................ [RUN]
[0m17:58:06.910174 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_title.3a779bd2de, now test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36)
[0m17:58:06.910844 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m17:58:06.917692 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m17:58:06.927530 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (compile): 17:58:06.911578 => 17:58:06.927115
[0m17:58:06.928206 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m17:58:06.932134 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m17:58:06.940990 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:06.941516 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m17:58:06.942094 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



      
    ) dbt_internal_test
[0m17:58:06.942707 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:07.672504 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m17:58:07.677017 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (execute): 17:58:06.928649 => 17:58:07.676648
[0m17:58:07.677845 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: ROLLBACK
[0m17:58:07.678529 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:07.679171 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m17:58:07.789890 [info ] [Thread-2  ]: 8 of 22 PASS not_null_silver_ratings_movie_id .................................. [[32mPASS[0m in 0.88s]
[0m17:58:07.791404 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m17:58:07.792236 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m17:58:07.792986 [info ] [Thread-2  ]: 9 of 22 START test not_null_silver_ratings_rating .............................. [RUN]
[0m17:58:07.794369 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36, now test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a)
[0m17:58:07.795149 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m17:58:07.801425 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m17:58:07.810683 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (compile): 17:58:07.795699 => 17:58:07.810358
[0m17:58:07.811414 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m17:58:07.815439 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m17:58:07.824434 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:07.824965 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m17:58:07.825536 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



      
    ) dbt_internal_test
[0m17:58:07.826112 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:08.534333 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m17:58:08.538896 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (execute): 17:58:07.811875 => 17:58:08.538567
[0m17:58:08.539757 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: ROLLBACK
[0m17:58:08.540670 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:08.541572 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m17:58:08.650058 [info ] [Thread-2  ]: 9 of 22 PASS not_null_silver_ratings_rating .................................... [[32mPASS[0m in 0.86s]
[0m17:58:08.651562 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m17:58:08.652498 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m17:58:08.653262 [info ] [Thread-2  ]: 10 of 22 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m17:58:08.654695 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a, now test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f)
[0m17:58:08.655519 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m17:58:08.670307 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m17:58:08.679520 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (compile): 17:58:08.656173 => 17:58:08.679127
[0m17:58:08.680307 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m17:58:08.684571 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m17:58:08.694051 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:08.694656 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m17:58:08.695213 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



      
    ) dbt_internal_test
[0m17:58:08.695860 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:09.392917 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m17:58:09.397118 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (execute): 17:58:08.680900 => 17:58:09.396790
[0m17:58:09.397900 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: ROLLBACK
[0m17:58:09.398812 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:09.399517 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m17:58:09.509565 [info ] [Thread-2  ]: 10 of 22 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 0.86s]
[0m17:58:09.511141 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m17:58:09.511927 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m17:58:09.512629 [info ] [Thread-2  ]: 11 of 22 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m17:58:09.513891 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f, now test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3)
[0m17:58:09.514663 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m17:58:09.523087 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m17:58:09.533527 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (compile): 17:58:09.515263 => 17:58:09.533074
[0m17:58:09.534319 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m17:58:09.538570 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m17:58:09.548559 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:09.549323 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m17:58:09.549958 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



      
    ) dbt_internal_test
[0m17:58:09.550678 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:10.312829 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m17:58:10.317194 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (execute): 17:58:09.534865 => 17:58:10.316869
[0m17:58:10.317995 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: ROLLBACK
[0m17:58:10.318809 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:10.319502 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m17:58:10.424232 [info ] [Thread-2  ]: 11 of 22 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 0.91s]
[0m17:58:10.425617 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m17:58:10.426632 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m17:58:10.427480 [info ] [Thread-2  ]: 12 of 22 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m17:58:10.428749 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3, now test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3)
[0m17:58:10.429432 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m17:58:10.438328 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m17:58:10.448399 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (compile): 17:58:10.429973 => 17:58:10.448020
[0m17:58:10.449109 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m17:58:10.453423 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m17:58:10.462950 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:10.463586 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m17:58:10.464194 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



      
    ) dbt_internal_test
[0m17:58:10.464984 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:11.188550 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m17:58:11.193434 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (execute): 17:58:10.449584 => 17:58:11.193101
[0m17:58:11.194223 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: ROLLBACK
[0m17:58:11.194896 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:11.195627 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m17:58:11.303426 [info ] [Thread-2  ]: 12 of 22 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 0.87s]
[0m17:58:11.304734 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m17:58:11.305539 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m17:58:11.306384 [info ] [Thread-2  ]: 13 of 22 START test not_null_silver_tags_tag ................................... [RUN]
[0m17:58:11.307690 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3, now test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8)
[0m17:58:11.308388 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m17:58:11.318981 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m17:58:11.329568 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (compile): 17:58:11.308924 => 17:58:11.329122
[0m17:58:11.330339 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m17:58:11.334549 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m17:58:11.344362 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:11.345063 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m17:58:11.345752 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



      
    ) dbt_internal_test
[0m17:58:11.346436 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:12.046581 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m17:58:12.050772 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (execute): 17:58:11.330804 => 17:58:12.050451
[0m17:58:12.051546 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: ROLLBACK
[0m17:58:12.052324 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:12.053091 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m17:58:12.165749 [info ] [Thread-2  ]: 13 of 22 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 0.86s]
[0m17:58:12.167160 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m17:58:12.168085 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m17:58:12.168826 [info ] [Thread-2  ]: 14 of 22 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m17:58:12.170182 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8, now test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86)
[0m17:58:12.170982 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m17:58:12.177553 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m17:58:12.187406 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (compile): 17:58:12.171593 => 17:58:12.187049
[0m17:58:12.188078 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m17:58:12.191815 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m17:58:12.200596 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:12.201289 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m17:58:12.201962 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



      
    ) dbt_internal_test
[0m17:58:12.202580 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:12.922362 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m17:58:12.926489 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (execute): 17:58:12.188560 => 17:58:12.926158
[0m17:58:12.927325 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: ROLLBACK
[0m17:58:12.928058 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:12.928663 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m17:58:13.044864 [info ] [Thread-2  ]: 14 of 22 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 0.87s]
[0m17:58:13.046526 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m17:58:13.047341 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m17:58:13.048171 [info ] [Thread-2  ]: 15 of 22 START test not_null_silver_tags_user_id ............................... [RUN]
[0m17:58:13.049429 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86, now test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3)
[0m17:58:13.050120 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m17:58:13.059136 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m17:58:13.072772 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (compile): 17:58:13.050589 => 17:58:13.072187
[0m17:58:13.073789 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m17:58:13.078272 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m17:58:13.089293 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:13.090173 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m17:58:13.090895 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



      
    ) dbt_internal_test
[0m17:58:13.091678 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:13.802897 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m17:58:13.807307 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (execute): 17:58:13.074391 => 17:58:13.806898
[0m17:58:13.808175 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: ROLLBACK
[0m17:58:13.808802 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:13.809434 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m17:58:13.929037 [info ] [Thread-2  ]: 15 of 22 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 0.88s]
[0m17:58:13.930342 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m17:58:13.931246 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:58:13.932095 [info ] [Thread-2  ]: 16 of 22 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:58:13.933582 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m17:58:13.934262 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:58:13.955137 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:58:13.965571 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 17:58:13.934759 => 17:58:13.965061
[0m17:58:13.966668 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:58:13.971005 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:58:13.981525 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:13.982557 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:58:13.983362 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:58:13.984077 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:14.525501 [debug] [Thread-2  ]: SQL status: OK in 0.5400000214576721 seconds
[0m17:58:14.530280 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 17:58:13.967213 => 17:58:14.529854
[0m17:58:14.531259 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m17:58:14.532096 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:14.532817 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m17:58:14.639164 [info ] [Thread-2  ]: 16 of 22 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.71s]
[0m17:58:14.641024 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:58:14.642049 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:58:14.642931 [info ] [Thread-2  ]: 17 of 22 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:58:14.644790 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m17:58:14.645698 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:58:14.655373 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:58:14.667042 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 17:58:14.646273 => 17:58:14.666442
[0m17:58:14.668032 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:58:14.672495 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:58:14.682241 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:14.682900 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:58:14.683739 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:58:14.684494 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:15.238733 [debug] [Thread-2  ]: SQL status: OK in 0.550000011920929 seconds
[0m17:58:15.242873 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 17:58:14.668581 => 17:58:15.242545
[0m17:58:15.243598 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m17:58:15.244289 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:15.244927 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m17:58:15.368894 [info ] [Thread-2  ]: 17 of 22 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.72s]
[0m17:58:15.370316 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:58:15.371167 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m17:58:15.371878 [info ] [Thread-2  ]: 18 of 22 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:58:15.373406 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32)
[0m17:58:15.374082 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m17:58:15.381806 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m17:58:15.391461 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (compile): 17:58:15.374541 => 17:58:15.391039
[0m17:58:15.392217 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m17:58:15.396534 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m17:58:15.405451 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:15.406038 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m17:58:15.406600 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:58:15.407199 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:16.238384 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m17:58:16.242987 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (execute): 17:58:15.392680 => 17:58:16.242580
[0m17:58:16.243856 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: ROLLBACK
[0m17:58:16.244549 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:16.245245 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m17:58:16.367741 [info ] [Thread-2  ]: 18 of 22 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.99s]
[0m17:58:16.369249 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m17:58:16.370275 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m17:58:16.371109 [info ] [Thread-2  ]: 19 of 22 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:58:16.372608 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32, now test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd)
[0m17:58:16.373333 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m17:58:16.389880 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m17:58:16.399646 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (compile): 17:58:16.373831 => 17:58:16.399163
[0m17:58:16.400523 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m17:58:16.404722 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m17:58:16.413568 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:16.414212 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m17:58:16.414786 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:58:16.415416 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:17.510566 [debug] [Thread-2  ]: SQL status: OK in 1.100000023841858 seconds
[0m17:58:17.514685 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (execute): 17:58:16.401125 => 17:58:17.514325
[0m17:58:17.515658 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: ROLLBACK
[0m17:58:17.516479 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:17.517184 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m17:58:17.630485 [info ] [Thread-2  ]: 19 of 22 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.26s]
[0m17:58:17.631791 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m17:58:17.632595 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m17:58:17.633303 [info ] [Thread-2  ]: 20 of 22 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:58:17.634576 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd, now test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719)
[0m17:58:17.635341 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m17:58:17.643563 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m17:58:17.653855 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (compile): 17:58:17.635807 => 17:58:17.653405
[0m17:58:17.654666 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m17:58:17.658893 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m17:58:17.668587 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:17.669329 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m17:58:17.669955 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:58:17.670711 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:18.630910 [debug] [Thread-2  ]: SQL status: OK in 0.9599999785423279 seconds
[0m17:58:18.634889 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (execute): 17:58:17.655237 => 17:58:18.634540
[0m17:58:18.635693 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: ROLLBACK
[0m17:58:18.636372 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:18.637069 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m17:58:18.747699 [info ] [Thread-2  ]: 20 of 22 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 1.11s]
[0m17:58:18.749195 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m17:58:18.750415 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m17:58:18.751311 [info ] [Thread-2  ]: 21 of 22 START test unique_silver_links_movie_id ............................... [RUN]
[0m17:58:18.752642 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719, now test.my_dbt_project.unique_silver_links_movie_id.c422609054)
[0m17:58:18.753381 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m17:58:18.762769 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m17:58:18.773093 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (compile): 17:58:18.753963 => 17:58:18.772654
[0m17:58:18.773934 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m17:58:18.778312 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m17:58:18.789060 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:18.789787 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m17:58:18.790358 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:58:18.790949 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:19.211142 [debug] [Thread-2  ]: SQL status: OK in 0.41999998688697815 seconds
[0m17:58:19.215310 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (execute): 17:58:18.774453 => 17:58:19.214987
[0m17:58:19.216060 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: ROLLBACK
[0m17:58:19.216867 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:19.217541 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m17:58:19.331597 [info ] [Thread-2  ]: 21 of 22 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 0.58s]
[0m17:58:19.332942 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m17:58:19.333761 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m17:58:19.334510 [info ] [Thread-2  ]: 22 of 22 START test unique_silver_movies_movie_id .............................. [RUN]
[0m17:58:19.335776 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_silver_links_movie_id.c422609054, now test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085)
[0m17:58:19.336493 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m17:58:19.343400 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m17:58:19.350926 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (compile): 17:58:19.337258 => 17:58:19.350555
[0m17:58:19.351584 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m17:58:19.364248 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m17:58:19.372237 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:19.372985 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m17:58:19.373508 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:58:19.374075 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:19.913747 [debug] [Thread-2  ]: SQL status: OK in 0.5400000214576721 seconds
[0m17:58:19.918350 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (execute): 17:58:19.352019 => 17:58:19.917961
[0m17:58:19.919186 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: ROLLBACK
[0m17:58:19.919985 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:19.920732 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m17:58:20.034266 [info ] [Thread-2  ]: 22 of 22 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 0.70s]
[0m17:58:20.035769 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m17:58:20.038536 [debug] [MainThread]: On master: ROLLBACK
[0m17:58:20.039157 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:58:20.373843 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:58:20.374823 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:20.375447 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:58:20.376079 [debug] [MainThread]: On master: ROLLBACK
[0m17:58:20.376745 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:58:20.377275 [debug] [MainThread]: On master: Close
[0m17:58:20.478506 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:58:20.479318 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085' was properly closed.
[0m17:58:20.481840 [info ] [MainThread]: 
[0m17:58:20.483205 [info ] [MainThread]: Finished running 22 tests in 0 hours 0 minutes and 20.55 seconds (20.55s).
[0m17:58:20.488270 [debug] [MainThread]: Command end result
[0m17:58:20.515542 [info ] [MainThread]: 
[0m17:58:20.520182 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:58:20.521119 [info ] [MainThread]: 
[0m17:58:20.521986 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=0 SKIP=0 TOTAL=22
[0m17:58:20.523569 [debug] [MainThread]: Command `dbt test` succeeded at 17:58:20.523400 after 23.26 seconds
[0m17:58:20.524281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50ac0835b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f507f31ac40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f507df9d490>]}
[0m17:58:20.524982 [debug] [MainThread]: Flushing usage events
[0m17:58:24.477115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d33307550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d318af160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d318af8b0>]}


============================== 17:58:24.484161 | fa580611-3fed-4f8c-b64a-380f548a19a9 ==============================
[0m17:58:24.484161 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:58:24.484984 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:58:25.394837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fa580611-3fed-4f8c-b64a-380f548a19a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d318af100>]}
[0m17:58:25.426649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fa580611-3fed-4f8c-b64a-380f548a19a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d065e9850>]}
[0m17:58:25.427720 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m17:58:25.470932 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:58:26.648979 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:58:26.649692 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:58:26.660508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fa580611-3fed-4f8c-b64a-380f548a19a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d065a03a0>]}
[0m17:58:26.691526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fa580611-3fed-4f8c-b64a-380f548a19a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d064ce3d0>]}
[0m17:58:26.692464 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m17:58:26.693217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa580611-3fed-4f8c-b64a-380f548a19a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d064ce430>]}
[0m17:58:26.697371 [info ] [MainThread]: 
[0m17:58:26.698754 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:58:26.701434 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m17:58:26.708413 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m17:58:26.708992 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m17:58:26.709678 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:58:27.291359 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m17:58:27.295029 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m17:58:27.403654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa580611-3fed-4f8c-b64a-380f548a19a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d064ce040>]}
[0m17:58:27.411323 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:27.421148 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:58:27.422523 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:58:27.423434 [info ] [MainThread]: 
[0m17:58:27.438544 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m17:58:27.439481 [info ] [Thread-2  ]: 1 of 35 START test accepted_rating_range_genre_popularity_avg_rating__5__0 ..... [RUN]
[0m17:58:27.440846 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413)
[0m17:58:27.441635 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m17:58:27.456105 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m17:58:27.465330 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (compile): 17:58:27.442198 => 17:58:27.464994
[0m17:58:27.465990 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m17:58:27.491920 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m17:58:27.501645 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:27.502339 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m17:58:27.502911 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m17:58:27.503528 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:28.303738 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m17:58:28.312110 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (execute): 17:58:27.466461 => 17:58:28.311721
[0m17:58:28.313003 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: ROLLBACK
[0m17:58:28.313760 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:28.314590 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: Close
[0m17:58:28.423336 [info ] [Thread-2  ]: 1 of 35 PASS accepted_rating_range_genre_popularity_avg_rating__5__0 ........... [[32mPASS[0m in 0.98s]
[0m17:58:28.424850 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m17:58:28.425869 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m17:58:28.426759 [info ] [Thread-2  ]: 2 of 35 START test accepted_rating_range_movie_enriched_avg_rating__5__0 ....... [RUN]
[0m17:58:28.428007 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413, now test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8)
[0m17:58:28.428715 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m17:58:28.435089 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m17:58:28.444752 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (compile): 17:58:28.429257 => 17:58:28.444470
[0m17:58:28.445450 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m17:58:28.449345 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m17:58:28.457567 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:28.458116 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m17:58:28.458656 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m17:58:28.459249 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:29.145772 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m17:58:29.150116 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (execute): 17:58:28.445976 => 17:58:29.149796
[0m17:58:29.151049 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: ROLLBACK
[0m17:58:29.151877 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:29.152704 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: Close
[0m17:58:29.267235 [info ] [Thread-2  ]: 2 of 35 PASS accepted_rating_range_movie_enriched_avg_rating__5__0 ............. [[32mPASS[0m in 0.84s]
[0m17:58:29.268663 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m17:58:29.269580 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m17:58:29.270482 [info ] [Thread-2  ]: 3 of 35 START test accepted_rating_range_movie_performance_weighted_avg_rating__5__0  [RUN]
[0m17:58:29.272034 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8, now test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083)
[0m17:58:29.272903 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m17:58:29.282078 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m17:58:29.292145 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (compile): 17:58:29.273593 => 17:58:29.291816
[0m17:58:29.292880 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m17:58:29.296828 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m17:58:29.305998 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:29.306535 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m17:58:29.307064 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating < 0
   or weighted_avg_rating > 5


      
    ) dbt_internal_test
[0m17:58:29.307686 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:30.034942 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m17:58:30.039092 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (execute): 17:58:29.293399 => 17:58:30.038769
[0m17:58:30.039890 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: ROLLBACK
[0m17:58:30.040618 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:30.041324 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: Close
[0m17:58:30.155064 [info ] [Thread-2  ]: 3 of 35 PASS accepted_rating_range_movie_performance_weighted_avg_rating__5__0 . [[32mPASS[0m in 0.88s]
[0m17:58:30.156425 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m17:58:30.157231 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m17:58:30.158063 [info ] [Thread-2  ]: 4 of 35 START test accepted_rating_range_user_activity_avg_user_rating__5__0 ... [RUN]
[0m17:58:30.159462 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083, now test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a)
[0m17:58:30.160276 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m17:58:30.172248 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m17:58:30.182022 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (compile): 17:58:30.160978 => 17:58:30.181615
[0m17:58:30.182754 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m17:58:30.186809 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m17:58:30.196191 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:30.196821 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m17:58:30.197371 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating < 0
   or avg_user_rating > 5


      
    ) dbt_internal_test
[0m17:58:30.198020 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:30.894635 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m17:58:30.898787 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (execute): 17:58:30.183247 => 17:58:30.898466
[0m17:58:30.899622 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: ROLLBACK
[0m17:58:30.900470 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:30.901240 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: Close
[0m17:58:31.014187 [info ] [Thread-2  ]: 4 of 35 PASS accepted_rating_range_user_activity_avg_user_rating__5__0 ......... [[32mPASS[0m in 0.85s]
[0m17:58:31.015856 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m17:58:31.016707 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m17:58:31.017563 [info ] [Thread-2  ]: 5 of 35 START test not_null_genre_popularity_avg_rating ........................ [RUN]
[0m17:58:31.018888 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a, now test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9)
[0m17:58:31.019568 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m17:58:31.029682 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m17:58:31.039426 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (compile): 17:58:31.020080 => 17:58:31.039039
[0m17:58:31.040147 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m17:58:31.044117 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m17:58:31.053315 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:31.053915 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m17:58:31.054489 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating is null



      
    ) dbt_internal_test
[0m17:58:31.055068 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:31.730038 [debug] [Thread-2  ]: SQL status: OK in 0.6700000166893005 seconds
[0m17:58:31.734414 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (execute): 17:58:31.040679 => 17:58:31.734084
[0m17:58:31.735197 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: ROLLBACK
[0m17:58:31.735917 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:31.736527 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: Close
[0m17:58:31.844872 [info ] [Thread-2  ]: 5 of 35 PASS not_null_genre_popularity_avg_rating .............................. [[32mPASS[0m in 0.83s]
[0m17:58:31.846348 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m17:58:31.847313 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m17:58:31.848166 [info ] [Thread-2  ]: 6 of 35 START test not_null_genre_popularity_first_rating_date ................. [RUN]
[0m17:58:31.849649 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9, now test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32)
[0m17:58:31.850500 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m17:58:31.856907 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m17:58:31.865952 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (compile): 17:58:31.851050 => 17:58:31.865631
[0m17:58:31.866650 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m17:58:31.870368 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m17:58:31.879838 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:31.880485 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m17:58:31.881035 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where first_rating_date is null



      
    ) dbt_internal_test
[0m17:58:31.881633 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:32.557484 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m17:58:32.561685 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (execute): 17:58:31.867134 => 17:58:32.561350
[0m17:58:32.562553 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: ROLLBACK
[0m17:58:32.563483 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:32.564261 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: Close
[0m17:58:32.678746 [info ] [Thread-2  ]: 6 of 35 PASS not_null_genre_popularity_first_rating_date ....................... [[32mPASS[0m in 0.83s]
[0m17:58:32.680271 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m17:58:32.681187 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m17:58:32.682110 [info ] [Thread-2  ]: 7 of 35 START test not_null_genre_popularity_genres ............................ [RUN]
[0m17:58:32.683546 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32, now test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa)
[0m17:58:32.684297 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m17:58:32.698194 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m17:58:32.707870 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (compile): 17:58:32.684813 => 17:58:32.707479
[0m17:58:32.708578 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m17:58:32.712814 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m17:58:32.721678 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:32.722277 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m17:58:32.722875 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`genre_popularity`
where genres is null



      
    ) dbt_internal_test
[0m17:58:32.723479 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:33.477121 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m17:58:33.481201 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (execute): 17:58:32.709063 => 17:58:33.480821
[0m17:58:33.481981 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: ROLLBACK
[0m17:58:33.482770 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:33.483460 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: Close
[0m17:58:33.588962 [info ] [Thread-2  ]: 7 of 35 PASS not_null_genre_popularity_genres .................................. [[32mPASS[0m in 0.91s]
[0m17:58:33.590244 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m17:58:33.591085 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m17:58:33.591892 [info ] [Thread-2  ]: 8 of 35 START test not_null_genre_popularity_last_rating_date .................. [RUN]
[0m17:58:33.593219 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa, now test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432)
[0m17:58:33.593973 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m17:58:33.602673 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m17:58:33.612704 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (compile): 17:58:33.594560 => 17:58:33.612290
[0m17:58:33.613510 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m17:58:33.618752 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m17:58:33.629333 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:33.630110 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m17:58:33.630714 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where last_rating_date is null



      
    ) dbt_internal_test
[0m17:58:33.631410 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:34.376643 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m17:58:34.380362 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (execute): 17:58:33.614273 => 17:58:34.380073
[0m17:58:34.381184 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: ROLLBACK
[0m17:58:34.382186 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:34.382802 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: Close
[0m17:58:34.815664 [info ] [Thread-2  ]: 8 of 35 PASS not_null_genre_popularity_last_rating_date ........................ [[32mPASS[0m in 1.22s]
[0m17:58:34.817191 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m17:58:34.818165 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m17:58:34.818981 [info ] [Thread-2  ]: 9 of 35 START test not_null_genre_popularity_popularity_rank ................... [RUN]
[0m17:58:34.820375 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432, now test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc)
[0m17:58:34.821149 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m17:58:34.830406 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m17:58:34.840093 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (compile): 17:58:34.821683 => 17:58:34.839750
[0m17:58:34.840777 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m17:58:34.845021 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m17:58:34.853754 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:34.854277 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m17:58:34.854867 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select popularity_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where popularity_rank is null



      
    ) dbt_internal_test
[0m17:58:34.855460 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:35.590293 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m17:58:35.594877 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (execute): 17:58:34.841248 => 17:58:35.594540
[0m17:58:35.595764 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: ROLLBACK
[0m17:58:35.596661 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:35.597326 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: Close
[0m17:58:35.707827 [info ] [Thread-2  ]: 9 of 35 PASS not_null_genre_popularity_popularity_rank ......................... [[32mPASS[0m in 0.89s]
[0m17:58:35.709453 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m17:58:35.710381 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m17:58:35.711237 [info ] [Thread-2  ]: 10 of 35 START test not_null_genre_popularity_quality_rank ..................... [RUN]
[0m17:58:35.712632 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc, now test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a)
[0m17:58:35.713404 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m17:58:35.724231 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m17:58:35.734409 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (compile): 17:58:35.713973 => 17:58:35.733993
[0m17:58:35.735165 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m17:58:35.739156 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m17:58:35.748096 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:35.748692 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m17:58:35.749332 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select quality_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where quality_rank is null



      
    ) dbt_internal_test
[0m17:58:35.750033 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:36.488024 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m17:58:36.492363 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (execute): 17:58:35.735643 => 17:58:36.492023
[0m17:58:36.493319 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: ROLLBACK
[0m17:58:36.494087 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:36.494801 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: Close
[0m17:58:36.605516 [info ] [Thread-2  ]: 10 of 35 PASS not_null_genre_popularity_quality_rank ........................... [[32mPASS[0m in 0.89s]
[0m17:58:36.607081 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m17:58:36.608363 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m17:58:36.609344 [info ] [Thread-2  ]: 11 of 35 START test not_null_genre_popularity_rating_count ..................... [RUN]
[0m17:58:36.610578 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a, now test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856)
[0m17:58:36.611262 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m17:58:36.617789 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m17:58:36.627719 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (compile): 17:58:36.611739 => 17:58:36.627153
[0m17:58:36.628554 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m17:58:36.632832 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m17:58:36.642793 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:36.643579 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m17:58:36.644222 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`genre_popularity`
where rating_count is null



      
    ) dbt_internal_test
[0m17:58:36.644928 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:37.752461 [debug] [Thread-2  ]: SQL status: OK in 1.1100000143051147 seconds
[0m17:58:37.756584 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (execute): 17:58:36.629025 => 17:58:37.756244
[0m17:58:37.757343 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: ROLLBACK
[0m17:58:37.758085 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:37.758702 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: Close
[0m17:58:37.871672 [info ] [Thread-2  ]: 11 of 35 PASS not_null_genre_popularity_rating_count ........................... [[32mPASS[0m in 1.26s]
[0m17:58:37.873056 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m17:58:37.873891 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m17:58:37.874606 [info ] [Thread-2  ]: 12 of 35 START test not_null_movie_enriched_avg_rating ......................... [RUN]
[0m17:58:37.875840 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856, now test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd)
[0m17:58:37.876573 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m17:58:37.882645 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m17:58:37.892377 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (compile): 17:58:37.877079 => 17:58:37.891941
[0m17:58:37.893098 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m17:58:37.897385 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m17:58:37.906961 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:37.907623 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m17:58:37.908261 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating is null



      
    ) dbt_internal_test
[0m17:58:37.908929 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:38.890358 [debug] [Thread-2  ]: SQL status: OK in 0.9800000190734863 seconds
[0m17:58:38.894583 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (execute): 17:58:37.893564 => 17:58:38.894245
[0m17:58:38.895329 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: ROLLBACK
[0m17:58:38.895930 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:38.896520 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: Close
[0m17:58:38.998436 [info ] [Thread-2  ]: 12 of 35 PASS not_null_movie_enriched_avg_rating ............................... [[32mPASS[0m in 1.12s]
[0m17:58:38.999824 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m17:58:39.000737 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m17:58:39.001552 [info ] [Thread-2  ]: 13 of 35 START test not_null_movie_enriched_genres ............................. [RUN]
[0m17:58:39.002731 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd, now test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27)
[0m17:58:39.003376 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m17:58:39.009717 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m17:58:39.019673 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (compile): 17:58:39.003976 => 17:58:39.019092
[0m17:58:39.020492 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m17:58:39.034713 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m17:58:39.044635 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:39.045337 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m17:58:39.045884 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`movie_enriched`
where genres is null



      
    ) dbt_internal_test
[0m17:58:39.046501 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:39.837785 [debug] [Thread-2  ]: SQL status: OK in 0.7900000214576721 seconds
[0m17:58:39.842299 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (execute): 17:58:39.021005 => 17:58:39.841953
[0m17:58:39.843165 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: ROLLBACK
[0m17:58:39.843870 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:39.844583 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: Close
[0m17:58:39.951061 [info ] [Thread-2  ]: 13 of 35 PASS not_null_movie_enriched_genres ................................... [[32mPASS[0m in 0.95s]
[0m17:58:39.952441 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m17:58:39.953408 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m17:58:39.954198 [info ] [Thread-2  ]: 14 of 35 START test not_null_movie_enriched_movie_id ........................... [RUN]
[0m17:58:39.955348 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27, now test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86)
[0m17:58:39.955993 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m17:58:39.962717 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m17:58:39.973212 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (compile): 17:58:39.956520 => 17:58:39.972780
[0m17:58:39.973993 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m17:58:39.978247 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m17:58:39.988205 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:39.988892 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m17:58:39.989484 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is null



      
    ) dbt_internal_test
[0m17:58:39.990186 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:40.738949 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m17:58:40.743789 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (execute): 17:58:39.974453 => 17:58:40.743430
[0m17:58:40.744695 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: ROLLBACK
[0m17:58:40.745349 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:40.745930 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: Close
[0m17:58:40.862204 [info ] [Thread-2  ]: 14 of 35 PASS not_null_movie_enriched_movie_id ................................. [[32mPASS[0m in 0.91s]
[0m17:58:40.863598 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m17:58:40.864422 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m17:58:40.865217 [info ] [Thread-2  ]: 15 of 35 START test not_null_movie_enriched_title .............................. [RUN]
[0m17:58:40.866460 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86, now test.my_dbt_project.not_null_movie_enriched_title.d20f00c292)
[0m17:58:40.867135 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m17:58:40.873790 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m17:58:40.885153 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (compile): 17:58:40.867633 => 17:58:40.884541
[0m17:58:40.886121 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m17:58:40.890507 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m17:58:40.902545 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:40.903416 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m17:58:40.904414 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`movie_enriched`
where title is null



      
    ) dbt_internal_test
[0m17:58:40.905106 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:41.583280 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m17:58:41.587379 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (execute): 17:58:40.886626 => 17:58:41.587078
[0m17:58:41.588121 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: ROLLBACK
[0m17:58:41.588829 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:41.589489 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: Close
[0m17:58:41.697768 [info ] [Thread-2  ]: 15 of 35 PASS not_null_movie_enriched_title .................................... [[32mPASS[0m in 0.83s]
[0m17:58:41.699261 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m17:58:41.700220 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m17:58:41.701177 [info ] [Thread-2  ]: 16 of 35 START test not_null_movie_enriched_total_ratings ...................... [RUN]
[0m17:58:41.702559 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_title.d20f00c292, now test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04)
[0m17:58:41.703428 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m17:58:41.710229 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m17:58:41.720474 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (compile): 17:58:41.704105 => 17:58:41.719989
[0m17:58:41.721310 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m17:58:41.734731 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m17:58:41.744570 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:41.745301 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m17:58:41.745895 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select total_ratings
from `workspace`.`movielens_volume`.`movie_enriched`
where total_ratings is null



      
    ) dbt_internal_test
[0m17:58:41.746555 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:42.510632 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m17:58:42.514978 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (execute): 17:58:41.721817 => 17:58:42.514507
[0m17:58:42.515930 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: ROLLBACK
[0m17:58:42.516687 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:42.517454 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: Close
[0m17:58:42.627144 [info ] [Thread-2  ]: 16 of 35 PASS not_null_movie_enriched_total_ratings ............................ [[32mPASS[0m in 0.92s]
[0m17:58:42.628534 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m17:58:42.629323 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m17:58:42.630017 [info ] [Thread-2  ]: 17 of 35 START test not_null_movie_performance_genre_rank ...................... [RUN]
[0m17:58:42.631204 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04, now test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e)
[0m17:58:42.631905 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m17:58:42.641584 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m17:58:42.652091 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (compile): 17:58:42.632749 => 17:58:42.651649
[0m17:58:42.652852 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m17:58:42.657514 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m17:58:42.668343 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:42.669342 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m17:58:42.670135 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genre_rank
from `workspace`.`movielens_volume`.`movie_performance`
where genre_rank is null



      
    ) dbt_internal_test
[0m17:58:42.670915 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:43.363346 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m17:58:43.367577 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (execute): 17:58:42.653331 => 17:58:43.367243
[0m17:58:43.368309 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: ROLLBACK
[0m17:58:43.369219 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:43.369909 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: Close
[0m17:58:43.790156 [info ] [Thread-2  ]: 17 of 35 PASS not_null_movie_performance_genre_rank ............................ [[32mPASS[0m in 1.16s]
[0m17:58:43.791610 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m17:58:43.792490 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m17:58:43.793198 [info ] [Thread-2  ]: 18 of 35 START test not_null_movie_performance_movie_id ........................ [RUN]
[0m17:58:43.794470 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e, now test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52)
[0m17:58:43.795084 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m17:58:43.803747 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m17:58:43.813954 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (compile): 17:58:43.795540 => 17:58:43.813577
[0m17:58:43.814654 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m17:58:43.818575 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m17:58:43.828395 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:43.829127 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m17:58:43.829812 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is null



      
    ) dbt_internal_test
[0m17:58:43.830519 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:44.683378 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m17:58:44.688421 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (execute): 17:58:43.815117 => 17:58:44.687997
[0m17:58:44.689378 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: ROLLBACK
[0m17:58:44.690175 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:44.690879 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: Close
[0m17:58:44.810333 [info ] [Thread-2  ]: 18 of 35 PASS not_null_movie_performance_movie_id .............................. [[32mPASS[0m in 1.02s]
[0m17:58:44.812433 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m17:58:44.813867 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m17:58:44.814736 [info ] [Thread-2  ]: 19 of 35 START test not_null_movie_performance_polarization_score .............. [RUN]
[0m17:58:44.816123 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52, now test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6)
[0m17:58:44.816846 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m17:58:44.829712 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m17:58:44.844379 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (compile): 17:58:44.817314 => 17:58:44.843824
[0m17:58:44.845484 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m17:58:44.855053 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m17:58:44.867203 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:44.867993 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m17:58:44.868837 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select polarization_score
from `workspace`.`movielens_volume`.`movie_performance`
where polarization_score is null



      
    ) dbt_internal_test
[0m17:58:44.869815 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:45.759503 [debug] [Thread-2  ]: SQL status: OK in 0.8899999856948853 seconds
[0m17:58:45.763882 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (execute): 17:58:44.846082 => 17:58:45.763452
[0m17:58:45.764794 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: ROLLBACK
[0m17:58:45.765522 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:45.766174 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: Close
[0m17:58:45.880258 [info ] [Thread-2  ]: 19 of 35 PASS not_null_movie_performance_polarization_score .................... [[32mPASS[0m in 1.06s]
[0m17:58:45.881958 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m17:58:45.882897 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m17:58:45.883686 [info ] [Thread-2  ]: 20 of 35 START test not_null_movie_performance_rating_count .................... [RUN]
[0m17:58:45.885276 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6, now test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac)
[0m17:58:45.885987 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m17:58:45.896205 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m17:58:45.907011 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (compile): 17:58:45.886482 => 17:58:45.906553
[0m17:58:45.907800 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m17:58:45.912472 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m17:58:45.923456 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:45.924328 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m17:58:45.925161 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`movie_performance`
where rating_count is null



      
    ) dbt_internal_test
[0m17:58:45.925894 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:46.668709 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m17:58:46.673407 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (execute): 17:58:45.908333 => 17:58:46.672985
[0m17:58:46.674301 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: ROLLBACK
[0m17:58:46.674988 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:46.675602 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: Close
[0m17:58:46.787361 [info ] [Thread-2  ]: 20 of 35 PASS not_null_movie_performance_rating_count .......................... [[32mPASS[0m in 0.90s]
[0m17:58:46.788792 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m17:58:46.789707 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m17:58:46.790482 [info ] [Thread-2  ]: 21 of 35 START test not_null_movie_performance_top_rating_percentage ........... [RUN]
[0m17:58:46.791841 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac, now test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb)
[0m17:58:46.792511 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m17:58:46.798827 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m17:58:46.808139 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (compile): 17:58:46.793044 => 17:58:46.807752
[0m17:58:46.808861 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m17:58:46.812898 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m17:58:46.821979 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:46.822556 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m17:58:46.823205 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_rating_percentage
from `workspace`.`movielens_volume`.`movie_performance`
where top_rating_percentage is null



      
    ) dbt_internal_test
[0m17:58:46.823784 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:47.567579 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m17:58:47.571945 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (execute): 17:58:46.809376 => 17:58:47.571582
[0m17:58:47.572804 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: ROLLBACK
[0m17:58:47.573591 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:47.574274 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: Close
[0m17:58:47.685043 [info ] [Thread-2  ]: 21 of 35 PASS not_null_movie_performance_top_rating_percentage ................. [[32mPASS[0m in 0.89s]
[0m17:58:47.686404 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m17:58:47.687307 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m17:58:47.688046 [info ] [Thread-2  ]: 22 of 35 START test not_null_movie_performance_weighted_avg_rating ............. [RUN]
[0m17:58:47.689346 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb, now test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5)
[0m17:58:47.690159 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m17:58:47.696817 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m17:58:47.705798 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (compile): 17:58:47.690751 => 17:58:47.705442
[0m17:58:47.706544 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m17:58:47.716209 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m17:58:47.725913 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:47.726594 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m17:58:47.727164 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select weighted_avg_rating
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating is null



      
    ) dbt_internal_test
[0m17:58:47.727762 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:48.469245 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m17:58:48.475546 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (execute): 17:58:47.707097 => 17:58:48.474955
[0m17:58:48.476726 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: ROLLBACK
[0m17:58:48.477711 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:48.478709 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: Close
[0m17:58:48.593638 [info ] [Thread-2  ]: 22 of 35 PASS not_null_movie_performance_weighted_avg_rating ................... [[32mPASS[0m in 0.90s]
[0m17:58:48.595011 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m17:58:48.596002 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m17:58:48.596895 [info ] [Thread-2  ]: 23 of 35 START test not_null_user_activity_avg_user_rating ..................... [RUN]
[0m17:58:48.598327 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5, now test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b)
[0m17:58:48.599075 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m17:58:48.605511 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m17:58:48.615750 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (compile): 17:58:48.599630 => 17:58:48.615377
[0m17:58:48.616443 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m17:58:48.620316 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m17:58:48.630467 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:48.631476 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m17:58:48.632146 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_user_rating
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating is null



      
    ) dbt_internal_test
[0m17:58:48.632893 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:49.224870 [debug] [Thread-2  ]: SQL status: OK in 0.5899999737739563 seconds
[0m17:58:49.228848 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (execute): 17:58:48.616896 => 17:58:49.228552
[0m17:58:49.229533 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: ROLLBACK
[0m17:58:49.230169 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:49.230809 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: Close
[0m17:58:49.338460 [info ] [Thread-2  ]: 23 of 35 PASS not_null_user_activity_avg_user_rating ........................... [[32mPASS[0m in 0.74s]
[0m17:58:49.340372 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m17:58:49.341595 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m17:58:49.342617 [info ] [Thread-2  ]: 24 of 35 START test not_null_user_activity_first_activity_date ................. [RUN]
[0m17:58:49.344399 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b, now test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac)
[0m17:58:49.345504 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m17:58:49.359824 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m17:58:49.372360 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (compile): 17:58:49.347526 => 17:58:49.371873
[0m17:58:49.373271 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m17:58:49.377895 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m17:58:49.388100 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:49.388850 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m17:58:49.389426 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where first_activity_date is null



      
    ) dbt_internal_test
[0m17:58:49.390061 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:50.051483 [debug] [Thread-2  ]: SQL status: OK in 0.6600000262260437 seconds
[0m17:58:50.055833 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (execute): 17:58:49.373804 => 17:58:50.055521
[0m17:58:50.056616 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: ROLLBACK
[0m17:58:50.057311 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:50.058089 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: Close
[0m17:58:50.174353 [info ] [Thread-2  ]: 24 of 35 PASS not_null_user_activity_first_activity_date ....................... [[32mPASS[0m in 0.83s]
[0m17:58:50.175931 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m17:58:50.176813 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m17:58:50.177530 [info ] [Thread-2  ]: 25 of 35 START test not_null_user_activity_influence_score ..................... [RUN]
[0m17:58:50.178768 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac, now test.my_dbt_project.not_null_user_activity_influence_score.758852d325)
[0m17:58:50.179418 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m17:58:50.185666 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m17:58:50.195021 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (compile): 17:58:50.179911 => 17:58:50.194494
[0m17:58:50.195875 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m17:58:50.208652 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m17:58:50.218120 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:50.218741 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m17:58:50.219325 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select influence_score
from `workspace`.`movielens_volume`.`user_activity`
where influence_score is null



      
    ) dbt_internal_test
[0m17:58:50.219965 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:50.892467 [debug] [Thread-2  ]: SQL status: OK in 0.6700000166893005 seconds
[0m17:58:50.896779 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (execute): 17:58:50.196336 => 17:58:50.896415
[0m17:58:50.897625 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: ROLLBACK
[0m17:58:50.898302 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:50.898933 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: Close
[0m17:58:51.013896 [info ] [Thread-2  ]: 25 of 35 PASS not_null_user_activity_influence_score ........................... [[32mPASS[0m in 0.84s]
[0m17:58:51.015203 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m17:58:51.015986 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m17:58:51.016739 [info ] [Thread-2  ]: 26 of 35 START test not_null_user_activity_last_activity_date .................. [RUN]
[0m17:58:51.018037 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_influence_score.758852d325, now test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad)
[0m17:58:51.018743 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m17:58:51.027194 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m17:58:51.036296 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (compile): 17:58:51.019311 => 17:58:51.035968
[0m17:58:51.036955 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m17:58:51.040669 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m17:58:51.050286 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:51.050946 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m17:58:51.051583 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where last_activity_date is null



      
    ) dbt_internal_test
[0m17:58:51.052152 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:51.731782 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m17:58:51.735537 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (execute): 17:58:51.037416 => 17:58:51.735228
[0m17:58:51.736248 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: ROLLBACK
[0m17:58:51.737246 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:51.737878 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: Close
[0m17:58:51.850768 [info ] [Thread-2  ]: 26 of 35 PASS not_null_user_activity_last_activity_date ........................ [[32mPASS[0m in 0.83s]
[0m17:58:51.852188 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m17:58:51.853159 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m17:58:51.854054 [info ] [Thread-2  ]: 27 of 35 START test not_null_user_activity_top_genre ........................... [RUN]
[0m17:58:51.855313 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad, now test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313)
[0m17:58:51.856005 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m17:58:51.864630 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m17:58:51.873890 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (compile): 17:58:51.856514 => 17:58:51.873503
[0m17:58:51.874650 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m17:58:51.878799 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m17:58:51.888556 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:51.889216 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m17:58:51.889810 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_genre
from `workspace`.`movielens_volume`.`user_activity`
where top_genre is null



      
    ) dbt_internal_test
[0m17:58:51.890425 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:52.608956 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m17:58:52.613466 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (execute): 17:58:51.875226 => 17:58:52.613091
[0m17:58:52.614381 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: ROLLBACK
[0m17:58:52.615109 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:52.615820 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: Close
[0m17:58:52.727575 [info ] [Thread-2  ]: 27 of 35 PASS not_null_user_activity_top_genre ................................. [[32mPASS[0m in 0.87s]
[0m17:58:52.729584 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m17:58:52.730680 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m17:58:52.731453 [info ] [Thread-2  ]: 28 of 35 START test not_null_user_activity_user_id ............................. [RUN]
[0m17:58:52.732996 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313, now test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36)
[0m17:58:52.733959 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m17:58:52.745577 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m17:58:52.756066 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (compile): 17:58:52.734591 => 17:58:52.755584
[0m17:58:52.756893 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m17:58:52.762897 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m17:58:52.774713 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:52.775784 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m17:58:52.776680 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`user_activity`
where user_id is null



      
    ) dbt_internal_test
[0m17:58:52.777663 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:53.336254 [debug] [Thread-2  ]: SQL status: OK in 0.5600000023841858 seconds
[0m17:58:53.340823 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (execute): 17:58:52.757465 => 17:58:53.340425
[0m17:58:53.341681 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: ROLLBACK
[0m17:58:53.342420 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:53.343121 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: Close
[0m17:58:53.465914 [info ] [Thread-2  ]: 28 of 35 PASS not_null_user_activity_user_id ................................... [[32mPASS[0m in 0.73s]
[0m17:58:53.467301 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m17:58:53.468146 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m17:58:53.468869 [info ] [Thread-2  ]: 29 of 35 START test not_null_user_activity_user_rating_count ................... [RUN]
[0m17:58:53.470156 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36, now test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e)
[0m17:58:53.470979 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m17:58:53.480532 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m17:58:53.490140 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (compile): 17:58:53.471653 => 17:58:53.489717
[0m17:58:53.490923 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m17:58:53.494995 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m17:58:53.504349 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:53.504882 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m17:58:53.505397 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_rating_count
from `workspace`.`movielens_volume`.`user_activity`
where user_rating_count is null



      
    ) dbt_internal_test
[0m17:58:53.505957 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:54.043422 [debug] [Thread-2  ]: SQL status: OK in 0.5400000214576721 seconds
[0m17:58:54.047369 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (execute): 17:58:53.491448 => 17:58:54.047060
[0m17:58:54.048066 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: ROLLBACK
[0m17:58:54.048737 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:54.049422 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: Close
[0m17:58:54.163105 [info ] [Thread-2  ]: 29 of 35 PASS not_null_user_activity_user_rating_count ......................... [[32mPASS[0m in 0.69s]
[0m17:58:54.164536 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m17:58:54.165472 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:58:54.166319 [info ] [Thread-2  ]: 30 of 35 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:58:54.167570 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m17:58:54.168185 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:58:54.178979 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:58:54.188404 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 17:58:54.168625 => 17:58:54.188001
[0m17:58:54.189127 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:58:54.193216 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:58:54.202455 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:54.203125 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m17:58:54.203796 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:58:54.204532 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:54.767922 [debug] [Thread-2  ]: SQL status: OK in 0.5600000023841858 seconds
[0m17:58:54.772247 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 17:58:54.189595 => 17:58:54.771916
[0m17:58:54.773015 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m17:58:54.773772 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:54.774557 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m17:58:54.877902 [info ] [Thread-2  ]: 30 of 35 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.71s]
[0m17:58:54.879247 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m17:58:54.880021 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:58:54.880807 [info ] [Thread-2  ]: 31 of 35 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m17:58:54.882511 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m17:58:54.883322 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:58:54.892073 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:58:54.902709 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 17:58:54.883872 => 17:58:54.902203
[0m17:58:54.903805 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:58:54.908297 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:58:54.918955 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:54.919718 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m17:58:54.920383 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m17:58:54.921044 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:55.489272 [debug] [Thread-2  ]: SQL status: OK in 0.5699999928474426 seconds
[0m17:58:55.493817 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 17:58:54.904640 => 17:58:55.493358
[0m17:58:55.494701 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m17:58:55.495443 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:55.496036 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m17:58:55.611955 [info ] [Thread-2  ]: 31 of 35 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.73s]
[0m17:58:55.613209 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m17:58:55.614060 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m17:58:55.614914 [info ] [Thread-2  ]: 32 of 35 START test unique_genre_popularity_genres ............................. [RUN]
[0m17:58:55.616146 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075)
[0m17:58:55.616788 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m17:58:55.625989 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m17:58:55.635581 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (compile): 17:58:55.617300 => 17:58:55.635222
[0m17:58:55.636296 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m17:58:55.640541 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m17:58:55.649507 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:55.650128 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m17:58:55.650695 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    genres as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`genre_popularity`
where genres is not null
group by genres
having count(*) > 1



      
    ) dbt_internal_test
[0m17:58:55.651296 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:56.208730 [debug] [Thread-2  ]: SQL status: OK in 0.5600000023841858 seconds
[0m17:58:56.212795 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (execute): 17:58:55.636810 => 17:58:56.212488
[0m17:58:56.213493 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: ROLLBACK
[0m17:58:56.214254 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:56.214893 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: Close
[0m17:58:56.333165 [info ] [Thread-2  ]: 32 of 35 PASS unique_genre_popularity_genres ................................... [[32mPASS[0m in 0.72s]
[0m17:58:56.334627 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m17:58:56.335570 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m17:58:56.336405 [info ] [Thread-2  ]: 33 of 35 START test unique_movie_enriched_movie_id ............................. [RUN]
[0m17:58:56.337692 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075, now test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a)
[0m17:58:56.338406 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m17:58:56.344338 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m17:58:56.352174 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (compile): 17:58:56.338927 => 17:58:56.351806
[0m17:58:56.352798 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m17:58:56.356835 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m17:58:56.364279 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:56.364969 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m17:58:56.365614 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:58:56.366242 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:56.901332 [debug] [Thread-2  ]: SQL status: OK in 0.5400000214576721 seconds
[0m17:58:56.905650 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (execute): 17:58:56.353212 => 17:58:56.905287
[0m17:58:56.906393 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: ROLLBACK
[0m17:58:56.907394 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:56.908086 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m17:58:57.019632 [info ] [Thread-2  ]: 33 of 35 PASS unique_movie_enriched_movie_id ................................... [[32mPASS[0m in 0.68s]
[0m17:58:57.021064 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m17:58:57.022070 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m17:58:57.022900 [info ] [Thread-2  ]: 34 of 35 START test unique_movie_performance_movie_id .......................... [RUN]
[0m17:58:57.024212 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a, now test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22)
[0m17:58:57.024804 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m17:58:57.031249 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m17:58:57.040050 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (compile): 17:58:57.025316 => 17:58:57.039492
[0m17:58:57.040903 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m17:58:57.045226 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m17:58:57.053670 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:57.054385 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m17:58:57.055073 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:58:57.055772 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:57.603126 [debug] [Thread-2  ]: SQL status: OK in 0.550000011920929 seconds
[0m17:58:57.613462 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (execute): 17:58:57.041396 => 17:58:57.611887
[0m17:58:57.614516 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: ROLLBACK
[0m17:58:57.615314 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:57.616007 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: Close
[0m17:58:57.735093 [info ] [Thread-2  ]: 34 of 35 PASS unique_movie_performance_movie_id ................................ [[32mPASS[0m in 0.71s]
[0m17:58:57.736600 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m17:58:57.737473 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m17:58:57.738252 [info ] [Thread-2  ]: 35 of 35 START test unique_user_activity_user_id ............................... [RUN]
[0m17:58:57.739726 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22, now test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda)
[0m17:58:57.740417 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m17:58:57.747471 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m17:58:57.757548 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (compile): 17:58:57.740922 => 17:58:57.757127
[0m17:58:57.758287 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m17:58:57.762465 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m17:58:57.771709 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:57.772358 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m17:58:57.772963 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`user_activity`
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:58:57.773582 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m17:58:58.302302 [debug] [Thread-2  ]: SQL status: OK in 0.5299999713897705 seconds
[0m17:58:58.307335 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (execute): 17:58:57.758787 => 17:58:58.306896
[0m17:58:58.308367 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: ROLLBACK
[0m17:58:58.309137 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m17:58:58.310011 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: Close
[0m17:58:58.729187 [info ] [Thread-2  ]: 35 of 35 PASS unique_user_activity_user_id ..................................... [[32mPASS[0m in 0.99s]
[0m17:58:58.730722 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m17:58:58.733618 [debug] [MainThread]: On master: ROLLBACK
[0m17:58:58.734528 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:58:59.118083 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:58:59.119196 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:58:59.120095 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:58:59.120866 [debug] [MainThread]: On master: ROLLBACK
[0m17:58:59.121570 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:58:59.122197 [debug] [MainThread]: On master: Close
[0m17:58:59.227437 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:58:59.228264 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda' was properly closed.
[0m17:58:59.231449 [info ] [MainThread]: 
[0m17:58:59.233689 [info ] [MainThread]: Finished running 35 tests in 0 hours 0 minutes and 32.53 seconds (32.53s).
[0m17:58:59.241747 [debug] [MainThread]: Command end result
[0m17:58:59.277336 [info ] [MainThread]: 
[0m17:58:59.278406 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:58:59.279481 [info ] [MainThread]: 
[0m17:58:59.280425 [info ] [MainThread]: Done. PASS=35 WARN=0 ERROR=0 SKIP=0 TOTAL=35
[0m17:58:59.281869 [debug] [MainThread]: Command `dbt test` succeeded at 17:58:59.281673 after 34.95 seconds
[0m17:58:59.282690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d33307550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d065e9850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7d062b55e0>]}
[0m17:58:59.283637 [debug] [MainThread]: Flushing usage events
[0m17:59:13.999932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6b21475e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6b06b0250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6b06b0970>]}


============================== 17:59:14.007210 | 7363e00b-90aa-41dd-9f03-a7f46fea9f8d ==============================
[0m17:59:14.007210 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:59:14.008235 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'profiles_dir': '/opt/airflow/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:59:15.007820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7363e00b-90aa-41dd-9f03-a7f46fea9f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6b06b01f0>]}
[0m17:59:15.042217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7363e00b-90aa-41dd-9f03-a7f46fea9f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc68941a9d0>]}
[0m17:59:15.043318 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m17:59:15.088435 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:59:16.259474 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:59:16.260191 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:59:16.271524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7363e00b-90aa-41dd-9f03-a7f46fea9f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6893a5430>]}
[0m17:59:16.305318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7363e00b-90aa-41dd-9f03-a7f46fea9f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6892d1460>]}
[0m17:59:16.306262 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m17:59:16.307015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7363e00b-90aa-41dd-9f03-a7f46fea9f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6892d14c0>]}
[0m17:59:16.309574 [info ] [MainThread]: 
[0m17:59:16.310888 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:59:16.312918 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:59:16.313595 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:59:16.314176 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m17:59:16.314689 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:59:16.887049 [debug] [ThreadPool]: SQL status: OK in 0.5699999928474426 seconds
[0m17:59:16.889129 [debug] [ThreadPool]: On list_workspace: Close
[0m17:59:17.002696 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m17:59:17.004207 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m17:59:17.018498 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:17.019400 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m17:59:17.020028 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m17:59:17.020559 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:59:17.654213 [debug] [ThreadPool]: SQL status: OK in 0.6299999952316284 seconds
[0m17:59:17.655919 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m17:59:17.656617 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m17:59:17.657446 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:59:17.658122 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m17:59:17.809644 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m17:59:17.818180 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m17:59:17.818988 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m17:59:17.819557 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:59:18.400068 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m17:59:18.403803 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m17:59:18.520558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7363e00b-90aa-41dd-9f03-a7f46fea9f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6b3cdddc0>]}
[0m17:59:18.521491 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:18.522163 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:59:18.523201 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:59:18.524091 [info ] [MainThread]: 
[0m17:59:18.539599 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_links
[0m17:59:18.540470 [info ] [Thread-4  ]: 1 of 4 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m17:59:18.541679 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.silver_links)
[0m17:59:18.542400 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_links
[0m17:59:18.552397 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m17:59:18.561096 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (compile): 17:59:18.542901 => 17:59:18.560670
[0m17:59:18.561783 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_links
[0m17:59:18.581227 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:18.582454 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m17:59:18.583158 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

      describe extended `workspace`.`movielens_volume`.`silver_links`
  
[0m17:59:18.583879 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:59:19.072459 [debug] [Thread-4  ]: SQL status: OK in 0.49000000953674316 seconds
[0m17:59:19.130422 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m17:59:19.138974 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m17:59:19.139738 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
    using delta
      
      
      
      
      
      
      as
      

with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m17:59:21.096450 [debug] [Thread-4  ]: SQL status: OK in 1.9600000381469727 seconds
[0m17:59:21.130508 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (execute): 17:59:18.562297 => 17:59:21.130227
[0m17:59:21.131651 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: ROLLBACK
[0m17:59:21.132560 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:59:21.133274 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: Close
[0m17:59:21.243082 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7363e00b-90aa-41dd-9f03-a7f46fea9f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc67bf4af10>]}
[0m17:59:21.244341 [info ] [Thread-4  ]: 1 of 4 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 2.70s]
[0m17:59:21.245731 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_links
[0m17:59:21.246506 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_movies
[0m17:59:21.247365 [info ] [Thread-4  ]: 2 of 4 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m17:59:21.248790 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_links, now model.my_dbt_project.silver_movies)
[0m17:59:21.249475 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_movies
[0m17:59:21.254937 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m17:59:21.262870 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (compile): 17:59:21.249974 => 17:59:21.262574
[0m17:59:21.263483 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_movies
[0m17:59:21.272716 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:21.273319 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m17:59:21.273831 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

      describe extended `workspace`.`movielens_volume`.`silver_movies`
  
[0m17:59:21.274418 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:59:21.896779 [debug] [Thread-4  ]: SQL status: OK in 0.6200000047683716 seconds
[0m17:59:21.903078 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m17:59:21.911478 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m17:59:21.912102 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m17:59:23.459325 [debug] [Thread-4  ]: SQL status: OK in 1.5499999523162842 seconds
[0m17:59:23.463241 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (execute): 17:59:21.263919 => 17:59:23.462927
[0m17:59:23.464123 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: ROLLBACK
[0m17:59:23.465023 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:59:23.465876 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: Close
[0m17:59:23.583750 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7363e00b-90aa-41dd-9f03-a7f46fea9f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc67bf60ca0>]}
[0m17:59:23.585047 [info ] [Thread-4  ]: 2 of 4 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 2.34s]
[0m17:59:23.586342 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_movies
[0m17:59:23.587256 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_ratings
[0m17:59:23.588076 [info ] [Thread-4  ]: 3 of 4 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m17:59:23.589298 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_movies, now model.my_dbt_project.silver_ratings)
[0m17:59:23.590068 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_ratings
[0m17:59:23.595575 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m17:59:23.604195 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (compile): 17:59:23.590760 => 17:59:23.603908
[0m17:59:23.604892 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_ratings
[0m17:59:23.610643 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:23.611176 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m17:59:23.611698 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

      describe extended `workspace`.`movielens_volume`.`silver_ratings`
  
[0m17:59:23.612286 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:59:24.221784 [debug] [Thread-4  ]: SQL status: OK in 0.6100000143051147 seconds
[0m17:59:24.229099 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m17:59:24.237862 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m17:59:24.238660 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m17:59:26.959746 [debug] [Thread-4  ]: SQL status: OK in 2.7200000286102295 seconds
[0m17:59:26.963538 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (execute): 17:59:23.605324 => 17:59:26.963260
[0m17:59:26.964295 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: ROLLBACK
[0m17:59:26.965109 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:59:26.965795 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: Close
[0m17:59:27.078145 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7363e00b-90aa-41dd-9f03-a7f46fea9f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc67bf60490>]}
[0m17:59:27.079484 [info ] [Thread-4  ]: 3 of 4 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 3.49s]
[0m17:59:27.080835 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_ratings
[0m17:59:27.081934 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_tags
[0m17:59:27.082966 [info ] [Thread-4  ]: 4 of 4 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m17:59:27.084180 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_ratings, now model.my_dbt_project.silver_tags)
[0m17:59:27.084838 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_tags
[0m17:59:27.096042 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m17:59:27.104872 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (compile): 17:59:27.085333 => 17:59:27.104418
[0m17:59:27.105575 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_tags
[0m17:59:27.111417 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:27.111971 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m17:59:27.112577 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

      describe extended `workspace`.`movielens_volume`.`silver_tags`
  
[0m17:59:27.113277 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:59:27.721377 [debug] [Thread-4  ]: SQL status: OK in 0.6100000143051147 seconds
[0m17:59:27.727421 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m17:59:27.735979 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m17:59:27.736675 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m17:59:29.996936 [debug] [Thread-4  ]: SQL status: OK in 2.259999990463257 seconds
[0m17:59:30.000583 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (execute): 17:59:27.106092 => 17:59:30.000306
[0m17:59:30.001576 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: ROLLBACK
[0m17:59:30.002391 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:59:30.003159 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: Close
[0m17:59:30.113510 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7363e00b-90aa-41dd-9f03-a7f46fea9f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc67bf729a0>]}
[0m17:59:30.114796 [info ] [Thread-4  ]: 4 of 4 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 3.03s]
[0m17:59:30.116028 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_tags
[0m17:59:30.118446 [debug] [MainThread]: On master: ROLLBACK
[0m17:59:30.119020 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:59:30.467376 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:59:30.468272 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:30.469030 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:59:30.469667 [debug] [MainThread]: On master: ROLLBACK
[0m17:59:30.470375 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:59:30.471028 [debug] [MainThread]: On master: Close
[0m17:59:30.572526 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:59:30.573479 [debug] [MainThread]: Connection 'model.my_dbt_project.silver_tags' was properly closed.
[0m17:59:30.575970 [info ] [MainThread]: 
[0m17:59:30.577571 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 14.27 seconds (14.27s).
[0m17:59:30.579802 [debug] [MainThread]: Command end result
[0m17:59:30.605251 [info ] [MainThread]: 
[0m17:59:30.606218 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:59:30.606907 [info ] [MainThread]: 
[0m17:59:30.607605 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m17:59:30.608759 [debug] [MainThread]: Command `dbt run` succeeded at 17:59:30.608628 after 16.75 seconds
[0m17:59:30.609335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc6b21475e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc689542430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc68941a6d0>]}
[0m17:59:30.609931 [debug] [MainThread]: Flushing usage events
[0m17:59:35.008455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75d7a06610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75d5f6f220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75d5f6f970>]}


============================== 17:59:35.015687 | 220f16a8-4332-4f86-8c8c-bec1d10b99ac ==============================
[0m17:59:35.015687 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:59:35.016685 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/opt/airflow/dbt', 'log_path': '/opt/airflow/dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:59:35.937764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '220f16a8-4332-4f86-8c8c-bec1d10b99ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75d5f6f1c0>]}
[0m17:59:35.969691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '220f16a8-4332-4f86-8c8c-bec1d10b99ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75aacb9550>]}
[0m17:59:35.970821 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m17:59:36.014623 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:59:37.200968 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:59:37.201686 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:59:37.212895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '220f16a8-4332-4f86-8c8c-bec1d10b99ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75aac45460>]}
[0m17:59:37.246014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '220f16a8-4332-4f86-8c8c-bec1d10b99ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75aab71490>]}
[0m17:59:37.246896 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m17:59:37.247696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '220f16a8-4332-4f86-8c8c-bec1d10b99ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75aab714f0>]}
[0m17:59:37.250362 [info ] [MainThread]: 
[0m17:59:37.251765 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:59:37.253907 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:59:37.254586 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:59:37.255188 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m17:59:37.255777 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:59:38.469419 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m17:59:38.471433 [debug] [ThreadPool]: On list_workspace: Close
[0m17:59:38.580935 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m17:59:38.582430 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m17:59:38.596517 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:38.597259 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m17:59:38.598002 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m17:59:38.598912 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:59:39.583609 [debug] [ThreadPool]: SQL status: OK in 0.9800000190734863 seconds
[0m17:59:39.585758 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m17:59:39.586532 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m17:59:39.587265 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:59:39.587957 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m17:59:39.720289 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m17:59:39.729661 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m17:59:39.730445 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m17:59:39.731174 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:59:40.325751 [debug] [ThreadPool]: SQL status: OK in 0.5899999737739563 seconds
[0m17:59:40.329673 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m17:59:40.452244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '220f16a8-4332-4f86-8c8c-bec1d10b99ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75aab77910>]}
[0m17:59:40.453337 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:40.454020 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:59:40.455356 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:59:40.456161 [info ] [MainThread]: 
[0m17:59:40.470847 [debug] [Thread-4  ]: Began running node model.my_dbt_project.genre_popularity
[0m17:59:40.471987 [info ] [Thread-4  ]: 1 of 5 START sql incremental model movielens_volume.genre_popularity ........... [RUN]
[0m17:59:40.473281 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.genre_popularity)
[0m17:59:40.473962 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.genre_popularity
[0m17:59:40.492491 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.genre_popularity"
[0m17:59:40.501557 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (compile): 17:59:40.474521 => 17:59:40.501102
[0m17:59:40.502344 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.genre_popularity
[0m17:59:40.546975 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:40.547772 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:59:40.548440 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m17:59:40.549073 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:59:41.161217 [debug] [Thread-4  ]: SQL status: OK in 0.6100000143051147 seconds
[0m17:59:41.188970 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:59:41.189753 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

  
    create temporary view `genre_popularity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings with Genres
-- Join silver ratings with silver movies to attach genres
-- Prepare rating timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        r.rating,
        r.rating_date as rating_timestamp,  
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Aggregate Genre Metrics
-- For each genre, calculate total ratings, avg rating,
-- and distribution insights
-- =======================================================
genre_stats as (
    select
        genres,  -- genres already formatted with spacing
        count(*) as rating_count,
        round(avg(rating), 2) as avg_rating,  
        to_date(min(rating_timestamp)) as first_rating_date,  
        to_date(max(rating_timestamp)) as last_rating_date   
    from ratings_base
    group by genres
),

-- =======================================================
-- Step 3: Ranking Genres
-- Rank genres by popularity (# of ratings)
-- and quality (avg rating)
-- =======================================================
ranked_genres as (
    select
        genres,
        rating_count,
        avg_rating,
        first_rating_date,
        last_rating_date,
        rank() over (order by rating_count desc) as popularity_rank,
        rank() over (order by avg_rating desc) as quality_rank
    from genre_stats
)

-- =======================================================
-- Step 4: Final Selection
-- One row per genre with all metrics
-- =======================================================
select
    genres,
    rating_count,
    avg_rating,
    popularity_rank,
    quality_rank,
    first_rating_date,
    last_rating_date
from ranked_genres


  -- Only insert new genres on incremental runs
  where genres not in (select genres from `workspace`.`movielens_volume`.`genre_popularity`)

  
[0m17:59:41.466547 [debug] [Thread-4  ]: SQL status: OK in 0.2800000011920929 seconds
[0m17:59:41.495593 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:59:41.496521 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m17:59:41.707884 [debug] [Thread-4  ]: SQL status: OK in 0.20999999344348907 seconds
[0m17:59:41.721049 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.genre_popularity"
[0m17:59:41.729462 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m17:59:41.730174 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`genre_popularity` as DBT_INTERNAL_DEST
      using `genre_popularity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.genres = DBT_INTERNAL_DEST.genres
          

      when matched then update set
         * 

      when not matched then insert *

[0m17:59:46.790609 [debug] [Thread-4  ]: SQL status: OK in 5.059999942779541 seconds
[0m17:59:46.826302 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (execute): 17:59:40.502910 => 17:59:46.826029
[0m17:59:46.827178 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: ROLLBACK
[0m17:59:46.827822 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:59:46.828462 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: Close
[0m17:59:46.937033 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '220f16a8-4332-4f86-8c8c-bec1d10b99ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75a980e640>]}
[0m17:59:46.938512 [info ] [Thread-4  ]: 1 of 5 OK created sql incremental model movielens_volume.genre_popularity ...... [[32mOK[0m in 6.46s]
[0m17:59:46.939869 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.genre_popularity
[0m17:59:46.940719 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_enriched
[0m17:59:46.941551 [info ] [Thread-4  ]: 2 of 5 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m17:59:46.942774 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.genre_popularity, now model.my_dbt_project.movie_enriched)
[0m17:59:46.943473 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_enriched
[0m17:59:46.949389 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m17:59:46.959108 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (compile): 17:59:46.943997 => 17:59:46.958563
[0m17:59:46.959917 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_enriched
[0m17:59:46.976807 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:46.977586 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m17:59:46.978273 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

      describe extended `workspace`.`movielens_volume`.`movie_enriched`
  
[0m17:59:46.978896 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:59:47.959853 [debug] [Thread-4  ]: SQL status: OK in 0.9800000190734863 seconds
[0m17:59:48.007324 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m17:59:48.015853 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m17:59:48.016681 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m17:59:49.994340 [debug] [Thread-4  ]: SQL status: OK in 1.9800000190734863 seconds
[0m17:59:50.002652 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (execute): 17:59:46.960411 => 17:59:50.002344
[0m17:59:50.003470 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: ROLLBACK
[0m17:59:50.004084 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:59:50.004690 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: Close
[0m17:59:50.114617 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '220f16a8-4332-4f86-8c8c-bec1d10b99ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75a980e280>]}
[0m17:59:50.115839 [info ] [Thread-4  ]: 2 of 5 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 3.17s]
[0m17:59:50.117029 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_enriched
[0m17:59:50.117788 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_performance
[0m17:59:50.118666 [info ] [Thread-4  ]: 3 of 5 START sql incremental model movielens_volume.movie_performance .......... [RUN]
[0m17:59:50.120088 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_enriched, now model.my_dbt_project.movie_performance)
[0m17:59:50.120855 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_performance
[0m17:59:50.128534 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_performance"
[0m17:59:50.136512 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (compile): 17:59:50.121405 => 17:59:50.136127
[0m17:59:50.137177 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_performance
[0m17:59:50.146764 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:50.147389 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:59:50.148150 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m17:59:50.148931 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:59:50.743505 [debug] [Thread-4  ]: SQL status: OK in 0.5899999737739563 seconds
[0m17:59:50.748947 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:59:50.750057 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

  
    create temporary view `movie_performance__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings Join
-- Attach movie metadata (title, genres) to ratings
-- =======================================================
with ratings_base as (
    select
        r.movie_id,
        m.title,
        regexp_replace(m.genres, '\\|', ' | ') as genres,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_date
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Weighted Rating
-- Compute avg rating, rating count, weighted avg
-- =======================================================
weighted_rating as (
    select
        movie_id,
        title,
        genres,
        round(avg(rating), 2) as avg_rating,
        count(*) as rating_count,
        round(sum(rating) / count(*), 2) as weighted_avg_rating
    from ratings_base
    group by movie_id, title, genres
),

-- =======================================================
-- Step 3: Polarization Score
-- Standard deviation of ratings per movie
-- =======================================================
polarization as (
    select
        movie_id,
        round(coalesce(stddev(rating), 0.0), 2) as polarization_score
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 4: Top Rating Percentage
-- Percent of ratings >= 4.5
-- =======================================================
top_rating_pct as (
    select
        movie_id,
        round(100.0 * sum(case when rating >= 4.5 then 1 else 0 end) / count(*), 2) as top_rating_percentage
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 5: Genre Ranking
-- Rank movies within each genre by weighted_avg_rating
-- =======================================================
genre_rank as (
    select
        movie_id,
        genres,
        weighted_avg_rating,
        rank() over (partition by genres order by weighted_avg_rating desc) as genre_rank
    from weighted_rating
),

-- =======================================================
-- Step 6: Tag Count
-- Count distinct tags per movie
-- =======================================================
tag_count as (
    select
        movie_id,
        count(distinct tag) as tag_count
    from `workspace`.`movielens_volume`.`silver_tags`
    group by movie_id
)

-- =======================================================
-- Step 7: Final Selection
-- Combine all metrics into the gold table
-- =======================================================
select
    w.movie_id,
    w.title,
    w.genres,
    w.weighted_avg_rating,
    w.rating_count,
    g.genre_rank,
    p.polarization_score,
    tr.top_rating_percentage,
    coalesce(tc.tag_count, 0) as tag_count
from weighted_rating w
join genre_rank g
    on w.movie_id = g.movie_id
join polarization p
    on w.movie_id = p.movie_id
join top_rating_pct tr
    on w.movie_id = tr.movie_id
left join tag_count tc
    on w.movie_id = tc.movie_id


-- Use NOT EXISTS for safer incremental inserts
where not exists (
    select 1 
    from `workspace`.`movielens_volume`.`movie_performance` t
    where t.movie_id = w.movie_id
)

  
[0m17:59:51.030765 [debug] [Thread-4  ]: SQL status: OK in 0.2800000011920929 seconds
[0m17:59:51.035790 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:59:51.036542 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m17:59:51.264520 [debug] [Thread-4  ]: SQL status: OK in 0.23000000417232513 seconds
[0m17:59:51.269000 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_performance"
[0m17:59:51.276909 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m17:59:51.277604 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`movie_performance` as DBT_INTERNAL_DEST
      using `movie_performance__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.movie_id = DBT_INTERNAL_DEST.movie_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m17:59:55.265530 [debug] [Thread-4  ]: SQL status: OK in 3.990000009536743 seconds
[0m17:59:55.269024 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (execute): 17:59:50.137666 => 17:59:55.268713
[0m17:59:55.269794 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: ROLLBACK
[0m17:59:55.270617 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:59:55.271415 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: Close
[0m17:59:55.381762 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '220f16a8-4332-4f86-8c8c-bec1d10b99ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75a980edc0>]}
[0m17:59:55.383010 [info ] [Thread-4  ]: 3 of 5 OK created sql incremental model movielens_volume.movie_performance ..... [[32mOK[0m in 5.26s]
[0m17:59:55.384402 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_performance
[0m17:59:55.385311 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_activity
[0m17:59:55.386247 [info ] [Thread-4  ]: 4 of 5 START sql incremental model movielens_volume.user_activity .............. [RUN]
[0m17:59:55.387601 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_performance, now model.my_dbt_project.user_activity)
[0m17:59:55.388437 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_activity
[0m17:59:55.395677 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_activity"
[0m17:59:55.403756 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (compile): 17:59:55.389096 => 17:59:55.403400
[0m17:59:55.404494 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_activity
[0m17:59:55.410640 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:59:55.411297 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:59:55.412117 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m17:59:55.412871 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m17:59:56.043019 [debug] [Thread-4  ]: SQL status: OK in 0.6299999952316284 seconds
[0m17:59:56.048114 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:59:56.049068 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

  
    create temporary view `user_activity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Join ratings with movies to attach genres and prepare timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_timestamp,
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: User Activity (General)
-- Calculate total ratings, average rating, first and last activity dates per user
-- =======================================================
user_activity as (
    select
        user_id,
        count(*) as user_rating_count,
        round(avg(rating), 2) as avg_user_rating,
        min(rating_timestamp) as first_activity_date,
        max(rating_timestamp) as last_activity_date
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 3: User Influence Score
-- Compute influence based on total ratings (more ratings = higher influence)
-- =======================================================
user_influence as (
    select
        user_id,
        user_rating_count as influence_score
    from user_activity
),

-- =======================================================
-- Step 4: Active Users by Genre
-- Count ratings per genre and rank genres per user
-- =======================================================
user_genre_activity as (
    select
        user_id,
        regexp_replace(genres, '\\|', ' | ') as genres,
        count(*) as genre_rating_count,
        rank() over (partition by user_id order by count(*) desc) as genre_rank
    from ratings_base
    group by user_id, genres
),

-- =======================================================
-- Step 5: New vs Loyal Users
-- Categorize users as new if first activity < 30 days ago, else loyal
-- =======================================================
user_type as (
    select
        user_id,
        case
            when min(rating_timestamp) >= date_sub(current_date(), 30)
                then 'new_user'
            else 'loyal_user'
        end as user_category
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 6: Top Genre per User
-- Identify the most active genre for each user
-- =======================================================
top_genre as (
    select
        user_id,
        max(genres) as top_genre
    from user_genre_activity
    group by user_id
)

-- =======================================================
-- Final Selection
-- Combine general activity, influence, type, and top genre per user
-- =======================================================
select
    ua.user_id,
    ua.user_rating_count,
    ua.avg_user_rating,
    ua.first_activity_date,
    ua.last_activity_date,
    ui.influence_score,
    ut.user_category,
    tg.top_genre
from user_activity ua
join user_influence ui on ua.user_id = ui.user_id
join user_type ut on ua.user_id = ut.user_id
left join top_genre tg on ua.user_id = tg.user_id


  where ua.user_id not in (select user_id from `workspace`.`movielens_volume`.`user_activity`)

  
[0m17:59:56.374599 [debug] [Thread-4  ]: SQL status: OK in 0.3199999928474426 seconds
[0m17:59:56.384152 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:59:56.384957 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m17:59:56.626360 [debug] [Thread-4  ]: SQL status: OK in 0.23999999463558197 seconds
[0m17:59:56.631944 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_activity"
[0m17:59:56.640235 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m17:59:56.640870 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_activity` as DBT_INTERNAL_DEST
      using `user_activity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m17:59:59.913509 [debug] [Thread-4  ]: SQL status: OK in 3.2699999809265137 seconds
[0m17:59:59.917787 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (execute): 17:59:55.404955 => 17:59:59.917475
[0m17:59:59.918753 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: ROLLBACK
[0m17:59:59.919450 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m17:59:59.920237 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: Close
[0m18:00:00.037842 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '220f16a8-4332-4f86-8c8c-bec1d10b99ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75a98bec10>]}
[0m18:00:00.038970 [info ] [Thread-4  ]: 4 of 5 OK created sql incremental model movielens_volume.user_activity ......... [[32mOK[0m in 4.65s]
[0m18:00:00.040084 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_activity
[0m18:00:00.040851 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_preferences
[0m18:00:00.041776 [info ] [Thread-4  ]: 5 of 5 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m18:00:00.043080 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.user_activity, now model.my_dbt_project.user_preferences)
[0m18:00:00.043724 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_preferences
[0m18:00:00.051465 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m18:00:00.060160 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (compile): 18:00:00.044254 => 18:00:00.059722
[0m18:00:00.060955 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_preferences
[0m18:00:00.068193 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:00.069089 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m18:00:00.069748 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m18:00:00.070436 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m18:00:00.844763 [debug] [Thread-4  ]: SQL status: OK in 0.7699999809265137 seconds
[0m18:00:00.849951 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m18:00:00.851178 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create temporary view `user_preferences__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score,  -- influence based on total ratings
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )

  
[0m18:00:01.572873 [debug] [Thread-4  ]: SQL status: OK in 0.7200000286102295 seconds
[0m18:00:01.578002 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m18:00:01.578858 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m18:00:01.881475 [debug] [Thread-4  ]: SQL status: OK in 0.30000001192092896 seconds
[0m18:00:01.886145 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m18:00:01.894711 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m18:00:01.895452 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
      using `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m18:00:08.090765 [debug] [Thread-4  ]: SQL status: OK in 6.190000057220459 seconds
[0m18:00:08.216478 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (execute): 18:00:00.061649 => 18:00:08.216186
[0m18:00:08.217392 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: ROLLBACK
[0m18:00:08.218333 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:08.219146 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: Close
[0m18:00:08.336420 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '220f16a8-4332-4f86-8c8c-bec1d10b99ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75a993ec40>]}
[0m18:00:08.337655 [info ] [Thread-4  ]: 5 of 5 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 8.29s]
[0m18:00:08.338901 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_preferences
[0m18:00:08.341586 [debug] [MainThread]: On master: ROLLBACK
[0m18:00:08.342269 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:00:08.703945 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:00:08.704863 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:08.705618 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:00:08.706424 [debug] [MainThread]: On master: ROLLBACK
[0m18:00:08.707137 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:00:08.707845 [debug] [MainThread]: On master: Close
[0m18:00:08.824179 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:00:08.824956 [debug] [MainThread]: Connection 'model.my_dbt_project.user_preferences' was properly closed.
[0m18:00:08.827218 [info ] [MainThread]: 
[0m18:00:08.828909 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 31.58 seconds (31.58s).
[0m18:00:08.831208 [debug] [MainThread]: Command end result
[0m18:00:08.863914 [info ] [MainThread]: 
[0m18:00:08.865042 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:00:08.865905 [info ] [MainThread]: 
[0m18:00:08.866735 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m18:00:08.867795 [debug] [MainThread]: Command `dbt run` succeeded at 18:00:08.867658 after 34.03 seconds
[0m18:00:08.868448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75d7a06610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75aa9fb490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75aacb9550>]}
[0m18:00:08.869173 [debug] [MainThread]: Flushing usage events
[0m18:00:13.330457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55e4143580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55e26af190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55e26af8e0>]}


============================== 18:00:13.339921 | 4d154c98-44d4-4309-a3ae-59ad46f1eaed ==============================
[0m18:00:13.339921 [info ] [MainThread]: Running with dbt=1.5.2
[0m18:00:13.341205 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m18:00:14.330419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4d154c98-44d4-4309-a3ae-59ad46f1eaed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55e26af130>]}
[0m18:00:14.362816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4d154c98-44d4-4309-a3ae-59ad46f1eaed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b73dab80>]}
[0m18:00:14.364030 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m18:00:14.413590 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m18:00:15.684177 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:00:15.684944 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:00:15.696810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4d154c98-44d4-4309-a3ae-59ad46f1eaed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b73913d0>]}
[0m18:00:15.732732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4d154c98-44d4-4309-a3ae-59ad46f1eaed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b72bf400>]}
[0m18:00:15.733647 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m18:00:15.734379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d154c98-44d4-4309-a3ae-59ad46f1eaed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b72bf460>]}
[0m18:00:15.738014 [info ] [MainThread]: 
[0m18:00:15.739363 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:00:15.742319 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m18:00:15.748925 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m18:00:15.749662 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m18:00:15.750254 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:00:16.395110 [debug] [ThreadPool]: SQL status: OK in 0.6399999856948853 seconds
[0m18:00:16.398486 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m18:00:16.507369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d154c98-44d4-4309-a3ae-59ad46f1eaed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b72bf070>]}
[0m18:00:16.508348 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:16.508978 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:00:16.510035 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:00:16.510746 [info ] [MainThread]: 
[0m18:00:16.527044 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m18:00:16.528028 [info ] [Thread-2  ]: 1 of 22 START test accepted_rating_range_silver_ratings_rating__5__0 ........... [RUN]
[0m18:00:16.529604 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff)
[0m18:00:16.530251 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m18:00:16.542606 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m18:00:16.554681 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (compile): 18:00:16.530726 => 18:00:16.554206
[0m18:00:16.555499 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m18:00:16.587662 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m18:00:16.597611 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:16.598404 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m18:00:16.599427 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`silver_ratings`
where rating < 0
   or rating > 5


      
    ) dbt_internal_test
[0m18:00:16.600325 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:17.353672 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m18:00:17.363001 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (execute): 18:00:16.556188 => 18:00:17.362627
[0m18:00:17.363903 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: ROLLBACK
[0m18:00:17.364668 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:17.365508 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: Close
[0m18:00:17.474422 [info ] [Thread-2  ]: 1 of 22 PASS accepted_rating_range_silver_ratings_rating__5__0 ................. [[32mPASS[0m in 0.95s]
[0m18:00:17.475912 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m18:00:17.476692 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m18:00:17.477452 [info ] [Thread-2  ]: 2 of 22 START test not_null_silver_links_imdb_id ............................... [RUN]
[0m18:00:17.478956 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff, now test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51)
[0m18:00:17.479725 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m18:00:17.489843 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m18:00:17.500659 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (compile): 18:00:17.480209 => 18:00:17.500182
[0m18:00:17.501491 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m18:00:17.505471 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m18:00:17.515556 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:17.516292 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m18:00:17.516956 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



      
    ) dbt_internal_test
[0m18:00:17.517677 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:18.246836 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m18:00:18.251554 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (execute): 18:00:17.501928 => 18:00:18.251141
[0m18:00:18.252468 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: ROLLBACK
[0m18:00:18.253298 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:18.253984 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m18:00:18.363342 [info ] [Thread-2  ]: 2 of 22 PASS not_null_silver_links_imdb_id ..................................... [[32mPASS[0m in 0.88s]
[0m18:00:18.364898 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m18:00:18.365729 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m18:00:18.366614 [info ] [Thread-2  ]: 3 of 22 START test not_null_silver_links_movie_id .............................. [RUN]
[0m18:00:18.368304 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51, now test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d)
[0m18:00:18.369054 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m18:00:18.378345 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m18:00:18.388742 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (compile): 18:00:18.369535 => 18:00:18.388300
[0m18:00:18.389529 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m18:00:18.393947 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m18:00:18.404233 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:18.405013 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m18:00:18.405683 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



      
    ) dbt_internal_test
[0m18:00:18.406319 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:18.973187 [debug] [Thread-2  ]: SQL status: OK in 0.5699999928474426 seconds
[0m18:00:18.977499 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (execute): 18:00:18.389985 => 18:00:18.977158
[0m18:00:18.978351 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: ROLLBACK
[0m18:00:18.979062 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:18.979714 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m18:00:19.099916 [info ] [Thread-2  ]: 3 of 22 PASS not_null_silver_links_movie_id .................................... [[32mPASS[0m in 0.73s]
[0m18:00:19.101251 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m18:00:19.102057 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m18:00:19.102824 [info ] [Thread-2  ]: 4 of 22 START test not_null_silver_links_tmdb_id ............................... [RUN]
[0m18:00:19.104089 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d, now test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f)
[0m18:00:19.104790 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m18:00:19.116143 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m18:00:19.126018 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (compile): 18:00:19.105363 => 18:00:19.125616
[0m18:00:19.126778 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m18:00:19.130683 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m18:00:19.140092 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:19.140728 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m18:00:19.141341 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



      
    ) dbt_internal_test
[0m18:00:19.141930 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:19.805192 [debug] [Thread-2  ]: SQL status: OK in 0.6600000262260437 seconds
[0m18:00:19.809202 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (execute): 18:00:19.127242 => 18:00:19.808891
[0m18:00:19.809891 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: ROLLBACK
[0m18:00:19.810532 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:19.811146 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m18:00:19.922230 [info ] [Thread-2  ]: 4 of 22 PASS not_null_silver_links_tmdb_id ..................................... [[32mPASS[0m in 0.82s]
[0m18:00:19.923747 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m18:00:19.924554 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m18:00:19.925335 [info ] [Thread-2  ]: 5 of 22 START test not_null_silver_movies_genres ............................... [RUN]
[0m18:00:19.926788 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f, now test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63)
[0m18:00:19.927502 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m18:00:19.935467 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m18:00:19.944284 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (compile): 18:00:19.927988 => 18:00:19.943912
[0m18:00:19.944978 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m18:00:19.948795 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m18:00:19.957838 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:19.958372 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m18:00:19.958905 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



      
    ) dbt_internal_test
[0m18:00:19.959464 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:20.628095 [debug] [Thread-2  ]: SQL status: OK in 0.6700000166893005 seconds
[0m18:00:20.632018 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (execute): 18:00:19.945446 => 18:00:20.631675
[0m18:00:20.632747 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: ROLLBACK
[0m18:00:20.633551 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:20.634192 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m18:00:20.740587 [info ] [Thread-2  ]: 5 of 22 PASS not_null_silver_movies_genres ..................................... [[32mPASS[0m in 0.81s]
[0m18:00:20.741871 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m18:00:20.742673 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m18:00:20.743353 [info ] [Thread-2  ]: 6 of 22 START test not_null_silver_movies_movie_id ............................. [RUN]
[0m18:00:20.744416 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63, now test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e)
[0m18:00:20.745058 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m18:00:20.750791 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m18:00:20.759538 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (compile): 18:00:20.745577 => 18:00:20.759230
[0m18:00:20.760154 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m18:00:20.763911 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m18:00:20.772307 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:20.772908 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m18:00:20.773458 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



      
    ) dbt_internal_test
[0m18:00:20.774008 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:21.481046 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m18:00:21.485104 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (execute): 18:00:20.760622 => 18:00:21.484716
[0m18:00:21.485855 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: ROLLBACK
[0m18:00:21.486614 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:21.487354 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m18:00:21.605188 [info ] [Thread-2  ]: 6 of 22 PASS not_null_silver_movies_movie_id ................................... [[32mPASS[0m in 0.86s]
[0m18:00:21.606584 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m18:00:21.607370 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m18:00:21.608122 [info ] [Thread-2  ]: 7 of 22 START test not_null_silver_movies_title ................................ [RUN]
[0m18:00:21.609369 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e, now test.my_dbt_project.not_null_silver_movies_title.3a779bd2de)
[0m18:00:21.610092 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m18:00:21.622029 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m18:00:21.631656 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (compile): 18:00:21.610626 => 18:00:21.631316
[0m18:00:21.632337 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m18:00:21.636123 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m18:00:21.644704 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:21.645325 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m18:00:21.645866 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



      
    ) dbt_internal_test
[0m18:00:21.646709 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:22.601187 [debug] [Thread-2  ]: SQL status: OK in 0.949999988079071 seconds
[0m18:00:22.605361 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (execute): 18:00:21.632800 => 18:00:22.605028
[0m18:00:22.606068 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: ROLLBACK
[0m18:00:22.606678 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:22.607227 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m18:00:22.723789 [info ] [Thread-2  ]: 7 of 22 PASS not_null_silver_movies_title ...................................... [[32mPASS[0m in 1.11s]
[0m18:00:22.725230 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m18:00:22.726137 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m18:00:22.726915 [info ] [Thread-2  ]: 8 of 22 START test not_null_silver_ratings_movie_id ............................ [RUN]
[0m18:00:22.728415 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_title.3a779bd2de, now test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36)
[0m18:00:22.729130 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m18:00:22.735943 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m18:00:22.745239 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (compile): 18:00:22.729637 => 18:00:22.744731
[0m18:00:22.745975 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m18:00:22.750378 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m18:00:22.759859 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:22.760494 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m18:00:22.761036 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



      
    ) dbt_internal_test
[0m18:00:22.761624 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:23.453463 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m18:00:23.457493 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (execute): 18:00:22.746433 => 18:00:23.457167
[0m18:00:23.458287 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: ROLLBACK
[0m18:00:23.459043 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:23.459695 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m18:00:23.579923 [info ] [Thread-2  ]: 8 of 22 PASS not_null_silver_ratings_movie_id .................................. [[32mPASS[0m in 0.85s]
[0m18:00:23.581318 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m18:00:23.582171 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m18:00:23.583017 [info ] [Thread-2  ]: 9 of 22 START test not_null_silver_ratings_rating .............................. [RUN]
[0m18:00:23.584187 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36, now test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a)
[0m18:00:23.584813 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m18:00:23.590483 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m18:00:23.600402 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (compile): 18:00:23.585352 => 18:00:23.600025
[0m18:00:23.601168 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m18:00:23.605500 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m18:00:23.614725 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:23.615375 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m18:00:23.615967 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



      
    ) dbt_internal_test
[0m18:00:23.616625 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:24.330870 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m18:00:24.336283 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (execute): 18:00:23.601632 => 18:00:24.335884
[0m18:00:24.337053 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: ROLLBACK
[0m18:00:24.337728 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:24.338502 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m18:00:24.449083 [info ] [Thread-2  ]: 9 of 22 PASS not_null_silver_ratings_rating .................................... [[32mPASS[0m in 0.87s]
[0m18:00:24.450465 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m18:00:24.451341 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m18:00:24.452146 [info ] [Thread-2  ]: 10 of 22 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m18:00:24.453485 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a, now test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f)
[0m18:00:24.454257 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m18:00:24.468285 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m18:00:24.477699 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (compile): 18:00:24.454733 => 18:00:24.477283
[0m18:00:24.478511 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m18:00:24.482858 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m18:00:24.491165 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:24.491728 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m18:00:24.492263 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



      
    ) dbt_internal_test
[0m18:00:24.492970 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:25.224116 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m18:00:25.228041 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (execute): 18:00:24.479012 => 18:00:25.227704
[0m18:00:25.228794 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: ROLLBACK
[0m18:00:25.229477 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:25.230187 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m18:00:25.362195 [info ] [Thread-2  ]: 10 of 22 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 0.91s]
[0m18:00:25.363721 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m18:00:25.364551 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m18:00:25.365303 [info ] [Thread-2  ]: 11 of 22 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m18:00:25.366565 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f, now test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3)
[0m18:00:25.367258 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m18:00:25.379684 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m18:00:25.391339 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (compile): 18:00:25.367750 => 18:00:25.390910
[0m18:00:25.392070 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m18:00:25.396221 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m18:00:25.406649 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:25.407374 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m18:00:25.407985 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



      
    ) dbt_internal_test
[0m18:00:25.408621 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:26.105189 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m18:00:26.109099 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (execute): 18:00:25.392534 => 18:00:26.108741
[0m18:00:26.109831 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: ROLLBACK
[0m18:00:26.110596 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:26.111280 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m18:00:26.233485 [info ] [Thread-2  ]: 11 of 22 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 0.87s]
[0m18:00:26.235049 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m18:00:26.235837 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m18:00:26.236521 [info ] [Thread-2  ]: 12 of 22 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m18:00:26.237721 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3, now test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3)
[0m18:00:26.238438 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m18:00:26.247109 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m18:00:26.256731 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (compile): 18:00:26.238984 => 18:00:26.256227
[0m18:00:26.257534 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m18:00:26.261451 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m18:00:26.270901 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:26.271505 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m18:00:26.272072 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



      
    ) dbt_internal_test
[0m18:00:26.272842 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:27.014529 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m18:00:27.018844 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (execute): 18:00:26.257990 => 18:00:27.018518
[0m18:00:27.019611 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: ROLLBACK
[0m18:00:27.020316 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:27.020975 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m18:00:27.128042 [info ] [Thread-2  ]: 12 of 22 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 0.89s]
[0m18:00:27.129595 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m18:00:27.130696 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m18:00:27.131579 [info ] [Thread-2  ]: 13 of 22 START test not_null_silver_tags_tag ................................... [RUN]
[0m18:00:27.132840 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3, now test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8)
[0m18:00:27.133520 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m18:00:27.143846 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m18:00:27.153293 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (compile): 18:00:27.134005 => 18:00:27.152959
[0m18:00:27.153999 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m18:00:27.157704 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m18:00:27.166546 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:27.167182 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m18:00:27.167758 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



      
    ) dbt_internal_test
[0m18:00:27.168386 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:27.862462 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m18:00:27.866648 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (execute): 18:00:27.154465 => 18:00:27.866358
[0m18:00:27.867406 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: ROLLBACK
[0m18:00:27.868219 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:27.868988 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m18:00:27.984480 [info ] [Thread-2  ]: 13 of 22 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 0.85s]
[0m18:00:27.985880 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m18:00:27.986822 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m18:00:27.987638 [info ] [Thread-2  ]: 14 of 22 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m18:00:27.988813 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8, now test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86)
[0m18:00:27.989433 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m18:00:27.995588 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m18:00:28.006353 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (compile): 18:00:27.989923 => 18:00:28.005900
[0m18:00:28.007184 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m18:00:28.011195 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m18:00:28.020815 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:28.021483 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m18:00:28.022115 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



      
    ) dbt_internal_test
[0m18:00:28.022828 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:28.749002 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m18:00:28.753291 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (execute): 18:00:28.007711 => 18:00:28.752935
[0m18:00:28.754102 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: ROLLBACK
[0m18:00:28.754812 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:28.755500 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m18:00:28.878570 [info ] [Thread-2  ]: 14 of 22 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 0.89s]
[0m18:00:28.879775 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m18:00:28.880539 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m18:00:28.881286 [info ] [Thread-2  ]: 15 of 22 START test not_null_silver_tags_user_id ............................... [RUN]
[0m18:00:28.882374 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86, now test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3)
[0m18:00:28.882963 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m18:00:28.889151 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m18:00:28.899472 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (compile): 18:00:28.883410 => 18:00:28.898972
[0m18:00:28.900274 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m18:00:28.904693 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m18:00:28.914993 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:28.915912 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m18:00:28.916699 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



      
    ) dbt_internal_test
[0m18:00:28.917439 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:29.614224 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m18:00:29.618538 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (execute): 18:00:28.900846 => 18:00:29.618053
[0m18:00:29.619498 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: ROLLBACK
[0m18:00:29.620162 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:29.620856 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m18:00:29.735148 [info ] [Thread-2  ]: 15 of 22 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 0.85s]
[0m18:00:29.736598 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m18:00:29.737435 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:00:29.738182 [info ] [Thread-2  ]: 16 of 22 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:00:29.739550 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m18:00:29.740271 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:00:29.765201 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:00:29.777204 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 18:00:29.740776 => 18:00:29.776490
[0m18:00:29.778350 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:00:29.783120 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:00:29.795507 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:29.796659 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:00:29.797626 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:00:29.798967 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:30.813359 [debug] [Thread-2  ]: SQL status: OK in 1.0099999904632568 seconds
[0m18:00:30.818372 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 18:00:29.778946 => 18:00:30.817942
[0m18:00:30.819211 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m18:00:30.819831 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:30.820421 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m18:00:30.937824 [info ] [Thread-2  ]: 16 of 22 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.20s]
[0m18:00:30.939103 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:00:30.940151 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:00:30.940892 [info ] [Thread-2  ]: 17 of 22 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:00:30.942079 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m18:00:30.942692 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:00:30.950676 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:00:30.959039 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 18:00:30.943160 => 18:00:30.958714
[0m18:00:30.959762 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:00:30.963741 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:00:30.972290 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:30.972877 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:00:30.973451 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:00:30.974149 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:31.793374 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m18:00:31.797809 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 18:00:30.960369 => 18:00:31.797493
[0m18:00:31.798748 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m18:00:31.799647 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:31.800319 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m18:00:31.907341 [info ] [Thread-2  ]: 17 of 22 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.97s]
[0m18:00:31.908783 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:00:31.909633 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m18:00:31.910367 [info ] [Thread-2  ]: 18 of 22 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:00:31.911541 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32)
[0m18:00:31.912149 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m18:00:31.919651 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m18:00:31.929490 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (compile): 18:00:31.912588 => 18:00:31.929018
[0m18:00:31.930286 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m18:00:31.934247 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m18:00:31.943735 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:31.944367 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m18:00:31.945049 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:00:31.945763 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:32.878027 [debug] [Thread-2  ]: SQL status: OK in 0.9300000071525574 seconds
[0m18:00:32.881987 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (execute): 18:00:31.930788 => 18:00:32.881659
[0m18:00:32.882707 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: ROLLBACK
[0m18:00:32.883392 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:32.884079 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m18:00:32.998304 [info ] [Thread-2  ]: 18 of 22 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.09s]
[0m18:00:32.999971 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m18:00:33.000895 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m18:00:33.001767 [info ] [Thread-2  ]: 19 of 22 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:00:33.003303 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32, now test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd)
[0m18:00:33.004116 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m18:00:33.022009 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m18:00:33.031535 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (compile): 18:00:33.004679 => 18:00:33.030941
[0m18:00:33.032361 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m18:00:33.036774 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m18:00:33.046229 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:33.047145 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m18:00:33.047783 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:00:33.048570 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:34.107651 [debug] [Thread-2  ]: SQL status: OK in 1.059999942779541 seconds
[0m18:00:34.112044 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (execute): 18:00:33.033032 => 18:00:34.111641
[0m18:00:34.112901 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: ROLLBACK
[0m18:00:34.113627 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:34.114221 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m18:00:34.232904 [info ] [Thread-2  ]: 19 of 22 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.23s]
[0m18:00:34.234301 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m18:00:34.235217 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m18:00:34.235921 [info ] [Thread-2  ]: 20 of 22 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:00:34.237184 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd, now test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719)
[0m18:00:34.237846 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m18:00:34.245878 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m18:00:34.255838 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (compile): 18:00:34.238342 => 18:00:34.255390
[0m18:00:34.256624 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m18:00:34.260578 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m18:00:34.270245 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:34.270907 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m18:00:34.271480 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:00:34.272097 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:35.236875 [debug] [Thread-2  ]: SQL status: OK in 0.9599999785423279 seconds
[0m18:00:35.241131 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (execute): 18:00:34.257113 => 18:00:35.240774
[0m18:00:35.241999 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: ROLLBACK
[0m18:00:35.242800 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:35.243450 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m18:00:35.358286 [info ] [Thread-2  ]: 20 of 22 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 1.12s]
[0m18:00:35.359815 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m18:00:35.360648 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m18:00:35.361542 [info ] [Thread-2  ]: 21 of 22 START test unique_silver_links_movie_id ............................... [RUN]
[0m18:00:35.362882 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719, now test.my_dbt_project.unique_silver_links_movie_id.c422609054)
[0m18:00:35.363683 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m18:00:35.373379 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m18:00:35.382667 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (compile): 18:00:35.364295 => 18:00:35.382259
[0m18:00:35.383391 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m18:00:35.387790 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m18:00:35.396757 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:35.397404 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m18:00:35.397989 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:00:35.398648 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:36.134920 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m18:00:36.139257 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (execute): 18:00:35.383986 => 18:00:36.138909
[0m18:00:36.140096 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: ROLLBACK
[0m18:00:36.140999 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:36.141866 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m18:00:36.265584 [info ] [Thread-2  ]: 21 of 22 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 0.90s]
[0m18:00:36.266989 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m18:00:36.267914 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m18:00:36.268781 [info ] [Thread-2  ]: 22 of 22 START test unique_silver_movies_movie_id .............................. [RUN]
[0m18:00:36.270197 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_silver_links_movie_id.c422609054, now test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085)
[0m18:00:36.270918 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m18:00:36.277236 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m18:00:36.286755 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (compile): 18:00:36.271435 => 18:00:36.286434
[0m18:00:36.287406 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m18:00:36.299913 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m18:00:36.309306 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:36.309986 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m18:00:36.310661 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:00:36.311381 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:37.069832 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m18:00:37.074259 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (execute): 18:00:36.287900 => 18:00:37.073944
[0m18:00:37.075133 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: ROLLBACK
[0m18:00:37.075858 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:37.076511 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m18:00:37.203228 [info ] [Thread-2  ]: 22 of 22 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 0.93s]
[0m18:00:37.204604 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m18:00:37.207169 [debug] [MainThread]: On master: ROLLBACK
[0m18:00:37.207793 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:00:37.864394 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:00:37.865362 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:37.866022 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:00:37.866804 [debug] [MainThread]: On master: ROLLBACK
[0m18:00:37.867628 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:00:37.868339 [debug] [MainThread]: On master: Close
[0m18:00:37.982334 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:00:37.983105 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085' was properly closed.
[0m18:00:37.985570 [info ] [MainThread]: 
[0m18:00:37.986902 [info ] [MainThread]: Finished running 22 tests in 0 hours 0 minutes and 22.25 seconds (22.25s).
[0m18:00:37.992483 [debug] [MainThread]: Command end result
[0m18:00:38.020754 [info ] [MainThread]: 
[0m18:00:38.021758 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:00:38.022486 [info ] [MainThread]: 
[0m18:00:38.023250 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=0 SKIP=0 TOTAL=22
[0m18:00:38.024424 [debug] [MainThread]: Command `dbt test` succeeded at 18:00:38.024286 after 24.85 seconds
[0m18:00:38.025053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55e4143580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b73dab80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b73ec6a0>]}
[0m18:00:38.025799 [debug] [MainThread]: Flushing usage events
[0m18:00:42.412746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58f3707580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58f1cb0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58f1cb08b0>]}


============================== 18:00:42.420159 | a045a9e4-39e1-4450-823b-ef4a041982a4 ==============================
[0m18:00:42.420159 [info ] [MainThread]: Running with dbt=1.5.2
[0m18:00:42.421047 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/opt/airflow/dbt', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m18:00:43.357704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a045a9e4-39e1-4450-823b-ef4a041982a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58f1cb0130>]}
[0m18:00:43.388221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a045a9e4-39e1-4450-823b-ef4a041982a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58c69d9b50>]}
[0m18:00:43.389301 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m18:00:43.435618 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m18:00:44.678905 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:00:44.679607 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:00:44.690975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a045a9e4-39e1-4450-823b-ef4a041982a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58c6991340>]}
[0m18:00:44.725897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a045a9e4-39e1-4450-823b-ef4a041982a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58c68be370>]}
[0m18:00:44.726951 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m18:00:44.727896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a045a9e4-39e1-4450-823b-ef4a041982a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58c68be3d0>]}
[0m18:00:44.732337 [info ] [MainThread]: 
[0m18:00:44.733847 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:00:44.736413 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m18:00:44.742742 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m18:00:44.743295 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m18:00:44.743838 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:00:45.699319 [debug] [ThreadPool]: SQL status: OK in 0.9599999785423279 seconds
[0m18:00:45.702455 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m18:00:45.821106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a045a9e4-39e1-4450-823b-ef4a041982a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58c69497f0>]}
[0m18:00:45.822210 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:45.822881 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:00:45.823897 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:00:45.824780 [info ] [MainThread]: 
[0m18:00:45.840743 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m18:00:45.841590 [info ] [Thread-2  ]: 1 of 35 START test accepted_rating_range_genre_popularity_avg_rating__5__0 ..... [RUN]
[0m18:00:45.842752 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413)
[0m18:00:45.843382 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m18:00:45.857224 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m18:00:45.866826 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (compile): 18:00:45.843851 => 18:00:45.866390
[0m18:00:45.867591 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m18:00:45.893950 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m18:00:45.903851 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:45.904406 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m18:00:45.904940 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m18:00:45.905638 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:46.747672 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m18:00:46.755757 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (execute): 18:00:45.868120 => 18:00:46.755359
[0m18:00:46.756689 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: ROLLBACK
[0m18:00:46.757462 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:46.758044 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: Close
[0m18:00:46.867129 [info ] [Thread-2  ]: 1 of 35 PASS accepted_rating_range_genre_popularity_avg_rating__5__0 ........... [[32mPASS[0m in 1.02s]
[0m18:00:46.868663 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m18:00:46.869496 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m18:00:46.870215 [info ] [Thread-2  ]: 2 of 35 START test accepted_rating_range_movie_enriched_avg_rating__5__0 ....... [RUN]
[0m18:00:46.871485 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413, now test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8)
[0m18:00:46.872168 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m18:00:46.878432 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m18:00:46.888324 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (compile): 18:00:46.872671 => 18:00:46.887934
[0m18:00:46.889139 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m18:00:46.893123 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m18:00:46.902173 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:46.902744 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m18:00:46.903383 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m18:00:46.904080 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:47.678710 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m18:00:47.682805 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (execute): 18:00:46.889646 => 18:00:47.682480
[0m18:00:47.683597 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: ROLLBACK
[0m18:00:47.684454 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:47.685269 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: Close
[0m18:00:47.805781 [info ] [Thread-2  ]: 2 of 35 PASS accepted_rating_range_movie_enriched_avg_rating__5__0 ............. [[32mPASS[0m in 0.93s]
[0m18:00:47.807135 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m18:00:47.808060 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m18:00:47.808879 [info ] [Thread-2  ]: 3 of 35 START test accepted_rating_range_movie_performance_weighted_avg_rating__5__0  [RUN]
[0m18:00:47.810095 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8, now test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083)
[0m18:00:47.810824 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m18:00:47.820395 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m18:00:47.830027 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (compile): 18:00:47.811458 => 18:00:47.829582
[0m18:00:47.830786 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m18:00:47.835242 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m18:00:47.844291 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:47.844935 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m18:00:47.845612 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating < 0
   or weighted_avg_rating > 5


      
    ) dbt_internal_test
[0m18:00:47.846344 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:48.546437 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m18:00:48.550948 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (execute): 18:00:47.831273 => 18:00:48.550567
[0m18:00:48.551896 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: ROLLBACK
[0m18:00:48.552632 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:48.553351 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: Close
[0m18:00:48.698068 [info ] [Thread-2  ]: 3 of 35 PASS accepted_rating_range_movie_performance_weighted_avg_rating__5__0 . [[32mPASS[0m in 0.89s]
[0m18:00:48.699449 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m18:00:48.700411 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m18:00:48.701297 [info ] [Thread-2  ]: 4 of 35 START test accepted_rating_range_user_activity_avg_user_rating__5__0 ... [RUN]
[0m18:00:48.702690 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083, now test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a)
[0m18:00:48.703604 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m18:00:48.715929 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m18:00:48.726299 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (compile): 18:00:48.704297 => 18:00:48.725875
[0m18:00:48.727022 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m18:00:48.731113 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m18:00:48.739660 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:48.740178 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m18:00:48.740879 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating < 0
   or avg_user_rating > 5


      
    ) dbt_internal_test
[0m18:00:48.741513 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:49.506084 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m18:00:49.509981 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (execute): 18:00:48.727554 => 18:00:49.509657
[0m18:00:49.510719 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: ROLLBACK
[0m18:00:49.511472 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:49.512164 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: Close
[0m18:00:49.616756 [info ] [Thread-2  ]: 4 of 35 PASS accepted_rating_range_user_activity_avg_user_rating__5__0 ......... [[32mPASS[0m in 0.91s]
[0m18:00:49.618143 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m18:00:49.618842 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m18:00:49.619532 [info ] [Thread-2  ]: 5 of 35 START test not_null_genre_popularity_avg_rating ........................ [RUN]
[0m18:00:49.620686 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a, now test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9)
[0m18:00:49.621267 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m18:00:49.630505 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m18:00:49.639761 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (compile): 18:00:49.621736 => 18:00:49.639345
[0m18:00:49.640611 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m18:00:49.644408 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m18:00:49.653556 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:49.654225 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m18:00:49.654876 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating is null



      
    ) dbt_internal_test
[0m18:00:49.655620 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:50.398942 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m18:00:50.403000 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (execute): 18:00:49.641063 => 18:00:50.402681
[0m18:00:50.403743 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: ROLLBACK
[0m18:00:50.404472 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:50.405103 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: Close
[0m18:00:50.510370 [info ] [Thread-2  ]: 5 of 35 PASS not_null_genre_popularity_avg_rating .............................. [[32mPASS[0m in 0.89s]
[0m18:00:50.511744 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m18:00:50.512556 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m18:00:50.513337 [info ] [Thread-2  ]: 6 of 35 START test not_null_genre_popularity_first_rating_date ................. [RUN]
[0m18:00:50.514634 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9, now test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32)
[0m18:00:50.515295 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m18:00:50.521595 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m18:00:50.531322 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (compile): 18:00:50.515767 => 18:00:50.530820
[0m18:00:50.532071 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m18:00:50.536073 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m18:00:50.545633 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:50.546309 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m18:00:50.546895 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where first_rating_date is null



      
    ) dbt_internal_test
[0m18:00:50.547566 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:51.233117 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m18:00:51.237159 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (execute): 18:00:50.532559 => 18:00:51.236833
[0m18:00:51.238016 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: ROLLBACK
[0m18:00:51.238693 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:51.239262 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: Close
[0m18:00:51.352736 [info ] [Thread-2  ]: 6 of 35 PASS not_null_genre_popularity_first_rating_date ....................... [[32mPASS[0m in 0.84s]
[0m18:00:51.354130 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m18:00:51.354948 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m18:00:51.355619 [info ] [Thread-2  ]: 7 of 35 START test not_null_genre_popularity_genres ............................ [RUN]
[0m18:00:51.356875 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32, now test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa)
[0m18:00:51.358399 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m18:00:51.374294 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m18:00:51.384001 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (compile): 18:00:51.359010 => 18:00:51.383573
[0m18:00:51.384777 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m18:00:51.389294 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m18:00:51.398966 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:51.399891 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m18:00:51.400562 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`genre_popularity`
where genres is null



      
    ) dbt_internal_test
[0m18:00:51.401298 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:52.124629 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m18:00:52.128768 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (execute): 18:00:51.385263 => 18:00:52.128431
[0m18:00:52.129536 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: ROLLBACK
[0m18:00:52.130143 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:52.130744 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: Close
[0m18:00:52.277529 [info ] [Thread-2  ]: 7 of 35 PASS not_null_genre_popularity_genres .................................. [[32mPASS[0m in 0.92s]
[0m18:00:52.278897 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m18:00:52.279651 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m18:00:52.280316 [info ] [Thread-2  ]: 8 of 35 START test not_null_genre_popularity_last_rating_date .................. [RUN]
[0m18:00:52.281425 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa, now test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432)
[0m18:00:52.282015 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m18:00:52.290407 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m18:00:52.300003 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (compile): 18:00:52.282467 => 18:00:52.299574
[0m18:00:52.300783 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m18:00:52.304808 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m18:00:52.314086 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:52.314852 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m18:00:52.315453 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where last_rating_date is null



      
    ) dbt_internal_test
[0m18:00:52.316123 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:53.023264 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m18:00:53.029104 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (execute): 18:00:52.301229 => 18:00:53.028676
[0m18:00:53.030222 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: ROLLBACK
[0m18:00:53.031178 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:53.032402 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: Close
[0m18:00:53.142850 [info ] [Thread-2  ]: 8 of 35 PASS not_null_genre_popularity_last_rating_date ........................ [[32mPASS[0m in 0.86s]
[0m18:00:53.144627 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m18:00:53.145450 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m18:00:53.146151 [info ] [Thread-2  ]: 9 of 35 START test not_null_genre_popularity_popularity_rank ................... [RUN]
[0m18:00:53.147356 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432, now test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc)
[0m18:00:53.148471 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m18:00:53.157709 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m18:00:53.168593 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (compile): 18:00:53.149069 => 18:00:53.167853
[0m18:00:53.169564 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m18:00:53.174801 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m18:00:53.185800 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:53.186561 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m18:00:53.187260 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select popularity_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where popularity_rank is null



      
    ) dbt_internal_test
[0m18:00:53.188062 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:53.897235 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m18:00:53.903080 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (execute): 18:00:53.170205 => 18:00:53.902601
[0m18:00:53.904041 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: ROLLBACK
[0m18:00:53.904836 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:53.905547 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: Close
[0m18:00:54.040967 [info ] [Thread-2  ]: 9 of 35 PASS not_null_genre_popularity_popularity_rank ......................... [[32mPASS[0m in 0.89s]
[0m18:00:54.042709 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m18:00:54.043953 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m18:00:54.045535 [info ] [Thread-2  ]: 10 of 35 START test not_null_genre_popularity_quality_rank ..................... [RUN]
[0m18:00:54.047460 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc, now test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a)
[0m18:00:54.048462 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m18:00:54.068716 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m18:00:54.083606 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (compile): 18:00:54.049182 => 18:00:54.082960
[0m18:00:54.084760 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m18:00:54.090481 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m18:00:54.102140 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:54.102996 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m18:00:54.103698 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select quality_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where quality_rank is null



      
    ) dbt_internal_test
[0m18:00:54.104399 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:54.978157 [debug] [Thread-2  ]: SQL status: OK in 0.8700000047683716 seconds
[0m18:00:54.985054 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (execute): 18:00:54.085500 => 18:00:54.984620
[0m18:00:54.986622 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: ROLLBACK
[0m18:00:54.987616 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:54.988535 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: Close
[0m18:00:55.104715 [info ] [Thread-2  ]: 10 of 35 PASS not_null_genre_popularity_quality_rank ........................... [[32mPASS[0m in 1.06s]
[0m18:00:55.107336 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m18:00:55.108965 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m18:00:55.110958 [info ] [Thread-2  ]: 11 of 35 START test not_null_genre_popularity_rating_count ..................... [RUN]
[0m18:00:55.114933 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a, now test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856)
[0m18:00:55.117204 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m18:00:55.129141 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m18:00:55.143348 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (compile): 18:00:55.118337 => 18:00:55.142586
[0m18:00:55.144989 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m18:00:55.152314 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m18:00:55.164776 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:55.165805 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m18:00:55.166913 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`genre_popularity`
where rating_count is null



      
    ) dbt_internal_test
[0m18:00:55.168016 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:56.050159 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m18:00:56.055226 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (execute): 18:00:55.146047 => 18:00:56.054802
[0m18:00:56.056206 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: ROLLBACK
[0m18:00:56.057110 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:56.057947 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: Close
[0m18:00:56.240867 [info ] [Thread-2  ]: 11 of 35 PASS not_null_genre_popularity_rating_count ........................... [[32mPASS[0m in 1.13s]
[0m18:00:56.243251 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m18:00:56.244365 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m18:00:56.245248 [info ] [Thread-2  ]: 12 of 35 START test not_null_movie_enriched_avg_rating ......................... [RUN]
[0m18:00:56.246584 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856, now test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd)
[0m18:00:56.247306 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m18:00:56.258731 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m18:00:56.271289 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (compile): 18:00:56.247890 => 18:00:56.270582
[0m18:00:56.272651 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m18:00:56.277851 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m18:00:56.289378 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:56.290284 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m18:00:56.291108 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating is null



      
    ) dbt_internal_test
[0m18:00:56.292029 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:56.989238 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m18:00:56.995869 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (execute): 18:00:56.273258 => 18:00:56.995435
[0m18:00:56.996889 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: ROLLBACK
[0m18:00:56.997703 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:56.998701 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: Close
[0m18:00:57.105201 [info ] [Thread-2  ]: 12 of 35 PASS not_null_movie_enriched_avg_rating ............................... [[32mPASS[0m in 0.86s]
[0m18:00:57.107311 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m18:00:57.108848 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m18:00:57.110386 [info ] [Thread-2  ]: 13 of 35 START test not_null_movie_enriched_genres ............................. [RUN]
[0m18:00:57.112613 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd, now test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27)
[0m18:00:57.113563 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m18:00:57.122405 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m18:00:57.134671 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (compile): 18:00:57.114237 => 18:00:57.134117
[0m18:00:57.135565 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m18:00:57.153687 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m18:00:57.164144 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:57.164991 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m18:00:57.165956 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`movie_enriched`
where genres is null



      
    ) dbt_internal_test
[0m18:00:57.166961 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:57.935953 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m18:00:57.940469 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (execute): 18:00:57.136184 => 18:00:57.940114
[0m18:00:57.941389 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: ROLLBACK
[0m18:00:57.942339 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:57.943001 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: Close
[0m18:00:58.047403 [info ] [Thread-2  ]: 13 of 35 PASS not_null_movie_enriched_genres ................................... [[32mPASS[0m in 0.94s]
[0m18:00:58.048774 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m18:00:58.049669 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m18:00:58.050410 [info ] [Thread-2  ]: 14 of 35 START test not_null_movie_enriched_movie_id ........................... [RUN]
[0m18:00:58.051656 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27, now test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86)
[0m18:00:58.052323 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m18:00:58.058635 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m18:00:58.068077 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (compile): 18:00:58.052805 => 18:00:58.067738
[0m18:00:58.068714 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m18:00:58.072555 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m18:00:58.081848 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:58.082566 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m18:00:58.083130 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is null



      
    ) dbt_internal_test
[0m18:00:58.083707 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:58.829875 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m18:00:58.834279 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (execute): 18:00:58.069166 => 18:00:58.833899
[0m18:00:58.835167 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: ROLLBACK
[0m18:00:58.835924 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:58.836508 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: Close
[0m18:00:58.953615 [info ] [Thread-2  ]: 14 of 35 PASS not_null_movie_enriched_movie_id ................................. [[32mPASS[0m in 0.90s]
[0m18:00:58.955226 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m18:00:58.956180 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m18:00:58.957151 [info ] [Thread-2  ]: 15 of 35 START test not_null_movie_enriched_title .............................. [RUN]
[0m18:00:58.959010 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86, now test.my_dbt_project.not_null_movie_enriched_title.d20f00c292)
[0m18:00:58.960130 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m18:00:58.969641 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m18:00:58.981243 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (compile): 18:00:58.960833 => 18:00:58.980755
[0m18:00:58.982039 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m18:00:58.986220 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m18:00:58.996308 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:58.997058 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m18:00:58.997685 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`movie_enriched`
where title is null



      
    ) dbt_internal_test
[0m18:00:58.998516 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:00:59.730253 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m18:00:59.734596 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (execute): 18:00:58.982515 => 18:00:59.734223
[0m18:00:59.735494 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: ROLLBACK
[0m18:00:59.736279 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:00:59.736913 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: Close
[0m18:00:59.852486 [info ] [Thread-2  ]: 15 of 35 PASS not_null_movie_enriched_title .................................... [[32mPASS[0m in 0.89s]
[0m18:00:59.854066 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m18:00:59.854866 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m18:00:59.855647 [info ] [Thread-2  ]: 16 of 35 START test not_null_movie_enriched_total_ratings ...................... [RUN]
[0m18:00:59.856993 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_title.d20f00c292, now test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04)
[0m18:00:59.857701 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m18:00:59.864060 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m18:00:59.874570 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (compile): 18:00:59.858298 => 18:00:59.874076
[0m18:00:59.875343 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m18:00:59.891412 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m18:00:59.901656 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:00:59.902608 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m18:00:59.903350 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select total_ratings
from `workspace`.`movielens_volume`.`movie_enriched`
where total_ratings is null



      
    ) dbt_internal_test
[0m18:00:59.904332 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:00.640282 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m18:01:00.644802 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (execute): 18:00:59.875848 => 18:01:00.644460
[0m18:01:00.645717 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: ROLLBACK
[0m18:01:00.646588 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:00.647853 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: Close
[0m18:01:00.762298 [info ] [Thread-2  ]: 16 of 35 PASS not_null_movie_enriched_total_ratings ............................ [[32mPASS[0m in 0.91s]
[0m18:01:00.763618 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m18:01:00.764346 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m18:01:00.765109 [info ] [Thread-2  ]: 17 of 35 START test not_null_movie_performance_genre_rank ...................... [RUN]
[0m18:01:00.766282 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04, now test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e)
[0m18:01:00.767094 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m18:01:00.776845 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m18:01:00.788109 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (compile): 18:01:00.767877 => 18:01:00.787475
[0m18:01:00.789016 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m18:01:00.793143 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m18:01:00.803864 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:00.804598 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m18:01:00.805156 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genre_rank
from `workspace`.`movielens_volume`.`movie_performance`
where genre_rank is null



      
    ) dbt_internal_test
[0m18:01:00.805759 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:01.512168 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m18:01:01.516418 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (execute): 18:01:00.789509 => 18:01:01.516061
[0m18:01:01.517383 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: ROLLBACK
[0m18:01:01.518221 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:01.518838 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: Close
[0m18:01:01.638766 [info ] [Thread-2  ]: 17 of 35 PASS not_null_movie_performance_genre_rank ............................ [[32mPASS[0m in 0.87s]
[0m18:01:01.640512 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m18:01:01.641775 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m18:01:01.642700 [info ] [Thread-2  ]: 18 of 35 START test not_null_movie_performance_movie_id ........................ [RUN]
[0m18:01:01.643990 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e, now test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52)
[0m18:01:01.644635 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m18:01:01.654316 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m18:01:01.665238 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (compile): 18:01:01.645113 => 18:01:01.664793
[0m18:01:01.666078 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m18:01:01.671003 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m18:01:01.682252 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:01.683198 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m18:01:01.683882 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is null



      
    ) dbt_internal_test
[0m18:01:01.684553 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:02.480226 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m18:01:02.484468 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (execute): 18:01:01.666564 => 18:01:02.484076
[0m18:01:02.485259 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: ROLLBACK
[0m18:01:02.485970 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:02.486657 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: Close
[0m18:01:02.600030 [info ] [Thread-2  ]: 18 of 35 PASS not_null_movie_performance_movie_id .............................. [[32mPASS[0m in 0.96s]
[0m18:01:02.601441 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m18:01:02.602220 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m18:01:02.602866 [info ] [Thread-2  ]: 19 of 35 START test not_null_movie_performance_polarization_score .............. [RUN]
[0m18:01:02.604062 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52, now test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6)
[0m18:01:02.604900 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m18:01:02.614397 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m18:01:02.624395 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (compile): 18:01:02.605538 => 18:01:02.623964
[0m18:01:02.625093 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m18:01:02.631620 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m18:01:02.642419 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:02.643163 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m18:01:02.643728 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select polarization_score
from `workspace`.`movielens_volume`.`movie_performance`
where polarization_score is null



      
    ) dbt_internal_test
[0m18:01:02.644426 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:03.399175 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m18:01:03.403644 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (execute): 18:01:02.625572 => 18:01:03.403274
[0m18:01:03.404595 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: ROLLBACK
[0m18:01:03.405333 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:03.405943 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: Close
[0m18:01:03.524230 [info ] [Thread-2  ]: 19 of 35 PASS not_null_movie_performance_polarization_score .................... [[32mPASS[0m in 0.92s]
[0m18:01:03.525723 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m18:01:03.526557 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m18:01:03.527379 [info ] [Thread-2  ]: 20 of 35 START test not_null_movie_performance_rating_count .................... [RUN]
[0m18:01:03.528893 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6, now test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac)
[0m18:01:03.529868 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m18:01:03.539063 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m18:01:03.549414 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (compile): 18:01:03.530470 => 18:01:03.548933
[0m18:01:03.550412 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m18:01:03.554711 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m18:01:03.565135 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:03.565837 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m18:01:03.566446 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`movie_performance`
where rating_count is null



      
    ) dbt_internal_test
[0m18:01:03.567112 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:04.332320 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m18:01:04.337235 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (execute): 18:01:03.551129 => 18:01:04.336774
[0m18:01:04.338194 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: ROLLBACK
[0m18:01:04.338959 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:04.339571 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: Close
[0m18:01:04.455322 [info ] [Thread-2  ]: 20 of 35 PASS not_null_movie_performance_rating_count .......................... [[32mPASS[0m in 0.93s]
[0m18:01:04.456824 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m18:01:04.457689 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m18:01:04.458459 [info ] [Thread-2  ]: 21 of 35 START test not_null_movie_performance_top_rating_percentage ........... [RUN]
[0m18:01:04.459696 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac, now test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb)
[0m18:01:04.460481 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m18:01:04.468276 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m18:01:04.479041 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (compile): 18:01:04.461094 => 18:01:04.478556
[0m18:01:04.479838 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m18:01:04.484490 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m18:01:04.495058 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:04.495902 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m18:01:04.496652 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_rating_percentage
from `workspace`.`movielens_volume`.`movie_performance`
where top_rating_percentage is null



      
    ) dbt_internal_test
[0m18:01:04.497499 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:05.233881 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m18:01:05.238214 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (execute): 18:01:04.480402 => 18:01:05.237707
[0m18:01:05.239319 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: ROLLBACK
[0m18:01:05.240251 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:05.241071 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: Close
[0m18:01:05.355808 [info ] [Thread-2  ]: 21 of 35 PASS not_null_movie_performance_top_rating_percentage ................. [[32mPASS[0m in 0.90s]
[0m18:01:05.357355 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m18:01:05.358580 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m18:01:05.359379 [info ] [Thread-2  ]: 22 of 35 START test not_null_movie_performance_weighted_avg_rating ............. [RUN]
[0m18:01:05.360756 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb, now test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5)
[0m18:01:05.361502 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m18:01:05.369253 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m18:01:05.380926 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (compile): 18:01:05.361996 => 18:01:05.380421
[0m18:01:05.381958 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m18:01:05.394021 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m18:01:05.405057 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:05.405803 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m18:01:05.406516 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select weighted_avg_rating
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating is null



      
    ) dbt_internal_test
[0m18:01:05.407591 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:06.463179 [debug] [Thread-2  ]: SQL status: OK in 1.059999942779541 seconds
[0m18:01:06.467605 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (execute): 18:01:05.382516 => 18:01:06.467249
[0m18:01:06.468608 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: ROLLBACK
[0m18:01:06.469480 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:06.470105 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: Close
[0m18:01:06.585213 [info ] [Thread-2  ]: 22 of 35 PASS not_null_movie_performance_weighted_avg_rating ................... [[32mPASS[0m in 1.22s]
[0m18:01:06.586637 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m18:01:06.587407 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m18:01:06.588278 [info ] [Thread-2  ]: 23 of 35 START test not_null_user_activity_avg_user_rating ..................... [RUN]
[0m18:01:06.589855 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5, now test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b)
[0m18:01:06.590593 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m18:01:06.597538 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m18:01:06.608351 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (compile): 18:01:06.591071 => 18:01:06.607807
[0m18:01:06.609124 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m18:01:06.613463 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m18:01:06.623776 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:06.624531 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m18:01:06.625173 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_user_rating
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating is null



      
    ) dbt_internal_test
[0m18:01:06.625925 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:07.351703 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m18:01:07.355975 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (execute): 18:01:06.609610 => 18:01:07.355633
[0m18:01:07.356840 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: ROLLBACK
[0m18:01:07.357700 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:07.358508 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: Close
[0m18:01:07.471492 [info ] [Thread-2  ]: 23 of 35 PASS not_null_user_activity_avg_user_rating ........................... [[32mPASS[0m in 0.88s]
[0m18:01:07.472925 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m18:01:07.473733 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m18:01:07.474505 [info ] [Thread-2  ]: 24 of 35 START test not_null_user_activity_first_activity_date ................. [RUN]
[0m18:01:07.475669 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b, now test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac)
[0m18:01:07.476343 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m18:01:07.482658 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m18:01:07.491903 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (compile): 18:01:07.476888 => 18:01:07.491597
[0m18:01:07.492587 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m18:01:07.496478 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m18:01:07.505385 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:07.505972 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m18:01:07.506612 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where first_activity_date is null



      
    ) dbt_internal_test
[0m18:01:07.507231 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:08.210805 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m18:01:08.215039 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (execute): 18:01:07.493125 => 18:01:08.214711
[0m18:01:08.215837 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: ROLLBACK
[0m18:01:08.216722 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:08.217358 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: Close
[0m18:01:08.328962 [info ] [Thread-2  ]: 24 of 35 PASS not_null_user_activity_first_activity_date ....................... [[32mPASS[0m in 0.85s]
[0m18:01:08.330416 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m18:01:08.331345 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m18:01:08.332253 [info ] [Thread-2  ]: 25 of 35 START test not_null_user_activity_influence_score ..................... [RUN]
[0m18:01:08.333730 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac, now test.my_dbt_project.not_null_user_activity_influence_score.758852d325)
[0m18:01:08.334488 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m18:01:08.341101 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m18:01:08.350775 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (compile): 18:01:08.335105 => 18:01:08.350368
[0m18:01:08.351762 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m18:01:08.366203 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m18:01:08.376326 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:08.377092 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m18:01:08.377701 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select influence_score
from `workspace`.`movielens_volume`.`user_activity`
where influence_score is null



      
    ) dbt_internal_test
[0m18:01:08.378462 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:09.078280 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m18:01:09.082690 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (execute): 18:01:08.352257 => 18:01:09.082356
[0m18:01:09.083783 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: ROLLBACK
[0m18:01:09.084764 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:09.085522 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: Close
[0m18:01:09.209678 [info ] [Thread-2  ]: 25 of 35 PASS not_null_user_activity_influence_score ........................... [[32mPASS[0m in 0.88s]
[0m18:01:09.211927 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m18:01:09.213140 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m18:01:09.214141 [info ] [Thread-2  ]: 26 of 35 START test not_null_user_activity_last_activity_date .................. [RUN]
[0m18:01:09.215990 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_influence_score.758852d325, now test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad)
[0m18:01:09.217545 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m18:01:09.230095 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m18:01:09.242208 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (compile): 18:01:09.218741 => 18:01:09.241733
[0m18:01:09.243027 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m18:01:09.247529 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m18:01:09.258305 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:09.259289 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m18:01:09.259981 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where last_activity_date is null



      
    ) dbt_internal_test
[0m18:01:09.260727 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:10.009223 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m18:01:10.013512 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (execute): 18:01:09.243566 => 18:01:10.013140
[0m18:01:10.014416 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: ROLLBACK
[0m18:01:10.015092 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:10.015746 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: Close
[0m18:01:10.130520 [info ] [Thread-2  ]: 26 of 35 PASS not_null_user_activity_last_activity_date ........................ [[32mPASS[0m in 0.92s]
[0m18:01:10.132008 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m18:01:10.132847 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m18:01:10.133574 [info ] [Thread-2  ]: 27 of 35 START test not_null_user_activity_top_genre ........................... [RUN]
[0m18:01:10.134803 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad, now test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313)
[0m18:01:10.135513 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m18:01:10.146295 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m18:01:10.157451 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (compile): 18:01:10.135969 => 18:01:10.157002
[0m18:01:10.158273 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m18:01:10.162741 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m18:01:10.172529 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:10.173232 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m18:01:10.173798 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_genre
from `workspace`.`movielens_volume`.`user_activity`
where top_genre is null



      
    ) dbt_internal_test
[0m18:01:10.174450 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:10.876666 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m18:01:10.880914 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (execute): 18:01:10.158934 => 18:01:10.880579
[0m18:01:10.881721 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: ROLLBACK
[0m18:01:10.882440 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:10.883116 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: Close
[0m18:01:10.987171 [info ] [Thread-2  ]: 27 of 35 PASS not_null_user_activity_top_genre ................................. [[32mPASS[0m in 0.85s]
[0m18:01:10.988541 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m18:01:10.989380 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m18:01:10.990066 [info ] [Thread-2  ]: 28 of 35 START test not_null_user_activity_user_id ............................. [RUN]
[0m18:01:10.991145 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313, now test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36)
[0m18:01:10.991740 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m18:01:10.999622 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m18:01:11.010462 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (compile): 18:01:10.992181 => 18:01:11.009981
[0m18:01:11.011238 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m18:01:11.016867 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m18:01:11.026779 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:11.027524 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m18:01:11.028084 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`user_activity`
where user_id is null



      
    ) dbt_internal_test
[0m18:01:11.028709 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:11.737583 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m18:01:11.741878 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (execute): 18:01:11.011722 => 18:01:11.741568
[0m18:01:11.742682 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: ROLLBACK
[0m18:01:11.743617 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:11.744249 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: Close
[0m18:01:11.860508 [info ] [Thread-2  ]: 28 of 35 PASS not_null_user_activity_user_id ................................... [[32mPASS[0m in 0.87s]
[0m18:01:11.861833 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m18:01:11.862627 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m18:01:11.863494 [info ] [Thread-2  ]: 29 of 35 START test not_null_user_activity_user_rating_count ................... [RUN]
[0m18:01:11.864959 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36, now test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e)
[0m18:01:11.865672 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m18:01:11.874717 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m18:01:11.884248 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (compile): 18:01:11.866329 => 18:01:11.883849
[0m18:01:11.884923 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m18:01:11.888735 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m18:01:11.898536 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:11.899205 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m18:01:11.899775 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_rating_count
from `workspace`.`movielens_volume`.`user_activity`
where user_rating_count is null



      
    ) dbt_internal_test
[0m18:01:11.900381 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:13.116578 [debug] [Thread-2  ]: SQL status: OK in 1.2200000286102295 seconds
[0m18:01:13.121844 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (execute): 18:01:11.885414 => 18:01:13.121463
[0m18:01:13.122816 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: ROLLBACK
[0m18:01:13.123511 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:13.124145 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: Close
[0m18:01:13.252613 [info ] [Thread-2  ]: 29 of 35 PASS not_null_user_activity_user_rating_count ......................... [[32mPASS[0m in 1.39s]
[0m18:01:13.255039 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m18:01:13.256329 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:01:13.257621 [info ] [Thread-2  ]: 30 of 35 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:01:13.260145 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m18:01:13.261775 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:01:13.289515 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:01:13.303589 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 18:01:13.262774 => 18:01:13.302914
[0m18:01:13.304937 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:01:13.311589 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:01:13.324184 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:13.325366 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:01:13.326177 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:01:13.327109 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:13.934611 [debug] [Thread-2  ]: SQL status: OK in 0.6100000143051147 seconds
[0m18:01:13.939039 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 18:01:13.305825 => 18:01:13.938686
[0m18:01:13.939936 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m18:01:13.940716 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:13.941496 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m18:01:14.055818 [info ] [Thread-2  ]: 30 of 35 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.80s]
[0m18:01:14.057158 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:01:14.058029 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:01:14.058777 [info ] [Thread-2  ]: 31 of 35 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:01:14.060230 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m18:01:14.061045 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:01:14.069857 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:01:14.079680 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 18:01:14.061547 => 18:01:14.079275
[0m18:01:14.080438 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:01:14.084747 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:01:14.094750 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:14.095560 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:01:14.096226 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:01:14.096883 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:14.680981 [debug] [Thread-2  ]: SQL status: OK in 0.5799999833106995 seconds
[0m18:01:14.685919 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 18:01:14.080967 => 18:01:14.685560
[0m18:01:14.686727 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m18:01:14.687389 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:14.688285 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m18:01:14.812769 [info ] [Thread-2  ]: 31 of 35 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.75s]
[0m18:01:14.814722 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:01:14.815873 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m18:01:14.816742 [info ] [Thread-2  ]: 32 of 35 START test unique_genre_popularity_genres ............................. [RUN]
[0m18:01:14.818139 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075)
[0m18:01:14.818858 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m18:01:14.828302 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m18:01:14.837992 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (compile): 18:01:14.819327 => 18:01:14.837487
[0m18:01:14.838760 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m18:01:14.843338 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m18:01:14.853899 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:14.854582 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m18:01:14.855282 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    genres as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`genre_popularity`
where genres is not null
group by genres
having count(*) > 1



      
    ) dbt_internal_test
[0m18:01:14.856018 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:15.675520 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m18:01:15.680349 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (execute): 18:01:14.839260 => 18:01:15.679960
[0m18:01:15.681296 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: ROLLBACK
[0m18:01:15.682030 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:15.682722 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: Close
[0m18:01:15.796025 [info ] [Thread-2  ]: 32 of 35 PASS unique_genre_popularity_genres ................................... [[32mPASS[0m in 0.98s]
[0m18:01:15.797439 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m18:01:15.798223 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m18:01:15.798957 [info ] [Thread-2  ]: 33 of 35 START test unique_movie_enriched_movie_id ............................. [RUN]
[0m18:01:15.800380 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075, now test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a)
[0m18:01:15.801146 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m18:01:15.808581 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m18:01:15.818507 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (compile): 18:01:15.801743 => 18:01:15.818109
[0m18:01:15.819550 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m18:01:15.823658 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m18:01:15.832797 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:15.833421 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m18:01:15.834080 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:01:15.834792 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:16.622543 [debug] [Thread-2  ]: SQL status: OK in 0.7900000214576721 seconds
[0m18:01:16.627132 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (execute): 18:01:15.820052 => 18:01:16.626793
[0m18:01:16.628071 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: ROLLBACK
[0m18:01:16.628894 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:16.629567 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m18:01:16.745581 [info ] [Thread-2  ]: 33 of 35 PASS unique_movie_enriched_movie_id ................................... [[32mPASS[0m in 0.95s]
[0m18:01:16.746951 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m18:01:16.747700 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m18:01:16.748538 [info ] [Thread-2  ]: 34 of 35 START test unique_movie_performance_movie_id .......................... [RUN]
[0m18:01:16.750043 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a, now test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22)
[0m18:01:16.751058 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m18:01:16.757921 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m18:01:16.769039 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (compile): 18:01:16.751650 => 18:01:16.768531
[0m18:01:16.770065 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m18:01:16.774865 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m18:01:16.786254 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:16.787038 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m18:01:16.787868 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:01:16.788642 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:17.531327 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m18:01:17.545834 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (execute): 18:01:16.770612 => 18:01:17.542950
[0m18:01:17.547012 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: ROLLBACK
[0m18:01:17.547968 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:17.548694 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: Close
[0m18:01:17.659982 [info ] [Thread-2  ]: 34 of 35 PASS unique_movie_performance_movie_id ................................ [[32mPASS[0m in 0.91s]
[0m18:01:17.661404 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m18:01:17.662116 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m18:01:17.662869 [info ] [Thread-2  ]: 35 of 35 START test unique_user_activity_user_id ............................... [RUN]
[0m18:01:17.664401 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22, now test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda)
[0m18:01:17.665250 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m18:01:17.672289 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m18:01:17.682923 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (compile): 18:01:17.665786 => 18:01:17.682500
[0m18:01:17.683714 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m18:01:17.688601 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m18:01:17.699598 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:17.700504 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m18:01:17.701146 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`user_activity`
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:01:17.701833 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:01:18.371552 [debug] [Thread-2  ]: SQL status: OK in 0.6700000166893005 seconds
[0m18:01:18.375957 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (execute): 18:01:17.684523 => 18:01:18.375549
[0m18:01:18.377057 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: ROLLBACK
[0m18:01:18.377923 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:01:18.378752 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: Close
[0m18:01:18.581666 [info ] [Thread-2  ]: 35 of 35 PASS unique_user_activity_user_id ..................................... [[32mPASS[0m in 0.92s]
[0m18:01:18.583197 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m18:01:18.585998 [debug] [MainThread]: On master: ROLLBACK
[0m18:01:18.586789 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:01:18.950482 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:01:18.951348 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:01:18.952001 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:01:18.952667 [debug] [MainThread]: On master: ROLLBACK
[0m18:01:18.953422 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:01:18.954221 [debug] [MainThread]: On master: Close
[0m18:01:19.068875 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:01:19.069591 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda' was properly closed.
[0m18:01:19.072246 [info ] [MainThread]: 
[0m18:01:19.074032 [info ] [MainThread]: Finished running 35 tests in 0 hours 0 minutes and 34.34 seconds (34.34s).
[0m18:01:19.080835 [debug] [MainThread]: Command end result
[0m18:01:19.111212 [info ] [MainThread]: 
[0m18:01:19.112309 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:01:19.113348 [info ] [MainThread]: 
[0m18:01:19.114249 [info ] [MainThread]: Done. PASS=35 WARN=0 ERROR=0 SKIP=0 TOTAL=35
[0m18:01:19.115668 [debug] [MainThread]: Command `dbt test` succeeded at 18:01:19.115484 after 36.96 seconds
[0m18:01:19.116263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58f3707580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58c69d9b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58c44f5040>]}
[0m18:01:19.116868 [debug] [MainThread]: Flushing usage events
[0m18:26:49.945475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ddef835b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ddd4ef1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ddd4ef910>]}


============================== 18:26:49.953712 | dd9201a7-55d6-4c92-a5f4-420ed9c62053 ==============================
[0m18:26:49.953712 [info ] [MainThread]: Running with dbt=1.5.2
[0m18:26:49.954889 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m18:26:51.014857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dd9201a7-55d6-4c92-a5f4-420ed9c62053', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ddd4ef160>]}
[0m18:26:51.046503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dd9201a7-55d6-4c92-a5f4-420ed9c62053', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db224bb80>]}
[0m18:26:51.047778 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m18:26:51.095175 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m18:26:52.385642 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:26:52.386459 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:26:52.398341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dd9201a7-55d6-4c92-a5f4-420ed9c62053', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db21f4400>]}
[0m18:26:52.432828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dd9201a7-55d6-4c92-a5f4-420ed9c62053', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db211d430>]}
[0m18:26:52.433739 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m18:26:52.434673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dd9201a7-55d6-4c92-a5f4-420ed9c62053', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db211d490>]}
[0m18:26:52.437420 [info ] [MainThread]: 
[0m18:26:52.438718 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:26:52.440906 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m18:26:52.441675 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m18:26:52.442390 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m18:26:52.442905 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:27:54.947038 [debug] [ThreadPool]: SQL status: OK in 62.5 seconds
[0m18:27:54.949456 [debug] [ThreadPool]: On list_workspace: Close
[0m18:27:55.068844 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m18:27:55.070139 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m18:27:55.085061 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m18:27:55.085963 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m18:27:55.086657 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m18:27:55.087236 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:27:57.229688 [debug] [ThreadPool]: SQL status: OK in 2.140000104904175 seconds
[0m18:27:57.231409 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m18:27:57.232112 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m18:27:57.232852 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m18:27:57.233430 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m18:27:57.354671 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m18:27:57.363772 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m18:27:57.364511 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m18:27:57.365212 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:27:58.182244 [debug] [ThreadPool]: SQL status: OK in 0.8199999928474426 seconds
[0m18:27:58.186141 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m18:27:58.307122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dd9201a7-55d6-4c92-a5f4-420ed9c62053', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db211d160>]}
[0m18:27:58.308099 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:27:58.308662 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:27:58.309814 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:27:58.310516 [info ] [MainThread]: 
[0m18:27:58.326361 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_links
[0m18:27:58.327426 [info ] [Thread-4  ]: 1 of 4 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m18:27:58.328758 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.silver_links)
[0m18:27:58.329518 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_links
[0m18:27:58.343210 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m18:27:58.352256 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (compile): 18:27:58.330143 => 18:27:58.351804
[0m18:27:58.353115 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_links
[0m18:27:58.373435 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:27:58.374218 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m18:27:58.374986 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

      describe extended `workspace`.`movielens_volume`.`silver_links`
  
[0m18:27:58.375612 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m18:28:01.557771 [debug] [Thread-4  ]: SQL status: OK in 3.180000066757202 seconds
[0m18:28:01.621218 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m18:28:01.629971 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m18:28:01.630760 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
    using delta
      
      
      
      
      
      
      as
      

with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m18:28:11.912981 [debug] [Thread-4  ]: SQL status: OK in 10.279999732971191 seconds
[0m18:28:12.096086 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (execute): 18:27:58.353754 => 18:28:12.095800
[0m18:28:12.096982 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: ROLLBACK
[0m18:28:12.097724 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m18:28:12.098354 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: Close
[0m18:28:12.213003 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dd9201a7-55d6-4c92-a5f4-420ed9c62053', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db0dcbf40>]}
[0m18:28:12.214269 [info ] [Thread-4  ]: 1 of 4 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 13.88s]
[0m18:28:12.215471 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_links
[0m18:28:12.216384 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_movies
[0m18:28:12.217200 [info ] [Thread-4  ]: 2 of 4 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m18:28:12.218454 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_links, now model.my_dbt_project.silver_movies)
[0m18:28:12.219289 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_movies
[0m18:28:12.224378 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m18:28:12.232390 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (compile): 18:28:12.219822 => 18:28:12.232099
[0m18:28:12.233027 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_movies
[0m18:28:12.242326 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:28:12.243043 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m18:28:12.243724 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

      describe extended `workspace`.`movielens_volume`.`silver_movies`
  
[0m18:28:12.244366 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m18:28:13.220381 [debug] [Thread-4  ]: SQL status: OK in 0.9800000190734863 seconds
[0m18:28:13.229335 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m18:28:13.237774 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m18:28:13.238401 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m18:28:16.895032 [debug] [Thread-4  ]: SQL status: OK in 3.6600000858306885 seconds
[0m18:28:16.898769 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (execute): 18:28:12.233521 => 18:28:16.898486
[0m18:28:16.899534 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: ROLLBACK
[0m18:28:16.900319 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m18:28:16.900908 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: Close
[0m18:28:17.025070 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dd9201a7-55d6-4c92-a5f4-420ed9c62053', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db1f9f2e0>]}
[0m18:28:17.026267 [info ] [Thread-4  ]: 2 of 4 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 4.81s]
[0m18:28:17.027568 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_movies
[0m18:28:17.028301 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_ratings
[0m18:28:17.029074 [info ] [Thread-4  ]: 3 of 4 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m18:28:17.030202 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_movies, now model.my_dbt_project.silver_ratings)
[0m18:28:17.030819 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_ratings
[0m18:28:17.036239 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m18:28:17.044851 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (compile): 18:28:17.031273 => 18:28:17.044348
[0m18:28:17.045612 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_ratings
[0m18:28:17.052971 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:28:17.054987 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m18:28:17.056010 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

      describe extended `workspace`.`movielens_volume`.`silver_ratings`
  
[0m18:28:17.056918 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m18:28:18.119895 [debug] [Thread-4  ]: SQL status: OK in 1.059999942779541 seconds
[0m18:28:18.126635 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m18:28:18.135701 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m18:28:18.136460 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m18:28:23.640787 [debug] [Thread-4  ]: SQL status: OK in 5.5 seconds
[0m18:28:23.781193 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (execute): 18:28:17.046097 => 18:28:23.780913
[0m18:28:23.782050 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: ROLLBACK
[0m18:28:23.782869 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m18:28:23.783493 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: Close
[0m18:28:23.892202 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dd9201a7-55d6-4c92-a5f4-420ed9c62053', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db1f7fd90>]}
[0m18:28:23.893457 [info ] [Thread-4  ]: 3 of 4 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 6.86s]
[0m18:28:23.894736 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_ratings
[0m18:28:23.895600 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_tags
[0m18:28:23.896393 [info ] [Thread-4  ]: 4 of 4 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m18:28:23.897630 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_ratings, now model.my_dbt_project.silver_tags)
[0m18:28:23.898284 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_tags
[0m18:28:23.909361 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m18:28:23.918364 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (compile): 18:28:23.898748 => 18:28:23.917963
[0m18:28:23.919080 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_tags
[0m18:28:23.925234 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:28:23.925799 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m18:28:23.926332 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

      describe extended `workspace`.`movielens_volume`.`silver_tags`
  
[0m18:28:23.926915 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m18:28:24.734878 [debug] [Thread-4  ]: SQL status: OK in 0.8100000023841858 seconds
[0m18:28:24.741036 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m18:28:24.749100 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m18:28:24.749892 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m18:28:28.795925 [debug] [Thread-4  ]: SQL status: OK in 4.050000190734863 seconds
[0m18:28:28.799887 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (execute): 18:28:23.919540 => 18:28:28.799610
[0m18:28:28.800580 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: ROLLBACK
[0m18:28:28.801323 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m18:28:28.802067 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: Close
[0m18:28:28.925028 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dd9201a7-55d6-4c92-a5f4-420ed9c62053', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db0dcbf40>]}
[0m18:28:28.926474 [info ] [Thread-4  ]: 4 of 4 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 5.03s]
[0m18:28:28.927704 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_tags
[0m18:28:28.930042 [debug] [MainThread]: On master: ROLLBACK
[0m18:28:28.930688 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:28:29.305222 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:28:29.306028 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:28:29.306612 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:28:29.307256 [debug] [MainThread]: On master: ROLLBACK
[0m18:28:29.307857 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:28:29.308398 [debug] [MainThread]: On master: Close
[0m18:28:29.432552 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:28:29.433380 [debug] [MainThread]: Connection 'model.my_dbt_project.silver_tags' was properly closed.
[0m18:28:29.435911 [info ] [MainThread]: 
[0m18:28:29.437343 [info ] [MainThread]: Finished running 4 table models in 0 hours 1 minutes and 37.00 seconds (97.00s).
[0m18:28:29.439199 [debug] [MainThread]: Command end result
[0m18:28:29.465145 [info ] [MainThread]: 
[0m18:28:29.466109 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:28:29.466793 [info ] [MainThread]: 
[0m18:28:29.467527 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m18:28:29.469128 [debug] [MainThread]: Command `dbt run` succeeded at 18:28:29.468973 after 99.97 seconds
[0m18:28:29.469808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ddef835b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db2261670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0db0ebc610>]}
[0m18:28:29.470413 [debug] [MainThread]: Flushing usage events
[0m18:28:34.924957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f130f283670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f130d82e280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f130d82ea30>]}


============================== 18:28:34.932901 | db113738-f5ed-45a7-9110-54460f38ccdb ==============================
[0m18:28:34.932901 [info ] [MainThread]: Running with dbt=1.5.2
[0m18:28:34.933940 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/opt/airflow/dbt', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m18:28:36.013835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'db113738-f5ed-45a7-9110-54460f38ccdb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f130d82e250>]}
[0m18:28:36.046501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'db113738-f5ed-45a7-9110-54460f38ccdb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e256da30>]}
[0m18:28:36.047561 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m18:28:36.095515 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m18:28:37.380774 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:28:37.381494 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:28:37.393754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'db113738-f5ed-45a7-9110-54460f38ccdb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e25134c0>]}
[0m18:28:37.429707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'db113738-f5ed-45a7-9110-54460f38ccdb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e24434c0>]}
[0m18:28:37.430711 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m18:28:37.431651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'db113738-f5ed-45a7-9110-54460f38ccdb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e2443490>]}
[0m18:28:37.434232 [info ] [MainThread]: 
[0m18:28:37.435821 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:28:37.438037 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m18:28:37.438804 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m18:28:37.439324 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m18:28:37.439895 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:28:38.035685 [debug] [ThreadPool]: SQL status: OK in 0.6000000238418579 seconds
[0m18:28:38.037677 [debug] [ThreadPool]: On list_workspace: Close
[0m18:28:38.146679 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m18:28:38.148258 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m18:28:38.161373 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m18:28:38.162249 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m18:28:38.163018 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m18:28:38.163673 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:28:38.888511 [debug] [ThreadPool]: SQL status: OK in 0.7200000286102295 seconds
[0m18:28:38.890172 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m18:28:38.890890 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m18:28:38.891552 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m18:28:38.892190 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m18:28:39.009212 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m18:28:39.018716 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m18:28:39.019555 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m18:28:39.020211 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:28:39.638507 [debug] [ThreadPool]: SQL status: OK in 0.6200000047683716 seconds
[0m18:28:39.641950 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m18:28:39.776330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'db113738-f5ed-45a7-9110-54460f38ccdb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e244a9d0>]}
[0m18:28:39.777307 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:28:39.777986 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:28:39.779187 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:28:39.780039 [info ] [MainThread]: 
[0m18:28:39.796535 [debug] [Thread-4  ]: Began running node model.my_dbt_project.genre_popularity
[0m18:28:39.797533 [info ] [Thread-4  ]: 1 of 5 START sql incremental model movielens_volume.genre_popularity ........... [RUN]
[0m18:28:39.798757 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.genre_popularity)
[0m18:28:39.799380 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.genre_popularity
[0m18:28:39.818227 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.genre_popularity"
[0m18:28:39.827237 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (compile): 18:28:39.799849 => 18:28:39.826768
[0m18:28:39.828037 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.genre_popularity
[0m18:28:39.875882 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:28:39.876674 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m18:28:39.877210 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m18:28:39.877848 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m18:28:40.594338 [debug] [Thread-4  ]: SQL status: OK in 0.7200000286102295 seconds
[0m18:28:40.621505 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m18:28:40.622480 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

  
    create temporary view `genre_popularity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings with Genres
-- Join silver ratings with silver movies to attach genres
-- Prepare rating timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        r.rating,
        r.rating_date as rating_timestamp,  
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Aggregate Genre Metrics
-- For each genre, calculate total ratings, avg rating,
-- and distribution insights
-- =======================================================
genre_stats as (
    select
        genres,  -- genres already formatted with spacing
        count(*) as rating_count,
        round(avg(rating), 2) as avg_rating,  
        to_date(min(rating_timestamp)) as first_rating_date,  
        to_date(max(rating_timestamp)) as last_rating_date   
    from ratings_base
    group by genres
),

-- =======================================================
-- Step 3: Ranking Genres
-- Rank genres by popularity (# of ratings)
-- and quality (avg rating)
-- =======================================================
ranked_genres as (
    select
        genres,
        rating_count,
        avg_rating,
        first_rating_date,
        last_rating_date,
        rank() over (order by rating_count desc) as popularity_rank,
        rank() over (order by avg_rating desc) as quality_rank
    from genre_stats
)

-- =======================================================
-- Step 4: Final Selection
-- One row per genre with all metrics
-- =======================================================
select
    genres,
    rating_count,
    avg_rating,
    popularity_rank,
    quality_rank,
    first_rating_date,
    last_rating_date
from ranked_genres


  -- Only insert new genres on incremental runs
  where genres not in (select genres from `workspace`.`movielens_volume`.`genre_popularity`)

  
[0m18:28:41.188891 [debug] [Thread-4  ]: SQL status: OK in 0.5699999928474426 seconds
[0m18:28:41.215978 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m18:28:41.216918 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m18:28:41.500154 [debug] [Thread-4  ]: SQL status: OK in 0.2800000011920929 seconds
[0m18:28:41.512597 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.genre_popularity"
[0m18:28:41.520389 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m18:28:41.521087 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`genre_popularity` as DBT_INTERNAL_DEST
      using `genre_popularity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.genres = DBT_INTERNAL_DEST.genres
          

      when matched then update set
         * 

      when not matched then insert *

[0m18:28:51.102255 [debug] [Thread-4  ]: SQL status: OK in 9.579999923706055 seconds
[0m18:28:51.261492 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (execute): 18:28:39.828562 => 18:28:51.261217
[0m18:28:51.262393 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: ROLLBACK
[0m18:28:51.263422 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m18:28:51.264127 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: Close
[0m18:28:51.373572 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db113738-f5ed-45a7-9110-54460f38ccdb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e1103fd0>]}
[0m18:28:51.374829 [info ] [Thread-4  ]: 1 of 5 OK created sql incremental model movielens_volume.genre_popularity ...... [[32mOK[0m in 11.58s]
[0m18:28:51.376131 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.genre_popularity
[0m18:28:51.376979 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_enriched
[0m18:28:51.378139 [info ] [Thread-4  ]: 2 of 5 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m18:28:51.379400 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.genre_popularity, now model.my_dbt_project.movie_enriched)
[0m18:28:51.380094 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_enriched
[0m18:28:51.385887 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m18:28:51.394645 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (compile): 18:28:51.380602 => 18:28:51.394325
[0m18:28:51.395281 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_enriched
[0m18:28:51.410842 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:28:51.411547 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m18:28:51.412393 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

      describe extended `workspace`.`movielens_volume`.`movie_enriched`
  
[0m18:28:51.413065 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m18:28:52.181916 [debug] [Thread-4  ]: SQL status: OK in 0.7699999809265137 seconds
[0m18:28:52.227259 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m18:28:52.235688 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m18:28:52.236445 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m18:28:56.167518 [debug] [Thread-4  ]: SQL status: OK in 3.930000066757202 seconds
[0m18:28:56.175484 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (execute): 18:28:51.395767 => 18:28:56.175208
[0m18:28:56.176233 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: ROLLBACK
[0m18:28:56.177030 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m18:28:56.177743 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: Close
[0m18:28:56.290083 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db113738-f5ed-45a7-9110-54460f38ccdb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e2283b20>]}
[0m18:28:56.291340 [info ] [Thread-4  ]: 2 of 5 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 4.91s]
[0m18:28:56.292567 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_enriched
[0m18:28:56.293353 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_performance
[0m18:28:56.294407 [info ] [Thread-4  ]: 3 of 5 START sql incremental model movielens_volume.movie_performance .......... [RUN]
[0m18:28:56.295682 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_enriched, now model.my_dbt_project.movie_performance)
[0m18:28:56.296369 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_performance
[0m18:28:56.304435 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_performance"
[0m18:28:56.313506 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (compile): 18:28:56.296889 => 18:28:56.313171
[0m18:28:56.314182 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_performance
[0m18:28:56.323778 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:28:56.324425 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m18:28:56.325107 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m18:28:56.325792 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m18:28:57.559181 [debug] [Thread-4  ]: SQL status: OK in 1.2300000190734863 seconds
[0m18:28:57.565359 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m18:28:57.566235 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

  
    create temporary view `movie_performance__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings Join
-- Attach movie metadata (title, genres) to ratings
-- =======================================================
with ratings_base as (
    select
        r.movie_id,
        m.title,
        regexp_replace(m.genres, '\\|', ' | ') as genres,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_date
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Weighted Rating
-- Compute avg rating, rating count, weighted avg
-- =======================================================
weighted_rating as (
    select
        movie_id,
        title,
        genres,
        round(avg(rating), 2) as avg_rating,
        count(*) as rating_count,
        round(sum(rating) / count(*), 2) as weighted_avg_rating
    from ratings_base
    group by movie_id, title, genres
),

-- =======================================================
-- Step 3: Polarization Score
-- Standard deviation of ratings per movie
-- =======================================================
polarization as (
    select
        movie_id,
        round(coalesce(stddev(rating), 0.0), 2) as polarization_score
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 4: Top Rating Percentage
-- Percent of ratings >= 4.5
-- =======================================================
top_rating_pct as (
    select
        movie_id,
        round(100.0 * sum(case when rating >= 4.5 then 1 else 0 end) / count(*), 2) as top_rating_percentage
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 5: Genre Ranking
-- Rank movies within each genre by weighted_avg_rating
-- =======================================================
genre_rank as (
    select
        movie_id,
        genres,
        weighted_avg_rating,
        rank() over (partition by genres order by weighted_avg_rating desc) as genre_rank
    from weighted_rating
),

-- =======================================================
-- Step 6: Tag Count
-- Count distinct tags per movie
-- =======================================================
tag_count as (
    select
        movie_id,
        count(distinct tag) as tag_count
    from `workspace`.`movielens_volume`.`silver_tags`
    group by movie_id
)

-- =======================================================
-- Step 7: Final Selection
-- Combine all metrics into the gold table
-- =======================================================
select
    w.movie_id,
    w.title,
    w.genres,
    w.weighted_avg_rating,
    w.rating_count,
    g.genre_rank,
    p.polarization_score,
    tr.top_rating_percentage,
    coalesce(tc.tag_count, 0) as tag_count
from weighted_rating w
join genre_rank g
    on w.movie_id = g.movie_id
join polarization p
    on w.movie_id = p.movie_id
join top_rating_pct tr
    on w.movie_id = tr.movie_id
left join tag_count tc
    on w.movie_id = tc.movie_id


-- Use NOT EXISTS for safer incremental inserts
where not exists (
    select 1 
    from `workspace`.`movielens_volume`.`movie_performance` t
    where t.movie_id = w.movie_id
)

  
[0m18:28:58.326039 [debug] [Thread-4  ]: SQL status: OK in 0.7599999904632568 seconds
[0m18:28:58.331173 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m18:28:58.332057 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m18:28:58.705250 [debug] [Thread-4  ]: SQL status: OK in 0.3700000047683716 seconds
[0m18:28:58.711140 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_performance"
[0m18:28:58.718663 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m18:28:58.719493 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`movie_performance` as DBT_INTERNAL_DEST
      using `movie_performance__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.movie_id = DBT_INTERNAL_DEST.movie_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m18:29:06.215421 [debug] [Thread-4  ]: SQL status: OK in 7.5 seconds
[0m18:29:06.354410 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (execute): 18:28:56.314684 => 18:29:06.353987
[0m18:29:06.355382 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: ROLLBACK
[0m18:29:06.356364 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:06.357136 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: Close
[0m18:29:06.475284 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db113738-f5ed-45a7-9110-54460f38ccdb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e2283f10>]}
[0m18:29:06.476840 [info ] [Thread-4  ]: 3 of 5 OK created sql incremental model movielens_volume.movie_performance ..... [[32mOK[0m in 10.18s]
[0m18:29:06.478395 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_performance
[0m18:29:06.479346 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_activity
[0m18:29:06.480582 [info ] [Thread-4  ]: 4 of 5 START sql incremental model movielens_volume.user_activity .............. [RUN]
[0m18:29:06.482063 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_performance, now model.my_dbt_project.user_activity)
[0m18:29:06.482851 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_activity
[0m18:29:06.493587 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_activity"
[0m18:29:06.500926 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (compile): 18:29:06.483364 => 18:29:06.500514
[0m18:29:06.501652 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_activity
[0m18:29:06.508960 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:06.509621 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m18:29:06.510240 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m18:29:06.510890 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m18:29:07.391705 [debug] [Thread-4  ]: SQL status: OK in 0.8799999952316284 seconds
[0m18:29:07.398252 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m18:29:07.399128 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

  
    create temporary view `user_activity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Join ratings with movies to attach genres and prepare timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_timestamp,
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: User Activity (General)
-- Calculate total ratings, average rating, first and last activity dates per user
-- =======================================================
user_activity as (
    select
        user_id,
        count(*) as user_rating_count,
        round(avg(rating), 2) as avg_user_rating,
        min(rating_timestamp) as first_activity_date,
        max(rating_timestamp) as last_activity_date
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 3: User Influence Score
-- Compute influence based on total ratings (more ratings = higher influence)
-- =======================================================
user_influence as (
    select
        user_id,
        user_rating_count as influence_score
    from user_activity
),

-- =======================================================
-- Step 4: Active Users by Genre
-- Count ratings per genre and rank genres per user
-- =======================================================
user_genre_activity as (
    select
        user_id,
        regexp_replace(genres, '\\|', ' | ') as genres,
        count(*) as genre_rating_count,
        rank() over (partition by user_id order by count(*) desc) as genre_rank
    from ratings_base
    group by user_id, genres
),

-- =======================================================
-- Step 5: New vs Loyal Users
-- Categorize users as new if first activity < 30 days ago, else loyal
-- =======================================================
user_type as (
    select
        user_id,
        case
            when min(rating_timestamp) >= date_sub(current_date(), 30)
                then 'new_user'
            else 'loyal_user'
        end as user_category
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 6: Top Genre per User
-- Identify the most active genre for each user
-- =======================================================
top_genre as (
    select
        user_id,
        max(genres) as top_genre
    from user_genre_activity
    group by user_id
)

-- =======================================================
-- Final Selection
-- Combine general activity, influence, type, and top genre per user
-- =======================================================
select
    ua.user_id,
    ua.user_rating_count,
    ua.avg_user_rating,
    ua.first_activity_date,
    ua.last_activity_date,
    ui.influence_score,
    ut.user_category,
    tg.top_genre
from user_activity ua
join user_influence ui on ua.user_id = ui.user_id
join user_type ut on ua.user_id = ut.user_id
left join top_genre tg on ua.user_id = tg.user_id


  where ua.user_id not in (select user_id from `workspace`.`movielens_volume`.`user_activity`)

  
[0m18:29:07.980029 [debug] [Thread-4  ]: SQL status: OK in 0.5799999833106995 seconds
[0m18:29:07.991533 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m18:29:07.992318 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m18:29:08.259086 [debug] [Thread-4  ]: SQL status: OK in 0.27000001072883606 seconds
[0m18:29:08.263981 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_activity"
[0m18:29:08.273622 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m18:29:08.274301 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_activity` as DBT_INTERNAL_DEST
      using `user_activity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m18:29:16.647171 [debug] [Thread-4  ]: SQL status: OK in 8.369999885559082 seconds
[0m18:29:16.783734 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (execute): 18:29:06.502084 => 18:29:16.783284
[0m18:29:16.784849 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: ROLLBACK
[0m18:29:16.785843 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:16.786567 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: Close
[0m18:29:16.904972 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db113738-f5ed-45a7-9110-54460f38ccdb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e25bd880>]}
[0m18:29:16.906580 [info ] [Thread-4  ]: 4 of 5 OK created sql incremental model movielens_volume.user_activity ......... [[32mOK[0m in 10.42s]
[0m18:29:16.908140 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_activity
[0m18:29:16.909231 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_preferences
[0m18:29:16.910566 [info ] [Thread-4  ]: 5 of 5 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m18:29:16.912024 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.user_activity, now model.my_dbt_project.user_preferences)
[0m18:29:16.912879 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_preferences
[0m18:29:16.923441 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m18:29:16.931355 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (compile): 18:29:16.913554 => 18:29:16.931074
[0m18:29:16.932015 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_preferences
[0m18:29:16.938982 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:16.939728 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m18:29:16.940242 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m18:29:16.940975 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m18:29:17.782771 [debug] [Thread-4  ]: SQL status: OK in 0.8399999737739563 seconds
[0m18:29:17.789937 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m18:29:17.790940 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create temporary view `user_preferences__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score,  -- influence based on total ratings
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )

  
[0m18:29:18.266720 [debug] [Thread-4  ]: SQL status: OK in 0.4699999988079071 seconds
[0m18:29:18.272090 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m18:29:18.272806 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m18:29:18.681967 [debug] [Thread-4  ]: SQL status: OK in 0.4099999964237213 seconds
[0m18:29:18.686687 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m18:29:18.695500 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m18:29:18.696264 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
      using `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m18:29:26.156454 [debug] [Thread-4  ]: SQL status: OK in 7.460000038146973 seconds
[0m18:29:26.290835 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (execute): 18:29:16.932450 => 18:29:26.290518
[0m18:29:26.291833 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: ROLLBACK
[0m18:29:26.292852 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:26.293699 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: Close
[0m18:29:26.408650 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db113738-f5ed-45a7-9110-54460f38ccdb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e10d8eb0>]}
[0m18:29:26.409842 [info ] [Thread-4  ]: 5 of 5 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 9.50s]
[0m18:29:26.411251 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_preferences
[0m18:29:26.414831 [debug] [MainThread]: On master: ROLLBACK
[0m18:29:26.415503 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:29:26.790007 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:29:26.790833 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:26.791503 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:29:26.792150 [debug] [MainThread]: On master: ROLLBACK
[0m18:29:26.792743 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:29:26.793385 [debug] [MainThread]: On master: Close
[0m18:29:26.904188 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:29:26.905036 [debug] [MainThread]: Connection 'model.my_dbt_project.user_preferences' was properly closed.
[0m18:29:26.907547 [info ] [MainThread]: 
[0m18:29:26.909050 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 49.47 seconds (49.47s).
[0m18:29:26.911158 [debug] [MainThread]: Command end result
[0m18:29:26.942532 [info ] [MainThread]: 
[0m18:29:26.943471 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:29:26.944264 [info ] [MainThread]: 
[0m18:29:26.945163 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m18:29:26.946275 [debug] [MainThread]: Command `dbt run` succeeded at 18:29:26.946138 after 52.32 seconds
[0m18:29:26.946853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f130f283670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e22c7910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12e256da30>]}
[0m18:29:26.947425 [debug] [MainThread]: Flushing usage events
[0m18:29:30.959437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde89147580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde876b01f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde876b0910>]}


============================== 18:29:30.966472 | 59d88252-5322-460e-8b79-d7c9332d5ced ==============================
[0m18:29:30.966472 [info ] [MainThread]: Running with dbt=1.5.2
[0m18:29:30.967732 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/opt/airflow/dbt', 'log_path': '/opt/airflow/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m18:29:31.943933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '59d88252-5322-460e-8b79-d7c9332d5ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde876b0190>]}
[0m18:29:31.974071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '59d88252-5322-460e-8b79-d7c9332d5ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde60414b80>]}
[0m18:29:31.974921 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m18:29:32.020653 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m18:29:33.233076 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:29:33.233806 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:29:33.244915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '59d88252-5322-460e-8b79-d7c9332d5ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde603ca3d0>]}
[0m18:29:33.277599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '59d88252-5322-460e-8b79-d7c9332d5ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde602f5400>]}
[0m18:29:33.278521 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m18:29:33.279308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '59d88252-5322-460e-8b79-d7c9332d5ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde602f5460>]}
[0m18:29:33.282824 [info ] [MainThread]: 
[0m18:29:33.284203 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:29:33.287995 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m18:29:33.294677 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m18:29:33.295296 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m18:29:33.295921 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:29:33.939363 [debug] [ThreadPool]: SQL status: OK in 0.6399999856948853 seconds
[0m18:29:33.943317 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m18:29:34.061464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '59d88252-5322-460e-8b79-d7c9332d5ced', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde602f5070>]}
[0m18:29:34.062502 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:34.063350 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:29:34.064577 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:29:34.065362 [info ] [MainThread]: 
[0m18:29:34.080924 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m18:29:34.081795 [info ] [Thread-2  ]: 1 of 22 START test accepted_rating_range_silver_ratings_rating__5__0 ........... [RUN]
[0m18:29:34.082963 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff)
[0m18:29:34.083566 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m18:29:34.094601 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m18:29:34.104500 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (compile): 18:29:34.084030 => 18:29:34.104163
[0m18:29:34.105216 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m18:29:34.134479 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m18:29:34.143980 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:34.144655 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m18:29:34.145374 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`silver_ratings`
where rating < 0
   or rating > 5


      
    ) dbt_internal_test
[0m18:29:34.146032 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:35.520891 [debug] [Thread-2  ]: SQL status: OK in 1.3700000047683716 seconds
[0m18:29:35.530465 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (execute): 18:29:34.105665 => 18:29:35.530072
[0m18:29:35.531436 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: ROLLBACK
[0m18:29:35.532098 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:35.532687 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: Close
[0m18:29:35.648707 [info ] [Thread-2  ]: 1 of 22 PASS accepted_rating_range_silver_ratings_rating__5__0 ................. [[32mPASS[0m in 1.57s]
[0m18:29:35.649900 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m18:29:35.650723 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m18:29:35.651566 [info ] [Thread-2  ]: 2 of 22 START test not_null_silver_links_imdb_id ............................... [RUN]
[0m18:29:35.652853 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff, now test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51)
[0m18:29:35.653574 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m18:29:35.664178 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m18:29:35.675916 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (compile): 18:29:35.654053 => 18:29:35.675379
[0m18:29:35.676721 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m18:29:35.681167 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m18:29:35.691967 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:35.692694 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m18:29:35.693300 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



      
    ) dbt_internal_test
[0m18:29:35.694054 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:36.590712 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m18:29:36.594873 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (execute): 18:29:35.677261 => 18:29:36.594542
[0m18:29:36.595684 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: ROLLBACK
[0m18:29:36.596446 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:36.597233 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m18:29:36.710867 [info ] [Thread-2  ]: 2 of 22 PASS not_null_silver_links_imdb_id ..................................... [[32mPASS[0m in 1.06s]
[0m18:29:36.712342 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m18:29:36.713282 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m18:29:36.714032 [info ] [Thread-2  ]: 3 of 22 START test not_null_silver_links_movie_id .............................. [RUN]
[0m18:29:36.715371 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51, now test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d)
[0m18:29:36.716168 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m18:29:36.725333 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m18:29:36.736211 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (compile): 18:29:36.716893 => 18:29:36.735762
[0m18:29:36.736982 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m18:29:36.740994 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m18:29:36.749795 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:36.750399 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m18:29:36.751040 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



      
    ) dbt_internal_test
[0m18:29:36.751764 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:37.544888 [debug] [Thread-2  ]: SQL status: OK in 0.7900000214576721 seconds
[0m18:29:37.549062 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (execute): 18:29:36.737504 => 18:29:37.548770
[0m18:29:37.549760 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: ROLLBACK
[0m18:29:37.550420 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:37.551072 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m18:29:37.664782 [info ] [Thread-2  ]: 3 of 22 PASS not_null_silver_links_movie_id .................................... [[32mPASS[0m in 0.95s]
[0m18:29:37.666172 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m18:29:37.667033 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m18:29:37.667885 [info ] [Thread-2  ]: 4 of 22 START test not_null_silver_links_tmdb_id ............................... [RUN]
[0m18:29:37.669254 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d, now test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f)
[0m18:29:37.669969 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m18:29:37.683450 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m18:29:37.693508 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (compile): 18:29:37.670673 => 18:29:37.693119
[0m18:29:37.694214 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m18:29:37.698426 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m18:29:37.708988 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:37.709690 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m18:29:37.710380 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



      
    ) dbt_internal_test
[0m18:29:37.711157 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:38.649616 [debug] [Thread-2  ]: SQL status: OK in 0.9399999976158142 seconds
[0m18:29:38.654070 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (execute): 18:29:37.694676 => 18:29:38.653733
[0m18:29:38.654869 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: ROLLBACK
[0m18:29:38.655613 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:38.656276 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m18:29:38.774532 [info ] [Thread-2  ]: 4 of 22 PASS not_null_silver_links_tmdb_id ..................................... [[32mPASS[0m in 1.11s]
[0m18:29:38.775990 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m18:29:38.776858 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m18:29:38.777624 [info ] [Thread-2  ]: 5 of 22 START test not_null_silver_movies_genres ............................... [RUN]
[0m18:29:38.778928 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f, now test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63)
[0m18:29:38.779781 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m18:29:38.788550 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m18:29:38.797813 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (compile): 18:29:38.780447 => 18:29:38.797461
[0m18:29:38.798502 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m18:29:38.802337 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m18:29:38.810884 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:38.811482 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m18:29:38.812136 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



      
    ) dbt_internal_test
[0m18:29:38.812971 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:39.603950 [debug] [Thread-2  ]: SQL status: OK in 0.7900000214576721 seconds
[0m18:29:39.612946 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (execute): 18:29:38.798984 => 18:29:39.612257
[0m18:29:39.614086 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: ROLLBACK
[0m18:29:39.614979 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:39.615706 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m18:29:39.732264 [info ] [Thread-2  ]: 5 of 22 PASS not_null_silver_movies_genres ..................................... [[32mPASS[0m in 0.95s]
[0m18:29:39.733855 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m18:29:39.735127 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m18:29:39.736039 [info ] [Thread-2  ]: 6 of 22 START test not_null_silver_movies_movie_id ............................. [RUN]
[0m18:29:39.737354 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63, now test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e)
[0m18:29:39.738147 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m18:29:39.746073 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m18:29:39.755852 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (compile): 18:29:39.738761 => 18:29:39.755397
[0m18:29:39.756608 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m18:29:39.761044 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m18:29:39.769692 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:39.770328 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m18:29:39.771074 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



      
    ) dbt_internal_test
[0m18:29:39.771786 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:40.656643 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m18:29:40.661587 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (execute): 18:29:39.757112 => 18:29:40.661185
[0m18:29:40.662459 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: ROLLBACK
[0m18:29:40.663176 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:40.663764 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m18:29:40.776112 [info ] [Thread-2  ]: 6 of 22 PASS not_null_silver_movies_movie_id ................................... [[32mPASS[0m in 1.04s]
[0m18:29:40.777555 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m18:29:40.778304 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m18:29:40.778989 [info ] [Thread-2  ]: 7 of 22 START test not_null_silver_movies_title ................................ [RUN]
[0m18:29:40.780366 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e, now test.my_dbt_project.not_null_silver_movies_title.3a779bd2de)
[0m18:29:40.781079 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m18:29:40.651849 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m18:29:40.661683 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (compile): 18:29:40.781661 => 18:29:40.661182
[0m18:29:40.662467 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m18:29:40.666684 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m18:29:40.676003 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:40.676699 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m18:29:40.677367 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



      
    ) dbt_internal_test
[0m18:29:40.678050 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:41.496973 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m18:29:41.501361 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (execute): 18:29:40.662931 => 18:29:41.500986
[0m18:29:41.502212 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: ROLLBACK
[0m18:29:41.502920 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:41.503583 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m18:29:41.606084 [info ] [Thread-2  ]: 7 of 22 PASS not_null_silver_movies_title ...................................... [[32mPASS[0m in 0.83s]
[0m18:29:41.607542 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m18:29:41.608438 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m18:29:41.609114 [info ] [Thread-2  ]: 8 of 22 START test not_null_silver_ratings_movie_id ............................ [RUN]
[0m18:29:41.610343 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_title.3a779bd2de, now test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36)
[0m18:29:41.611052 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m18:29:41.616943 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m18:29:41.626378 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (compile): 18:29:41.611547 => 18:29:41.625895
[0m18:29:41.627236 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m18:29:41.631213 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m18:29:41.640058 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:41.640744 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m18:29:41.641344 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



      
    ) dbt_internal_test
[0m18:29:41.642092 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:42.375342 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m18:29:42.379196 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (execute): 18:29:41.627758 => 18:29:42.378915
[0m18:29:42.379866 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: ROLLBACK
[0m18:29:42.380428 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:42.380995 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m18:29:42.491801 [info ] [Thread-2  ]: 8 of 22 PASS not_null_silver_ratings_movie_id .................................. [[32mPASS[0m in 0.88s]
[0m18:29:42.493072 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m18:29:42.493878 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m18:29:42.494603 [info ] [Thread-2  ]: 9 of 22 START test not_null_silver_ratings_rating .............................. [RUN]
[0m18:29:42.495891 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36, now test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a)
[0m18:29:42.496606 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m18:29:42.502691 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m18:29:42.512064 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (compile): 18:29:42.497133 => 18:29:42.511667
[0m18:29:42.512859 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m18:29:42.517187 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m18:29:42.526858 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:42.527712 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m18:29:42.528291 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



      
    ) dbt_internal_test
[0m18:29:42.528919 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:43.303558 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m18:29:43.307824 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (execute): 18:29:42.513378 => 18:29:43.307365
[0m18:29:43.308668 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: ROLLBACK
[0m18:29:43.309299 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:43.310268 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m18:29:43.727197 [info ] [Thread-2  ]: 9 of 22 PASS not_null_silver_ratings_rating .................................... [[32mPASS[0m in 1.23s]
[0m18:29:43.728408 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m18:29:43.729211 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m18:29:43.730012 [info ] [Thread-2  ]: 10 of 22 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m18:29:43.731278 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a, now test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f)
[0m18:29:43.731976 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m18:29:43.745585 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m18:29:43.754814 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (compile): 18:29:43.732398 => 18:29:43.754407
[0m18:29:43.755550 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m18:29:43.759450 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m18:29:43.768409 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:43.769163 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m18:29:43.769712 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



      
    ) dbt_internal_test
[0m18:29:43.770302 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:44.658240 [debug] [Thread-2  ]: SQL status: OK in 0.8899999856948853 seconds
[0m18:29:44.662014 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (execute): 18:29:43.755995 => 18:29:44.661670
[0m18:29:44.662690 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: ROLLBACK
[0m18:29:44.663260 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:44.663887 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m18:29:44.769353 [info ] [Thread-2  ]: 10 of 22 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 1.04s]
[0m18:29:44.770742 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m18:29:44.771576 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m18:29:44.772312 [info ] [Thread-2  ]: 11 of 22 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m18:29:44.773476 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f, now test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3)
[0m18:29:44.774105 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m18:29:44.783649 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m18:29:44.793092 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (compile): 18:29:44.774583 => 18:29:44.792624
[0m18:29:44.793958 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m18:29:44.798286 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m18:29:44.807237 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:44.807873 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m18:29:44.808590 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



      
    ) dbt_internal_test
[0m18:29:44.809262 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:45.737829 [debug] [Thread-2  ]: SQL status: OK in 0.9300000071525574 seconds
[0m18:29:45.741763 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (execute): 18:29:44.794473 => 18:29:45.741430
[0m18:29:45.751887 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: ROLLBACK
[0m18:29:45.752800 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:45.753563 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m18:29:45.871809 [info ] [Thread-2  ]: 11 of 22 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 1.10s]
[0m18:29:45.873114 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m18:29:45.873927 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m18:29:45.874594 [info ] [Thread-2  ]: 12 of 22 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m18:29:45.875660 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3, now test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3)
[0m18:29:45.876285 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m18:29:45.884477 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m18:29:45.893966 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (compile): 18:29:45.876790 => 18:29:45.893494
[0m18:29:45.894690 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m18:29:45.898883 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m18:29:45.908345 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:45.909023 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m18:29:45.909724 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



      
    ) dbt_internal_test
[0m18:29:45.910388 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:46.876395 [debug] [Thread-2  ]: SQL status: OK in 0.9700000286102295 seconds
[0m18:29:46.880188 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (execute): 18:29:45.895227 => 18:29:46.879910
[0m18:29:46.880794 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: ROLLBACK
[0m18:29:46.881436 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:46.882123 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m18:29:46.995829 [info ] [Thread-2  ]: 12 of 22 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 1.12s]
[0m18:29:46.997097 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m18:29:46.997830 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m18:29:46.998548 [info ] [Thread-2  ]: 13 of 22 START test not_null_silver_tags_tag ................................... [RUN]
[0m18:29:46.999709 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3, now test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8)
[0m18:29:47.000352 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m18:29:47.009593 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m18:29:47.018184 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (compile): 18:29:47.000842 => 18:29:47.017878
[0m18:29:47.018809 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m18:29:47.022388 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m18:29:47.030674 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:47.031207 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m18:29:47.031790 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



      
    ) dbt_internal_test
[0m18:29:47.032488 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:47.929435 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m18:29:47.933572 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (execute): 18:29:47.019278 => 18:29:47.933257
[0m18:29:47.934364 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: ROLLBACK
[0m18:29:47.935165 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:47.935957 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m18:29:48.048372 [info ] [Thread-2  ]: 13 of 22 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 1.05s]
[0m18:29:48.049788 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m18:29:48.050674 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m18:29:48.051450 [info ] [Thread-2  ]: 14 of 22 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m18:29:48.052686 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8, now test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86)
[0m18:29:48.053345 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m18:29:48.059743 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m18:29:48.069333 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (compile): 18:29:48.053832 => 18:29:48.068887
[0m18:29:48.070072 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m18:29:48.073825 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m18:29:48.083068 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:48.083697 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m18:29:48.084275 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



      
    ) dbt_internal_test
[0m18:29:48.084949 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:48.914554 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m18:29:48.918977 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (execute): 18:29:48.070511 => 18:29:48.918596
[0m18:29:48.919849 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: ROLLBACK
[0m18:29:48.920803 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:48.921611 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m18:29:49.033951 [info ] [Thread-2  ]: 14 of 22 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 0.98s]
[0m18:29:49.035460 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m18:29:49.036405 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m18:29:49.037183 [info ] [Thread-2  ]: 15 of 22 START test not_null_silver_tags_user_id ............................... [RUN]
[0m18:29:49.038517 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86, now test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3)
[0m18:29:49.039168 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m18:29:49.045400 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m18:29:49.055151 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (compile): 18:29:49.039650 => 18:29:49.054755
[0m18:29:49.055838 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m18:29:49.059846 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m18:29:49.068659 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:49.069220 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m18:29:49.069772 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



      
    ) dbt_internal_test
[0m18:29:49.070477 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:49.882368 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m18:29:49.886882 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (execute): 18:29:49.056363 => 18:29:49.886515
[0m18:29:49.887687 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: ROLLBACK
[0m18:29:49.888361 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:49.888999 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m18:29:50.001039 [info ] [Thread-2  ]: 15 of 22 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 0.96s]
[0m18:29:50.002466 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m18:29:50.003372 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:29:50.004261 [info ] [Thread-2  ]: 16 of 22 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:29:50.005620 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m18:29:50.006522 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:29:50.027132 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:29:50.037124 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 18:29:50.007092 => 18:29:50.036677
[0m18:29:50.037895 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:29:50.041777 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:29:50.050702 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:50.051260 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:29:50.051980 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:29:50.052573 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:51.224728 [debug] [Thread-2  ]: SQL status: OK in 1.1699999570846558 seconds
[0m18:29:51.228791 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 18:29:50.038395 => 18:29:51.228489
[0m18:29:51.229438 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m18:29:51.230021 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:51.230578 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m18:29:51.345875 [info ] [Thread-2  ]: 16 of 22 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.34s]
[0m18:29:51.347339 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:29:51.348179 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:29:51.348957 [info ] [Thread-2  ]: 17 of 22 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:29:51.350301 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m18:29:51.351075 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:29:51.358689 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:29:51.368095 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 18:29:51.351591 => 18:29:51.367635
[0m18:29:51.368876 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:29:51.372745 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:29:51.382447 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:51.383128 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:29:51.383791 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:29:51.384523 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:52.427739 [debug] [Thread-2  ]: SQL status: OK in 1.0399999618530273 seconds
[0m18:29:52.432110 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 18:29:51.369408 => 18:29:52.431737
[0m18:29:52.432927 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m18:29:52.433658 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:52.434453 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m18:29:52.542330 [info ] [Thread-2  ]: 17 of 22 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.19s]
[0m18:29:52.543714 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:29:52.544531 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m18:29:52.545310 [info ] [Thread-2  ]: 18 of 22 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:29:52.546620 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32)
[0m18:29:52.547297 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m18:29:52.555633 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m18:29:52.562866 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (compile): 18:29:52.547771 => 18:29:52.562565
[0m18:29:52.563622 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m18:29:52.567708 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m18:29:52.575597 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:52.576123 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m18:29:52.576731 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:29:52.577375 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:53.781041 [debug] [Thread-2  ]: SQL status: OK in 1.2000000476837158 seconds
[0m18:29:53.785534 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (execute): 18:29:52.564252 => 18:29:53.785133
[0m18:29:53.786300 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: ROLLBACK
[0m18:29:53.787271 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:53.788183 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m18:29:54.192645 [info ] [Thread-2  ]: 18 of 22 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.65s]
[0m18:29:54.194107 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m18:29:54.194895 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m18:29:54.195641 [info ] [Thread-2  ]: 19 of 22 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:29:54.196908 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32, now test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd)
[0m18:29:54.197617 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m18:29:54.214132 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m18:29:54.222083 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (compile): 18:29:54.198108 => 18:29:54.221698
[0m18:29:54.222751 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m18:29:54.226565 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m18:29:54.234059 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:54.234577 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m18:29:54.235147 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:29:54.235740 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:55.525953 [debug] [Thread-2  ]: SQL status: OK in 1.2899999618530273 seconds
[0m18:29:55.537740 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (execute): 18:29:54.223176 => 18:29:55.536887
[0m18:29:55.538905 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: ROLLBACK
[0m18:29:55.540416 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:55.541234 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m18:29:55.702290 [info ] [Thread-2  ]: 19 of 22 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.51s]
[0m18:29:55.703884 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m18:29:55.705100 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m18:29:55.706114 [info ] [Thread-2  ]: 20 of 22 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:29:55.707724 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd, now test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719)
[0m18:29:55.708407 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m18:29:55.718271 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m18:29:55.727662 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (compile): 18:29:55.708914 => 18:29:55.727333
[0m18:29:55.728356 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m18:29:55.732472 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m18:29:55.742051 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:55.742634 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m18:29:55.743323 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:29:55.744045 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:56.852620 [debug] [Thread-2  ]: SQL status: OK in 1.1100000143051147 seconds
[0m18:29:56.857166 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (execute): 18:29:55.728822 => 18:29:56.856819
[0m18:29:56.858023 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: ROLLBACK
[0m18:29:56.858835 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:56.859644 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m18:29:57.006645 [info ] [Thread-2  ]: 20 of 22 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 1.30s]
[0m18:29:57.008143 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m18:29:57.009325 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m18:29:57.010285 [info ] [Thread-2  ]: 21 of 22 START test unique_silver_links_movie_id ............................... [RUN]
[0m18:29:57.011612 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719, now test.my_dbt_project.unique_silver_links_movie_id.c422609054)
[0m18:29:57.012338 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m18:29:57.023555 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m18:29:57.033573 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (compile): 18:29:57.012837 => 18:29:57.033159
[0m18:29:57.034352 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m18:29:57.038616 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m18:29:57.047718 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:57.048337 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m18:29:57.048986 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:29:57.049570 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:58.436763 [debug] [Thread-2  ]: SQL status: OK in 1.3899999856948853 seconds
[0m18:29:58.441051 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (execute): 18:29:57.034916 => 18:29:58.440717
[0m18:29:58.441904 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: ROLLBACK
[0m18:29:58.442827 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:58.443581 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m18:29:58.557193 [info ] [Thread-2  ]: 21 of 22 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 1.55s]
[0m18:29:58.558691 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m18:29:58.559647 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m18:29:58.560488 [info ] [Thread-2  ]: 22 of 22 START test unique_silver_movies_movie_id .............................. [RUN]
[0m18:29:58.561857 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_silver_links_movie_id.c422609054, now test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085)
[0m18:29:58.562552 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m18:29:58.568775 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m18:29:58.578652 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (compile): 18:29:58.563106 => 18:29:58.578282
[0m18:29:58.579367 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m18:29:58.592558 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m18:29:58.602127 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:58.602771 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m18:29:58.603452 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:29:58.604108 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:29:59.422103 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m18:29:59.426316 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (execute): 18:29:58.579812 => 18:29:59.425976
[0m18:29:59.427218 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: ROLLBACK
[0m18:29:59.427980 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:29:59.428613 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m18:29:59.541598 [info ] [Thread-2  ]: 22 of 22 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 0.98s]
[0m18:29:59.542879 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m18:29:59.546391 [debug] [MainThread]: On master: ROLLBACK
[0m18:29:59.547103 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:29:59.928502 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:29:59.929443 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:29:59.930062 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:29:59.930644 [debug] [MainThread]: On master: ROLLBACK
[0m18:29:59.931342 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:29:59.932268 [debug] [MainThread]: On master: Close
[0m18:30:00.041072 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:30:00.041877 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085' was properly closed.
[0m18:30:00.044245 [info ] [MainThread]: 
[0m18:30:00.045699 [info ] [MainThread]: Finished running 22 tests in 0 hours 0 minutes and 26.76 seconds (26.76s).
[0m18:30:00.050580 [debug] [MainThread]: Command end result
[0m18:30:00.079827 [info ] [MainThread]: 
[0m18:30:00.080962 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:30:00.081799 [info ] [MainThread]: 
[0m18:30:00.082837 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=0 SKIP=0 TOTAL=22
[0m18:30:00.084157 [debug] [MainThread]: Command `dbt test` succeeded at 18:30:00.083981 after 29.29 seconds
[0m18:30:00.085013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde89147580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde60414b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde604266a0>]}
[0m18:30:00.086043 [debug] [MainThread]: Flushing usage events
[0m18:30:04.341640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63facc35e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63f926e1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63f926e940>]}


============================== 18:30:04.349195 | 71df5de2-4d6b-495a-bd04-97e24327f3af ==============================
[0m18:30:04.349195 [info ] [MainThread]: Running with dbt=1.5.2
[0m18:30:04.350420 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/opt/airflow/dbt', 'log_path': '/opt/airflow/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m18:30:05.334284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '71df5de2-4d6b-495a-bd04-97e24327f3af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63f926e190>]}
[0m18:30:05.365994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '71df5de2-4d6b-495a-bd04-97e24327f3af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63cdfcf9a0>]}
[0m18:30:05.367083 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m18:30:05.414288 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m18:30:06.649827 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:30:06.650539 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:30:06.661534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '71df5de2-4d6b-495a-bd04-97e24327f3af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63cdf7f430>]}
[0m18:30:06.692614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '71df5de2-4d6b-495a-bd04-97e24327f3af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63cdda5460>]}
[0m18:30:06.693558 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m18:30:06.694350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '71df5de2-4d6b-495a-bd04-97e24327f3af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63cdda54c0>]}
[0m18:30:06.698719 [info ] [MainThread]: 
[0m18:30:06.700194 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:30:06.702842 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m18:30:06.709319 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m18:30:06.709877 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m18:30:06.710425 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:30:07.349580 [debug] [ThreadPool]: SQL status: OK in 0.6399999856948853 seconds
[0m18:30:07.353305 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m18:30:07.465131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '71df5de2-4d6b-495a-bd04-97e24327f3af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63cdda5340>]}
[0m18:30:07.466251 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:07.466979 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:30:07.468030 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:30:07.468839 [info ] [MainThread]: 
[0m18:30:07.487274 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m18:30:07.488242 [info ] [Thread-2  ]: 1 of 35 START test accepted_rating_range_genre_popularity_avg_rating__5__0 ..... [RUN]
[0m18:30:07.489504 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413)
[0m18:30:07.490261 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m18:30:07.504399 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m18:30:07.513873 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (compile): 18:30:07.490789 => 18:30:07.513549
[0m18:30:07.514593 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m18:30:07.541682 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m18:30:07.551499 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:07.552146 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m18:30:07.552720 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m18:30:07.553391 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:08.435312 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m18:30:08.443447 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (execute): 18:30:07.515044 => 18:30:08.443119
[0m18:30:08.444231 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: ROLLBACK
[0m18:30:08.444872 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:08.445433 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: Close
[0m18:30:08.559560 [info ] [Thread-2  ]: 1 of 35 PASS accepted_rating_range_genre_popularity_avg_rating__5__0 ........... [[32mPASS[0m in 1.07s]
[0m18:30:08.561072 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m18:30:08.562004 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m18:30:08.562811 [info ] [Thread-2  ]: 2 of 35 START test accepted_rating_range_movie_enriched_avg_rating__5__0 ....... [RUN]
[0m18:30:08.564537 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413, now test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8)
[0m18:30:08.565416 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m18:30:08.571939 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m18:30:08.581610 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (compile): 18:30:08.566008 => 18:30:08.581149
[0m18:30:08.582341 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m18:30:08.586534 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m18:30:08.596791 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:08.597645 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m18:30:08.598273 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m18:30:08.598918 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:09.438003 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m18:30:09.442262 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (execute): 18:30:08.582845 => 18:30:09.441890
[0m18:30:09.443146 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: ROLLBACK
[0m18:30:09.443807 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:09.444409 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: Close
[0m18:30:09.565225 [info ] [Thread-2  ]: 2 of 35 PASS accepted_rating_range_movie_enriched_avg_rating__5__0 ............. [[32mPASS[0m in 1.00s]
[0m18:30:09.566615 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m18:30:09.567364 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m18:30:09.568120 [info ] [Thread-2  ]: 3 of 35 START test accepted_rating_range_movie_performance_weighted_avg_rating__5__0  [RUN]
[0m18:30:09.569517 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8, now test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083)
[0m18:30:09.570207 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m18:30:09.580738 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m18:30:09.590898 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (compile): 18:30:09.570898 => 18:30:09.590457
[0m18:30:09.591665 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m18:30:09.597089 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m18:30:09.607770 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:09.608882 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m18:30:09.609690 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating < 0
   or weighted_avg_rating > 5


      
    ) dbt_internal_test
[0m18:30:09.610400 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:10.469445 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m18:30:10.473623 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (execute): 18:30:09.592220 => 18:30:10.473305
[0m18:30:10.474570 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: ROLLBACK
[0m18:30:10.475451 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:10.476217 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: Close
[0m18:30:10.590619 [info ] [Thread-2  ]: 3 of 35 PASS accepted_rating_range_movie_performance_weighted_avg_rating__5__0 . [[32mPASS[0m in 1.02s]
[0m18:30:10.591968 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m18:30:10.592741 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m18:30:10.593587 [info ] [Thread-2  ]: 4 of 35 START test accepted_rating_range_user_activity_avg_user_rating__5__0 ... [RUN]
[0m18:30:10.595364 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083, now test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a)
[0m18:30:10.596334 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m18:30:10.609632 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m18:30:10.619314 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (compile): 18:30:10.597077 => 18:30:10.618815
[0m18:30:10.620115 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m18:30:10.624371 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m18:30:10.634390 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:10.635116 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m18:30:10.636117 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating < 0
   or avg_user_rating > 5


      
    ) dbt_internal_test
[0m18:30:10.637170 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:11.313561 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m18:30:11.317661 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (execute): 18:30:10.620594 => 18:30:11.317315
[0m18:30:11.318443 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: ROLLBACK
[0m18:30:11.319260 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:11.320385 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: Close
[0m18:30:11.431759 [info ] [Thread-2  ]: 4 of 35 PASS accepted_rating_range_user_activity_avg_user_rating__5__0 ......... [[32mPASS[0m in 0.84s]
[0m18:30:11.433164 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m18:30:11.434009 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m18:30:11.434805 [info ] [Thread-2  ]: 5 of 35 START test not_null_genre_popularity_avg_rating ........................ [RUN]
[0m18:30:11.436111 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a, now test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9)
[0m18:30:11.436796 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m18:30:11.446362 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m18:30:11.457267 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (compile): 18:30:11.437352 => 18:30:11.456819
[0m18:30:11.458038 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m18:30:11.462670 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m18:30:11.472965 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:11.473718 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m18:30:11.474510 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating is null



      
    ) dbt_internal_test
[0m18:30:11.475481 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:12.223532 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m18:30:12.227813 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (execute): 18:30:11.458500 => 18:30:12.227473
[0m18:30:12.228624 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: ROLLBACK
[0m18:30:12.229562 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:12.230281 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: Close
[0m18:30:12.339830 [info ] [Thread-2  ]: 5 of 35 PASS not_null_genre_popularity_avg_rating .............................. [[32mPASS[0m in 0.90s]
[0m18:30:12.341197 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m18:30:12.342130 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m18:30:12.342982 [info ] [Thread-2  ]: 6 of 35 START test not_null_genre_popularity_first_rating_date ................. [RUN]
[0m18:30:12.344235 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9, now test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32)
[0m18:30:12.344934 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m18:30:12.350651 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m18:30:12.359261 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (compile): 18:30:12.345420 => 18:30:12.358935
[0m18:30:12.359885 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m18:30:12.364070 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m18:30:12.372905 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:12.373521 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m18:30:12.374172 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where first_rating_date is null



      
    ) dbt_internal_test
[0m18:30:12.374827 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:13.119101 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m18:30:13.123358 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (execute): 18:30:12.360392 => 18:30:13.123001
[0m18:30:13.124241 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: ROLLBACK
[0m18:30:13.124982 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:13.125639 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: Close
[0m18:30:13.232214 [info ] [Thread-2  ]: 6 of 35 PASS not_null_genre_popularity_first_rating_date ....................... [[32mPASS[0m in 0.89s]
[0m18:30:13.233818 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m18:30:13.234680 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m18:30:13.235432 [info ] [Thread-2  ]: 7 of 35 START test not_null_genre_popularity_genres ............................ [RUN]
[0m18:30:13.236691 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32, now test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa)
[0m18:30:13.237439 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m18:30:13.251867 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m18:30:13.261100 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (compile): 18:30:13.237965 => 18:30:13.260682
[0m18:30:13.261805 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m18:30:13.265850 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m18:30:13.276284 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:13.277039 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m18:30:13.277716 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`genre_popularity`
where genres is null



      
    ) dbt_internal_test
[0m18:30:13.278384 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:14.191637 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m18:30:14.195795 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (execute): 18:30:13.262240 => 18:30:14.195450
[0m18:30:14.196579 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: ROLLBACK
[0m18:30:14.197269 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:14.197964 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: Close
[0m18:30:14.307604 [info ] [Thread-2  ]: 7 of 35 PASS not_null_genre_popularity_genres .................................. [[32mPASS[0m in 1.07s]
[0m18:30:14.309061 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m18:30:14.309985 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m18:30:14.310744 [info ] [Thread-2  ]: 8 of 35 START test not_null_genre_popularity_last_rating_date .................. [RUN]
[0m18:30:14.311975 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa, now test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432)
[0m18:30:14.312703 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m18:30:14.321110 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m18:30:14.330781 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (compile): 18:30:14.313312 => 18:30:14.330331
[0m18:30:14.331530 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m18:30:14.335560 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m18:30:14.344306 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:14.344855 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m18:30:14.345458 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where last_rating_date is null



      
    ) dbt_internal_test
[0m18:30:14.346204 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:15.103777 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m18:30:15.108225 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (execute): 18:30:14.332012 => 18:30:15.107858
[0m18:30:15.109026 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: ROLLBACK
[0m18:30:15.109931 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:15.110603 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: Close
[0m18:30:15.216163 [info ] [Thread-2  ]: 8 of 35 PASS not_null_genre_popularity_last_rating_date ........................ [[32mPASS[0m in 0.90s]
[0m18:30:15.217555 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m18:30:15.218321 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m18:30:15.219026 [info ] [Thread-2  ]: 9 of 35 START test not_null_genre_popularity_popularity_rank ................... [RUN]
[0m18:30:15.220203 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432, now test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc)
[0m18:30:15.220858 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m18:30:15.230071 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m18:30:15.239176 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (compile): 18:30:15.221482 => 18:30:15.238825
[0m18:30:15.239852 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m18:30:15.243719 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m18:30:15.252369 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:15.252958 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m18:30:15.253574 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select popularity_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where popularity_rank is null



      
    ) dbt_internal_test
[0m18:30:15.254161 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:16.120501 [debug] [Thread-2  ]: SQL status: OK in 0.8700000047683716 seconds
[0m18:30:16.125445 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (execute): 18:30:15.240295 => 18:30:16.125026
[0m18:30:16.126488 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: ROLLBACK
[0m18:30:16.127461 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:16.128181 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: Close
[0m18:30:16.250612 [info ] [Thread-2  ]: 9 of 35 PASS not_null_genre_popularity_popularity_rank ......................... [[32mPASS[0m in 1.03s]
[0m18:30:16.251917 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m18:30:16.252818 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m18:30:16.253616 [info ] [Thread-2  ]: 10 of 35 START test not_null_genre_popularity_quality_rank ..................... [RUN]
[0m18:30:16.254976 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc, now test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a)
[0m18:30:16.255794 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m18:30:16.264641 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m18:30:16.273892 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (compile): 18:30:16.256350 => 18:30:16.273488
[0m18:30:16.274621 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m18:30:16.278442 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m18:30:16.286986 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:16.287512 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m18:30:16.288089 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select quality_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where quality_rank is null



      
    ) dbt_internal_test
[0m18:30:16.288694 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:17.051632 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m18:30:17.055517 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (execute): 18:30:16.275148 => 18:30:17.055164
[0m18:30:17.056216 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: ROLLBACK
[0m18:30:17.056891 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:17.057469 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: Close
[0m18:30:17.175458 [info ] [Thread-2  ]: 10 of 35 PASS not_null_genre_popularity_quality_rank ........................... [[32mPASS[0m in 0.92s]
[0m18:30:17.176906 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m18:30:17.177704 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m18:30:17.178417 [info ] [Thread-2  ]: 11 of 35 START test not_null_genre_popularity_rating_count ..................... [RUN]
[0m18:30:17.179607 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a, now test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856)
[0m18:30:17.180261 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m18:30:17.186631 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m18:30:17.196185 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (compile): 18:30:17.180743 => 18:30:17.195882
[0m18:30:17.196843 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m18:30:17.200638 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m18:30:17.208865 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:17.209446 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m18:30:17.209973 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`genre_popularity`
where rating_count is null



      
    ) dbt_internal_test
[0m18:30:17.210547 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:18.053227 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m18:30:18.057427 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (execute): 18:30:17.197333 => 18:30:18.057047
[0m18:30:18.058307 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: ROLLBACK
[0m18:30:18.059127 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:18.059832 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: Close
[0m18:30:18.180803 [info ] [Thread-2  ]: 11 of 35 PASS not_null_genre_popularity_rating_count ........................... [[32mPASS[0m in 1.00s]
[0m18:30:18.182308 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m18:30:18.183226 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m18:30:18.184066 [info ] [Thread-2  ]: 12 of 35 START test not_null_movie_enriched_avg_rating ......................... [RUN]
[0m18:30:18.185346 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856, now test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd)
[0m18:30:18.186022 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m18:30:18.192356 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m18:30:18.201616 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (compile): 18:30:18.186532 => 18:30:18.201329
[0m18:30:18.202240 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m18:30:18.206035 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m18:30:18.214868 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:18.215465 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m18:30:18.216097 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating is null



      
    ) dbt_internal_test
[0m18:30:18.216799 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:18.985952 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m18:30:18.990344 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (execute): 18:30:18.202718 => 18:30:18.989979
[0m18:30:18.991209 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: ROLLBACK
[0m18:30:18.991892 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:18.992576 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: Close
[0m18:30:19.109481 [info ] [Thread-2  ]: 12 of 35 PASS not_null_movie_enriched_avg_rating ............................... [[32mPASS[0m in 0.92s]
[0m18:30:19.110932 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m18:30:19.111845 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m18:30:19.112778 [info ] [Thread-2  ]: 13 of 35 START test not_null_movie_enriched_genres ............................. [RUN]
[0m18:30:19.114143 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd, now test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27)
[0m18:30:19.114875 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m18:30:19.121302 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m18:30:19.131423 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (compile): 18:30:19.115469 => 18:30:19.131037
[0m18:30:19.132063 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m18:30:19.143874 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m18:30:19.153543 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:19.154321 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m18:30:19.154930 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`movie_enriched`
where genres is null



      
    ) dbt_internal_test
[0m18:30:19.155704 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:19.997065 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m18:30:20.001225 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (execute): 18:30:19.132520 => 18:30:20.000912
[0m18:30:20.001978 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: ROLLBACK
[0m18:30:20.002821 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:20.003557 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: Close
[0m18:30:20.118594 [info ] [Thread-2  ]: 13 of 35 PASS not_null_movie_enriched_genres ................................... [[32mPASS[0m in 1.00s]
[0m18:30:20.119871 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m18:30:20.120727 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m18:30:20.121601 [info ] [Thread-2  ]: 14 of 35 START test not_null_movie_enriched_movie_id ........................... [RUN]
[0m18:30:20.122919 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27, now test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86)
[0m18:30:20.123609 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m18:30:20.129980 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m18:30:20.139735 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (compile): 18:30:20.124204 => 18:30:20.139387
[0m18:30:20.140428 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m18:30:20.144276 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m18:30:20.153204 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:20.153989 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m18:30:20.154640 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is null



      
    ) dbt_internal_test
[0m18:30:20.155218 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:20.963876 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m18:30:20.968599 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (execute): 18:30:20.140889 => 18:30:20.968214
[0m18:30:20.969481 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: ROLLBACK
[0m18:30:20.970505 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:20.971222 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: Close
[0m18:30:21.082139 [info ] [Thread-2  ]: 14 of 35 PASS not_null_movie_enriched_movie_id ................................. [[32mPASS[0m in 0.96s]
[0m18:30:21.083552 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m18:30:21.084429 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m18:30:21.085209 [info ] [Thread-2  ]: 15 of 35 START test not_null_movie_enriched_title .............................. [RUN]
[0m18:30:21.086482 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86, now test.my_dbt_project.not_null_movie_enriched_title.d20f00c292)
[0m18:30:21.087241 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m18:30:21.093726 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m18:30:21.103899 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (compile): 18:30:21.087876 => 18:30:21.103390
[0m18:30:21.104642 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m18:30:21.108638 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m18:30:21.118530 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:21.119241 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m18:30:21.119861 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`movie_enriched`
where title is null



      
    ) dbt_internal_test
[0m18:30:21.120549 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:21.997054 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m18:30:22.001013 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (execute): 18:30:21.105095 => 18:30:22.000680
[0m18:30:22.001726 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: ROLLBACK
[0m18:30:22.002374 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:22.003066 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: Close
[0m18:30:22.111651 [info ] [Thread-2  ]: 15 of 35 PASS not_null_movie_enriched_title .................................... [[32mPASS[0m in 1.03s]
[0m18:30:22.113037 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m18:30:22.113900 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m18:30:22.114748 [info ] [Thread-2  ]: 16 of 35 START test not_null_movie_enriched_total_ratings ...................... [RUN]
[0m18:30:22.115966 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_title.d20f00c292, now test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04)
[0m18:30:22.116629 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m18:30:22.122659 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m18:30:22.132014 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (compile): 18:30:22.117126 => 18:30:22.131623
[0m18:30:22.132731 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m18:30:22.144535 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m18:30:22.153515 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:22.154166 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m18:30:22.154756 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select total_ratings
from `workspace`.`movielens_volume`.`movie_enriched`
where total_ratings is null



      
    ) dbt_internal_test
[0m18:30:22.155415 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:22.925662 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m18:30:22.929940 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (execute): 18:30:22.133189 => 18:30:22.929610
[0m18:30:22.930710 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: ROLLBACK
[0m18:30:22.931404 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:22.932154 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: Close
[0m18:30:23.051590 [info ] [Thread-2  ]: 16 of 35 PASS not_null_movie_enriched_total_ratings ............................ [[32mPASS[0m in 0.94s]
[0m18:30:23.052999 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m18:30:23.053910 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m18:30:23.054650 [info ] [Thread-2  ]: 17 of 35 START test not_null_movie_performance_genre_rank ...................... [RUN]
[0m18:30:23.055978 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04, now test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e)
[0m18:30:23.056786 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m18:30:23.065590 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m18:30:23.075363 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (compile): 18:30:23.057365 => 18:30:23.075053
[0m18:30:23.076020 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m18:30:23.079961 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m18:30:23.088862 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:23.089477 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m18:30:23.090045 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genre_rank
from `workspace`.`movielens_volume`.`movie_performance`
where genre_rank is null



      
    ) dbt_internal_test
[0m18:30:23.090638 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:23.911507 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m18:30:23.915980 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (execute): 18:30:23.076473 => 18:30:23.915599
[0m18:30:23.916736 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: ROLLBACK
[0m18:30:23.917507 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:23.918141 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: Close
[0m18:30:24.075155 [info ] [Thread-2  ]: 17 of 35 PASS not_null_movie_performance_genre_rank ............................ [[32mPASS[0m in 1.02s]
[0m18:30:24.076812 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m18:30:24.077755 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m18:30:24.078713 [info ] [Thread-2  ]: 18 of 35 START test not_null_movie_performance_movie_id ........................ [RUN]
[0m18:30:24.080297 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e, now test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52)
[0m18:30:24.081052 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m18:30:24.089550 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m18:30:24.098924 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (compile): 18:30:24.081575 => 18:30:24.098571
[0m18:30:24.099637 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m18:30:24.103494 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m18:30:24.112643 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:24.113377 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m18:30:24.114130 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is null



      
    ) dbt_internal_test
[0m18:30:24.114769 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:24.922304 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m18:30:24.926469 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (execute): 18:30:24.100071 => 18:30:24.926112
[0m18:30:24.927347 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: ROLLBACK
[0m18:30:24.928013 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:24.928679 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: Close
[0m18:30:25.043930 [info ] [Thread-2  ]: 18 of 35 PASS not_null_movie_performance_movie_id .............................. [[32mPASS[0m in 0.96s]
[0m18:30:25.045479 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m18:30:25.046358 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m18:30:25.047118 [info ] [Thread-2  ]: 19 of 35 START test not_null_movie_performance_polarization_score .............. [RUN]
[0m18:30:25.048405 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52, now test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6)
[0m18:30:25.049032 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m18:30:25.057910 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m18:30:25.067545 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (compile): 18:30:25.049540 => 18:30:25.067093
[0m18:30:25.068290 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m18:30:25.073574 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m18:30:25.082753 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:25.083419 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m18:30:25.084000 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select polarization_score
from `workspace`.`movielens_volume`.`movie_performance`
where polarization_score is null



      
    ) dbt_internal_test
[0m18:30:25.084732 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:25.915213 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m18:30:25.923098 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (execute): 18:30:25.068750 => 18:30:25.922610
[0m18:30:25.924296 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: ROLLBACK
[0m18:30:25.925282 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:25.926119 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: Close
[0m18:30:26.037859 [info ] [Thread-2  ]: 19 of 35 PASS not_null_movie_performance_polarization_score .................... [[32mPASS[0m in 0.99s]
[0m18:30:26.039635 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m18:30:26.040755 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m18:30:26.041686 [info ] [Thread-2  ]: 20 of 35 START test not_null_movie_performance_rating_count .................... [RUN]
[0m18:30:26.043166 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6, now test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac)
[0m18:30:26.043964 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m18:30:26.056141 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m18:30:26.067130 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (compile): 18:30:26.044481 => 18:30:26.066658
[0m18:30:26.067944 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m18:30:26.072730 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m18:30:26.083131 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:26.083846 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m18:30:26.084592 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`movie_performance`
where rating_count is null



      
    ) dbt_internal_test
[0m18:30:26.085344 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:26.990060 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m18:30:26.996607 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (execute): 18:30:26.068491 => 18:30:26.996187
[0m18:30:26.997472 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: ROLLBACK
[0m18:30:26.998256 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:26.998997 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: Close
[0m18:30:27.113390 [info ] [Thread-2  ]: 20 of 35 PASS not_null_movie_performance_rating_count .......................... [[32mPASS[0m in 1.07s]
[0m18:30:27.115208 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m18:30:27.115948 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m18:30:27.116696 [info ] [Thread-2  ]: 21 of 35 START test not_null_movie_performance_top_rating_percentage ........... [RUN]
[0m18:30:27.118045 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac, now test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb)
[0m18:30:27.118698 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m18:30:27.126370 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m18:30:27.136245 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (compile): 18:30:27.119219 => 18:30:27.135801
[0m18:30:27.136952 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m18:30:27.141074 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m18:30:27.150497 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:27.151189 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m18:30:27.151804 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_rating_percentage
from `workspace`.`movielens_volume`.`movie_performance`
where top_rating_percentage is null



      
    ) dbt_internal_test
[0m18:30:27.152541 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:27.923858 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m18:30:27.928311 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (execute): 18:30:27.137397 => 18:30:27.927916
[0m18:30:27.929245 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: ROLLBACK
[0m18:30:27.930028 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:27.930765 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: Close
[0m18:30:28.039505 [info ] [Thread-2  ]: 21 of 35 PASS not_null_movie_performance_top_rating_percentage ................. [[32mPASS[0m in 0.92s]
[0m18:30:28.041194 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m18:30:28.042215 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m18:30:28.043031 [info ] [Thread-2  ]: 22 of 35 START test not_null_movie_performance_weighted_avg_rating ............. [RUN]
[0m18:30:28.044389 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb, now test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5)
[0m18:30:28.045005 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m18:30:28.051579 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m18:30:28.061746 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (compile): 18:30:28.045452 => 18:30:28.061239
[0m18:30:28.062629 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m18:30:28.073581 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m18:30:28.084065 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:28.084930 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m18:30:28.085566 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select weighted_avg_rating
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating is null



      
    ) dbt_internal_test
[0m18:30:28.086209 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:28.913521 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m18:30:28.917913 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (execute): 18:30:28.063203 => 18:30:28.917590
[0m18:30:28.918725 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: ROLLBACK
[0m18:30:28.919569 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:28.920255 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: Close
[0m18:30:29.030957 [info ] [Thread-2  ]: 22 of 35 PASS not_null_movie_performance_weighted_avg_rating ................... [[32mPASS[0m in 0.99s]
[0m18:30:29.032371 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m18:30:29.033229 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m18:30:29.034043 [info ] [Thread-2  ]: 23 of 35 START test not_null_user_activity_avg_user_rating ..................... [RUN]
[0m18:30:29.035478 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5, now test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b)
[0m18:30:29.036244 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m18:30:29.042849 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m18:30:29.052415 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (compile): 18:30:29.036840 => 18:30:29.052035
[0m18:30:29.053139 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m18:30:29.057235 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m18:30:29.066486 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:29.067100 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m18:30:29.067680 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_user_rating
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating is null



      
    ) dbt_internal_test
[0m18:30:29.068328 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:29.925201 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m18:30:29.929489 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (execute): 18:30:29.053660 => 18:30:29.929176
[0m18:30:29.930314 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: ROLLBACK
[0m18:30:29.931177 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:29.931889 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: Close
[0m18:30:30.060342 [info ] [Thread-2  ]: 23 of 35 PASS not_null_user_activity_avg_user_rating ........................... [[32mPASS[0m in 1.03s]
[0m18:30:30.061898 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m18:30:30.062924 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m18:30:30.064030 [info ] [Thread-2  ]: 24 of 35 START test not_null_user_activity_first_activity_date ................. [RUN]
[0m18:30:30.065518 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b, now test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac)
[0m18:30:30.066222 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m18:30:30.072765 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m18:30:30.082219 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (compile): 18:30:30.066771 => 18:30:30.081765
[0m18:30:30.083011 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m18:30:30.086918 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m18:30:30.095979 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:30.096575 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m18:30:30.097157 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where first_activity_date is null



      
    ) dbt_internal_test
[0m18:30:30.097762 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:31.190473 [debug] [Thread-2  ]: SQL status: OK in 1.090000033378601 seconds
[0m18:30:31.194811 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (execute): 18:30:30.083449 => 18:30:31.194429
[0m18:30:31.195657 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: ROLLBACK
[0m18:30:31.196655 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:31.197363 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: Close
[0m18:30:31.312679 [info ] [Thread-2  ]: 24 of 35 PASS not_null_user_activity_first_activity_date ....................... [[32mPASS[0m in 1.25s]
[0m18:30:31.314235 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m18:30:31.315066 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m18:30:31.316019 [info ] [Thread-2  ]: 25 of 35 START test not_null_user_activity_influence_score ..................... [RUN]
[0m18:30:31.317496 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac, now test.my_dbt_project.not_null_user_activity_influence_score.758852d325)
[0m18:30:31.318184 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m18:30:31.325236 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m18:30:31.335283 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (compile): 18:30:31.318694 => 18:30:31.334816
[0m18:30:31.336093 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m18:30:31.349411 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m18:30:31.359454 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:31.360276 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m18:30:31.360994 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select influence_score
from `workspace`.`movielens_volume`.`user_activity`
where influence_score is null



      
    ) dbt_internal_test
[0m18:30:31.361765 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:32.187045 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m18:30:32.191115 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (execute): 18:30:31.336909 => 18:30:32.190800
[0m18:30:32.191908 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: ROLLBACK
[0m18:30:32.192585 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:32.193290 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: Close
[0m18:30:32.317380 [info ] [Thread-2  ]: 25 of 35 PASS not_null_user_activity_influence_score ........................... [[32mPASS[0m in 1.00s]
[0m18:30:32.318847 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m18:30:32.319726 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m18:30:32.320538 [info ] [Thread-2  ]: 26 of 35 START test not_null_user_activity_last_activity_date .................. [RUN]
[0m18:30:32.321849 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_influence_score.758852d325, now test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad)
[0m18:30:32.322607 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m18:30:32.331716 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m18:30:32.342005 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (compile): 18:30:32.323118 => 18:30:32.341593
[0m18:30:32.342730 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m18:30:32.347007 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m18:30:32.356492 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:32.357280 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m18:30:32.357963 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where last_activity_date is null



      
    ) dbt_internal_test
[0m18:30:32.358633 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:33.124090 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m18:30:33.128615 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (execute): 18:30:32.343182 => 18:30:33.128247
[0m18:30:33.129508 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: ROLLBACK
[0m18:30:33.130411 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:33.131081 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: Close
[0m18:30:33.245053 [info ] [Thread-2  ]: 26 of 35 PASS not_null_user_activity_last_activity_date ........................ [[32mPASS[0m in 0.92s]
[0m18:30:33.246607 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m18:30:33.247428 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m18:30:33.248246 [info ] [Thread-2  ]: 27 of 35 START test not_null_user_activity_top_genre ........................... [RUN]
[0m18:30:33.249682 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad, now test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313)
[0m18:30:33.250405 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m18:30:33.259399 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m18:30:33.269531 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (compile): 18:30:33.250916 => 18:30:33.269142
[0m18:30:33.270265 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m18:30:33.274469 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m18:30:33.284055 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:33.284745 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m18:30:33.285377 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_genre
from `workspace`.`movielens_volume`.`user_activity`
where top_genre is null



      
    ) dbt_internal_test
[0m18:30:33.285981 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:34.098654 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m18:30:34.102844 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (execute): 18:30:33.270805 => 18:30:34.102506
[0m18:30:34.103804 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: ROLLBACK
[0m18:30:34.104528 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:34.105221 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: Close
[0m18:30:34.227507 [info ] [Thread-2  ]: 27 of 35 PASS not_null_user_activity_top_genre ................................. [[32mPASS[0m in 0.98s]
[0m18:30:34.228938 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m18:30:34.229865 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m18:30:34.230826 [info ] [Thread-2  ]: 28 of 35 START test not_null_user_activity_user_id ............................. [RUN]
[0m18:30:34.232246 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313, now test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36)
[0m18:30:34.232957 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m18:30:34.241815 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m18:30:34.252718 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (compile): 18:30:34.233459 => 18:30:34.252269
[0m18:30:34.253556 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m18:30:34.259662 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m18:30:34.269730 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:34.270389 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m18:30:34.270928 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`user_activity`
where user_id is null



      
    ) dbt_internal_test
[0m18:30:34.271557 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:35.073610 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m18:30:35.078177 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (execute): 18:30:34.254121 => 18:30:35.077819
[0m18:30:35.079035 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: ROLLBACK
[0m18:30:35.079775 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:35.080421 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: Close
[0m18:30:35.199680 [info ] [Thread-2  ]: 28 of 35 PASS not_null_user_activity_user_id ................................... [[32mPASS[0m in 0.97s]
[0m18:30:35.201108 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m18:30:35.201999 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m18:30:35.202743 [info ] [Thread-2  ]: 29 of 35 START test not_null_user_activity_user_rating_count ................... [RUN]
[0m18:30:35.204099 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36, now test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e)
[0m18:30:35.204957 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m18:30:35.213793 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m18:30:35.222859 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (compile): 18:30:35.205520 => 18:30:35.222504
[0m18:30:35.223660 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m18:30:35.227529 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m18:30:35.236514 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:35.237080 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m18:30:35.237672 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_rating_count
from `workspace`.`movielens_volume`.`user_activity`
where user_rating_count is null



      
    ) dbt_internal_test
[0m18:30:35.238322 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:36.057423 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m18:30:36.061535 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (execute): 18:30:35.224184 => 18:30:36.061171
[0m18:30:36.062302 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: ROLLBACK
[0m18:30:36.063026 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:36.063682 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: Close
[0m18:30:36.185437 [info ] [Thread-2  ]: 29 of 35 PASS not_null_user_activity_user_rating_count ......................... [[32mPASS[0m in 0.98s]
[0m18:30:36.186802 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m18:30:36.187815 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:30:36.188542 [info ] [Thread-2  ]: 30 of 35 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:30:36.189868 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m18:30:36.190711 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:30:36.204085 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:30:36.214581 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 18:30:36.191286 => 18:30:36.214146
[0m18:30:36.215328 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:30:36.219593 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:30:36.228738 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:36.229603 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m18:30:36.230370 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:30:36.231305 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:36.910132 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m18:30:36.914603 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 18:30:36.215913 => 18:30:36.914224
[0m18:30:36.915447 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m18:30:36.916175 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:36.916970 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m18:30:37.025002 [info ] [Thread-2  ]: 30 of 35 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.84s]
[0m18:30:37.026331 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m18:30:37.027233 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:30:37.028064 [info ] [Thread-2  ]: 31 of 35 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m18:30:37.029199 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m18:30:37.029834 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:30:37.037860 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:30:37.047688 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 18:30:37.030325 => 18:30:37.047254
[0m18:30:37.048453 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:30:37.052351 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:30:37.061577 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:37.062160 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m18:30:37.062740 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:30:37.063398 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:37.964269 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m18:30:37.968696 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 18:30:37.048978 => 18:30:37.968368
[0m18:30:37.969509 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m18:30:37.970302 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:37.970999 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m18:30:38.086477 [info ] [Thread-2  ]: 31 of 35 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.06s]
[0m18:30:38.087815 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m18:30:38.088690 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m18:30:38.089574 [info ] [Thread-2  ]: 32 of 35 START test unique_genre_popularity_genres ............................. [RUN]
[0m18:30:38.091081 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075)
[0m18:30:38.091828 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m18:30:38.101813 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m18:30:38.111180 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (compile): 18:30:38.092372 => 18:30:38.110692
[0m18:30:38.111957 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m18:30:38.115999 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m18:30:38.125254 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:38.125910 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m18:30:38.126463 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    genres as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`genre_popularity`
where genres is not null
group by genres
having count(*) > 1



      
    ) dbt_internal_test
[0m18:30:38.127070 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:38.958398 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m18:30:38.962957 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (execute): 18:30:38.112464 => 18:30:38.962605
[0m18:30:38.963912 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: ROLLBACK
[0m18:30:38.964813 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:38.965675 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: Close
[0m18:30:39.076746 [info ] [Thread-2  ]: 32 of 35 PASS unique_genre_popularity_genres ................................... [[32mPASS[0m in 0.99s]
[0m18:30:39.078411 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m18:30:39.079394 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m18:30:39.080208 [info ] [Thread-2  ]: 33 of 35 START test unique_movie_enriched_movie_id ............................. [RUN]
[0m18:30:39.081502 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075, now test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a)
[0m18:30:39.082216 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m18:30:39.088533 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m18:30:39.097891 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (compile): 18:30:39.082787 => 18:30:39.097567
[0m18:30:39.098568 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m18:30:39.102594 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m18:30:39.111305 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:39.111899 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m18:30:39.112532 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:30:39.113195 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:39.956605 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m18:30:39.961120 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (execute): 18:30:39.099172 => 18:30:39.960774
[0m18:30:39.961976 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: ROLLBACK
[0m18:30:39.962728 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:39.963459 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m18:30:40.081136 [info ] [Thread-2  ]: 33 of 35 PASS unique_movie_enriched_movie_id ................................... [[32mPASS[0m in 1.00s]
[0m18:30:40.082539 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m18:30:40.083408 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m18:30:40.084262 [info ] [Thread-2  ]: 34 of 35 START test unique_movie_performance_movie_id .......................... [RUN]
[0m18:30:40.085577 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a, now test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22)
[0m18:30:40.086253 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m18:30:40.092697 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m18:30:40.102126 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (compile): 18:30:40.086835 => 18:30:40.101664
[0m18:30:40.102899 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m18:30:40.106968 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m18:30:40.117544 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:40.118328 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m18:30:40.119010 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:30:40.119690 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:40.808309 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m18:30:40.819309 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (execute): 18:30:40.103368 => 18:30:40.817304
[0m18:30:40.820174 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: ROLLBACK
[0m18:30:40.820864 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:40.821509 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: Close
[0m18:30:40.939447 [info ] [Thread-2  ]: 34 of 35 PASS unique_movie_performance_movie_id ................................ [[32mPASS[0m in 0.85s]
[0m18:30:40.940851 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m18:30:40.941853 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m18:30:40.942729 [info ] [Thread-2  ]: 35 of 35 START test unique_user_activity_user_id ............................... [RUN]
[0m18:30:40.944125 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22, now test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda)
[0m18:30:40.944900 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m18:30:40.951459 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m18:30:40.961885 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (compile): 18:30:40.945461 => 18:30:40.961456
[0m18:30:40.962664 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m18:30:40.966673 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m18:30:40.976658 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:40.977324 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m18:30:40.977902 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`user_activity`
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:30:40.978491 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m18:30:41.731227 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m18:30:41.735411 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (execute): 18:30:40.963101 => 18:30:41.735090
[0m18:30:41.736182 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: ROLLBACK
[0m18:30:41.736934 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m18:30:41.737660 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: Close
[0m18:30:41.848159 [info ] [Thread-2  ]: 35 of 35 PASS unique_user_activity_user_id ..................................... [[32mPASS[0m in 0.90s]
[0m18:30:41.849598 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m18:30:41.852777 [debug] [MainThread]: On master: ROLLBACK
[0m18:30:41.853390 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:30:42.567219 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:30:42.568056 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:30:42.568642 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:30:42.569191 [debug] [MainThread]: On master: ROLLBACK
[0m18:30:42.569740 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:30:42.570266 [debug] [MainThread]: On master: Close
[0m18:30:42.682406 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:30:42.683150 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda' was properly closed.
[0m18:30:42.685676 [info ] [MainThread]: 
[0m18:30:42.687050 [info ] [MainThread]: Finished running 35 tests in 0 hours 0 minutes and 35.99 seconds (35.99s).
[0m18:30:42.694858 [debug] [MainThread]: Command end result
[0m18:30:42.723649 [info ] [MainThread]: 
[0m18:30:42.724531 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:30:42.725243 [info ] [MainThread]: 
[0m18:30:42.726020 [info ] [MainThread]: Done. PASS=35 WARN=0 ERROR=0 SKIP=0 TOTAL=35
[0m18:30:42.727431 [debug] [MainThread]: Command `dbt test` succeeded at 18:30:42.727290 after 38.69 seconds
[0m18:30:42.728323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63facc35e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63cdcdd130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63cdfcf6d0>]}
[0m18:30:42.729057 [debug] [MainThread]: Flushing usage events
[0m20:27:15.842044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34b7643580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34b5baf190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34b5baf8e0>]}


============================== 20:27:15.851909 | 982bec71-ce70-4685-851c-640d29f7438d ==============================
[0m20:27:15.851909 [info ] [MainThread]: Running with dbt=1.5.2
[0m20:27:15.852890 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m20:27:17.510018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '982bec71-ce70-4685-851c-640d29f7438d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34b5baf130>]}
[0m20:27:17.554884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '982bec71-ce70-4685-851c-640d29f7438d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f348a8f5ac0>]}
[0m20:27:17.556267 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m20:27:17.619402 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m20:27:18.993174 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:27:18.993981 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:27:19.005937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '982bec71-ce70-4685-851c-640d29f7438d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f348a8a03d0>]}
[0m20:27:19.044944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '982bec71-ce70-4685-851c-640d29f7438d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f348a7cd400>]}
[0m20:27:19.045880 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m20:27:19.046646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '982bec71-ce70-4685-851c-640d29f7438d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f348a7cd460>]}
[0m20:27:19.049460 [info ] [MainThread]: 
[0m20:27:19.050861 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:27:19.053122 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:27:19.053924 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:27:19.054465 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m20:27:19.055071 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:28:20.009717 [debug] [ThreadPool]: SQL status: OK in 60.95000076293945 seconds
[0m20:28:20.012040 [debug] [ThreadPool]: On list_workspace: Close
[0m20:28:20.127671 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m20:28:20.129123 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m20:28:20.141642 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:28:20.142246 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m20:28:20.142813 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m20:28:20.143375 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:28:22.243249 [debug] [ThreadPool]: SQL status: OK in 2.0999999046325684 seconds
[0m20:28:22.244775 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m20:28:22.245411 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m20:28:22.245958 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:28:22.246499 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m20:28:22.365211 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m20:28:22.374772 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m20:28:22.375593 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m20:28:22.376566 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:28:23.247192 [debug] [ThreadPool]: SQL status: OK in 0.8700000047683716 seconds
[0m20:28:23.250714 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m20:28:23.367193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '982bec71-ce70-4685-851c-640d29f7438d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f348a7cd040>]}
[0m20:28:23.368291 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:28:23.368904 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:28:23.369896 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:28:23.370723 [info ] [MainThread]: 
[0m20:28:23.391483 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_links
[0m20:28:23.392463 [info ] [Thread-4  ]: 1 of 4 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m20:28:23.393701 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.silver_links)
[0m20:28:23.394420 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_links
[0m20:28:23.404532 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m20:28:23.413223 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (compile): 20:28:23.394905 => 20:28:23.412744
[0m20:28:23.413993 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_links
[0m20:28:23.433425 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:28:23.434127 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m20:28:23.434991 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

      describe extended `workspace`.`movielens_volume`.`silver_links`
  
[0m20:28:23.435747 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m20:28:27.871240 [debug] [Thread-4  ]: SQL status: OK in 4.440000057220459 seconds
[0m20:28:27.941583 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m20:28:27.951231 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m20:28:27.951996 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
    using delta
      
      
      
      
      
      
      as
      

with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m20:28:36.688459 [debug] [Thread-4  ]: SQL status: OK in 8.739999771118164 seconds
[0m20:28:36.885076 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (execute): 20:28:23.414470 => 20:28:36.884808
[0m20:28:36.886096 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: ROLLBACK
[0m20:28:36.886791 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m20:28:36.887420 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: Close
[0m20:28:37.006347 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '982bec71-ce70-4685-851c-640d29f7438d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f348948de80>]}
[0m20:28:37.007551 [info ] [Thread-4  ]: 1 of 4 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 13.61s]
[0m20:28:37.008925 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_links
[0m20:28:37.009874 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_movies
[0m20:28:37.010737 [info ] [Thread-4  ]: 2 of 4 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m20:28:37.012104 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_links, now model.my_dbt_project.silver_movies)
[0m20:28:37.013001 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_movies
[0m20:28:37.018049 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m20:28:37.026620 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (compile): 20:28:37.013675 => 20:28:37.026226
[0m20:28:37.027367 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_movies
[0m20:28:37.037044 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:28:37.037819 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m20:28:37.038510 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

      describe extended `workspace`.`movielens_volume`.`silver_movies`
  
[0m20:28:37.039115 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m20:28:38.294072 [debug] [Thread-4  ]: SQL status: OK in 1.25 seconds
[0m20:28:38.303110 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m20:28:38.313564 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m20:28:38.314571 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m20:28:41.879713 [debug] [Thread-4  ]: SQL status: OK in 3.559999942779541 seconds
[0m20:28:41.883796 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (execute): 20:28:37.027862 => 20:28:41.883469
[0m20:28:41.884623 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: ROLLBACK
[0m20:28:41.885266 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m20:28:41.885831 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: Close
[0m20:28:41.995198 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '982bec71-ce70-4685-851c-640d29f7438d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f348a6171c0>]}
[0m20:28:41.996415 [info ] [Thread-4  ]: 2 of 4 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 4.98s]
[0m20:28:41.997683 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_movies
[0m20:28:41.998469 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_ratings
[0m20:28:41.999291 [info ] [Thread-4  ]: 3 of 4 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m20:28:42.000660 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_movies, now model.my_dbt_project.silver_ratings)
[0m20:28:42.001379 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_ratings
[0m20:28:42.006669 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m20:28:42.015878 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (compile): 20:28:42.001841 => 20:28:42.015380
[0m20:28:42.016648 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_ratings
[0m20:28:42.023255 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:28:42.024001 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m20:28:42.024619 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

      describe extended `workspace`.`movielens_volume`.`silver_ratings`
  
[0m20:28:42.025506 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m20:28:43.484451 [debug] [Thread-4  ]: SQL status: OK in 1.4600000381469727 seconds
[0m20:28:43.492089 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m20:28:43.502366 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m20:28:43.503523 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m20:28:49.232841 [debug] [Thread-4  ]: SQL status: OK in 5.730000019073486 seconds
[0m20:28:49.364364 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (execute): 20:28:42.017209 => 20:28:49.363750
[0m20:28:49.365731 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: ROLLBACK
[0m20:28:49.366700 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m20:28:49.368139 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: Close
[0m20:28:49.476283 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '982bec71-ce70-4685-851c-640d29f7438d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34894634f0>]}
[0m20:28:49.477603 [info ] [Thread-4  ]: 3 of 4 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 7.48s]
[0m20:28:49.479354 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_ratings
[0m20:28:49.480194 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_tags
[0m20:28:49.481121 [info ] [Thread-4  ]: 4 of 4 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m20:28:49.482479 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_ratings, now model.my_dbt_project.silver_tags)
[0m20:28:49.483230 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_tags
[0m20:28:49.498673 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m20:28:49.508432 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (compile): 20:28:49.483789 => 20:28:49.508054
[0m20:28:49.509158 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_tags
[0m20:28:49.515877 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:28:49.516492 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m20:28:49.517042 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

      describe extended `workspace`.`movielens_volume`.`silver_tags`
  
[0m20:28:49.517670 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m20:28:50.761475 [debug] [Thread-4  ]: SQL status: OK in 1.2400000095367432 seconds
[0m20:28:50.774626 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m20:28:50.784739 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m20:28:50.785585 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m20:28:54.605661 [debug] [Thread-4  ]: SQL status: OK in 3.819999933242798 seconds
[0m20:28:54.609542 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (execute): 20:28:49.509703 => 20:28:54.609212
[0m20:28:54.610342 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: ROLLBACK
[0m20:28:54.611249 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m20:28:54.611921 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: Close
[0m20:28:54.730344 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '982bec71-ce70-4685-851c-640d29f7438d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3489568580>]}
[0m20:28:54.731667 [info ] [Thread-4  ]: 4 of 4 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 5.25s]
[0m20:28:54.733125 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_tags
[0m20:28:54.735901 [debug] [MainThread]: On master: ROLLBACK
[0m20:28:54.736664 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:28:55.092435 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:28:55.093427 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:28:55.094028 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:28:55.094746 [debug] [MainThread]: On master: ROLLBACK
[0m20:28:55.095507 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:28:55.096164 [debug] [MainThread]: On master: Close
[0m20:28:55.205127 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:28:55.206035 [debug] [MainThread]: Connection 'model.my_dbt_project.silver_tags' was properly closed.
[0m20:28:55.208574 [info ] [MainThread]: 
[0m20:28:55.209932 [info ] [MainThread]: Finished running 4 table models in 0 hours 1 minutes and 36.16 seconds (96.16s).
[0m20:28:55.212031 [debug] [MainThread]: Command end result
[0m20:28:55.240441 [info ] [MainThread]: 
[0m20:28:55.241472 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:28:55.242342 [info ] [MainThread]: 
[0m20:28:55.243242 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m20:28:55.244654 [debug] [MainThread]: Command `dbt run` succeeded at 20:28:55.244487 after 102.00 seconds
[0m20:28:55.245412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34b7643580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f348a8f5ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f348a5e7580>]}
[0m20:28:55.246049 [debug] [MainThread]: Flushing usage events
[0m20:28:59.294450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8ee486670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8ec9ee280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8ec9eea30>]}


============================== 20:28:59.302085 | da98f69a-b741-432b-bdb5-aecd8313cb38 ==============================
[0m20:28:59.302085 [info ] [MainThread]: Running with dbt=1.5.2
[0m20:28:59.303145 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/opt/airflow/dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m20:29:00.424671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'da98f69a-b741-432b-bdb5-aecd8313cb38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8ec9ee250>]}
[0m20:29:00.456323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'da98f69a-b741-432b-bdb5-aecd8313cb38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c177fa60>]}
[0m20:29:00.457351 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m20:29:00.501805 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m20:29:01.726068 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:29:01.726842 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:29:01.738841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'da98f69a-b741-432b-bdb5-aecd8313cb38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c170c4c0>]}
[0m20:29:01.775008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'da98f69a-b741-432b-bdb5-aecd8313cb38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c16374c0>]}
[0m20:29:01.776032 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m20:29:01.776890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da98f69a-b741-432b-bdb5-aecd8313cb38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c1637490>]}
[0m20:29:01.779654 [info ] [MainThread]: 
[0m20:29:01.781059 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:29:01.783187 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:29:01.784015 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:29:01.784719 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m20:29:01.785265 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:29:02.411184 [debug] [ThreadPool]: SQL status: OK in 0.6299999952316284 seconds
[0m20:29:02.414011 [debug] [ThreadPool]: On list_workspace: Close
[0m20:29:02.528128 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m20:29:02.529718 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m20:29:02.546482 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:02.547301 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m20:29:02.547918 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m20:29:02.548559 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:29:03.727734 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m20:29:03.729915 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m20:29:03.730651 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m20:29:03.731256 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:29:03.731907 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m20:29:03.854374 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m20:29:03.865529 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m20:29:03.866307 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m20:29:03.866912 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:29:04.910126 [debug] [ThreadPool]: SQL status: OK in 1.0399999618530273 seconds
[0m20:29:04.914407 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m20:29:05.040476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da98f69a-b741-432b-bdb5-aecd8313cb38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c163d9d0>]}
[0m20:29:05.041526 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:05.042207 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:29:05.043347 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:29:05.044260 [info ] [MainThread]: 
[0m20:29:05.060780 [debug] [Thread-4  ]: Began running node model.my_dbt_project.genre_popularity
[0m20:29:05.061839 [info ] [Thread-4  ]: 1 of 5 START sql incremental model movielens_volume.genre_popularity ........... [RUN]
[0m20:29:05.063181 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.genre_popularity)
[0m20:29:05.063935 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.genre_popularity
[0m20:29:05.082119 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.genre_popularity"
[0m20:29:05.090457 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (compile): 20:29:05.064487 => 20:29:05.090153
[0m20:29:05.091234 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.genre_popularity
[0m20:29:05.135800 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:05.136542 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m20:29:05.137143 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m20:29:05.137741 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m20:29:06.666120 [debug] [Thread-4  ]: SQL status: OK in 1.5299999713897705 seconds
[0m20:29:06.696543 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m20:29:06.697683 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

  
    create temporary view `genre_popularity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings with Genres
-- Join silver ratings with silver movies to attach genres
-- Prepare rating timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        r.rating,
        r.rating_date as rating_timestamp,  
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Aggregate Genre Metrics
-- For each genre, calculate total ratings, avg rating,
-- and distribution insights
-- =======================================================
genre_stats as (
    select
        genres,  -- genres already formatted with spacing
        count(*) as rating_count,
        round(avg(rating), 2) as avg_rating,  
        to_date(min(rating_timestamp)) as first_rating_date,  
        to_date(max(rating_timestamp)) as last_rating_date   
    from ratings_base
    group by genres
),

-- =======================================================
-- Step 3: Ranking Genres
-- Rank genres by popularity (# of ratings)
-- and quality (avg rating)
-- =======================================================
ranked_genres as (
    select
        genres,
        rating_count,
        avg_rating,
        first_rating_date,
        last_rating_date,
        rank() over (order by rating_count desc) as popularity_rank,
        rank() over (order by avg_rating desc) as quality_rank
    from genre_stats
)

-- =======================================================
-- Step 4: Final Selection
-- One row per genre with all metrics
-- =======================================================
select
    genres,
    rating_count,
    avg_rating,
    popularity_rank,
    quality_rank,
    first_rating_date,
    last_rating_date
from ranked_genres


  -- Only insert new genres on incremental runs
  where genres not in (select genres from `workspace`.`movielens_volume`.`genre_popularity`)

  
[0m20:29:07.510171 [debug] [Thread-4  ]: SQL status: OK in 0.8100000023841858 seconds
[0m20:29:07.538421 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m20:29:07.539545 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m20:29:07.860777 [debug] [Thread-4  ]: SQL status: OK in 0.3199999928474426 seconds
[0m20:29:07.873222 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.genre_popularity"
[0m20:29:07.882467 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m20:29:07.883185 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`genre_popularity` as DBT_INTERNAL_DEST
      using `genre_popularity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.genres = DBT_INTERNAL_DEST.genres
          

      when matched then update set
         * 

      when not matched then insert *

[0m20:29:17.938564 [debug] [Thread-4  ]: SQL status: OK in 10.050000190734863 seconds
[0m20:29:18.114597 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (execute): 20:29:05.091776 => 20:29:18.114284
[0m20:29:18.115619 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: ROLLBACK
[0m20:29:18.116685 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m20:29:18.117634 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: Close
[0m20:29:18.227777 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da98f69a-b741-432b-bdb5-aecd8313cb38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c02f8fd0>]}
[0m20:29:18.229077 [info ] [Thread-4  ]: 1 of 5 OK created sql incremental model movielens_volume.genre_popularity ...... [[32mOK[0m in 13.16s]
[0m20:29:18.230284 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.genre_popularity
[0m20:29:18.230994 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_enriched
[0m20:29:18.231778 [info ] [Thread-4  ]: 2 of 5 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m20:29:18.232993 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.genre_popularity, now model.my_dbt_project.movie_enriched)
[0m20:29:18.233694 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_enriched
[0m20:29:18.239251 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m20:29:18.247933 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (compile): 20:29:18.234169 => 20:29:18.247558
[0m20:29:18.248713 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_enriched
[0m20:29:18.265082 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:18.265840 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m20:29:18.266501 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

      describe extended `workspace`.`movielens_volume`.`movie_enriched`
  
[0m20:29:18.267226 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m20:29:19.573807 [debug] [Thread-4  ]: SQL status: OK in 1.309999942779541 seconds
[0m20:29:19.625055 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m20:29:19.634204 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m20:29:19.635148 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m20:29:23.191487 [debug] [Thread-4  ]: SQL status: OK in 3.559999942779541 seconds
[0m20:29:23.201719 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (execute): 20:29:18.249153 => 20:29:23.201403
[0m20:29:23.202736 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: ROLLBACK
[0m20:29:23.203413 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m20:29:23.203990 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: Close
[0m20:29:23.313469 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da98f69a-b741-432b-bdb5-aecd8313cb38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c02b7b20>]}
[0m20:29:23.314764 [info ] [Thread-4  ]: 2 of 5 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 5.08s]
[0m20:29:23.316139 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_enriched
[0m20:29:23.316910 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_performance
[0m20:29:23.317876 [info ] [Thread-4  ]: 3 of 5 START sql incremental model movielens_volume.movie_performance .......... [RUN]
[0m20:29:23.319328 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_enriched, now model.my_dbt_project.movie_performance)
[0m20:29:23.320069 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_performance
[0m20:29:23.327995 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_performance"
[0m20:29:23.340351 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (compile): 20:29:23.320690 => 20:29:23.339474
[0m20:29:23.341935 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_performance
[0m20:29:23.354755 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:23.355574 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m20:29:23.356240 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m20:29:23.356892 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m20:29:24.894599 [debug] [Thread-4  ]: SQL status: OK in 1.5399999618530273 seconds
[0m20:29:24.902016 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m20:29:24.903392 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

  
    create temporary view `movie_performance__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings Join
-- Attach movie metadata (title, genres) to ratings
-- =======================================================
with ratings_base as (
    select
        r.movie_id,
        m.title,
        regexp_replace(m.genres, '\\|', ' | ') as genres,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_date
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Weighted Rating
-- Compute avg rating, rating count, weighted avg
-- =======================================================
weighted_rating as (
    select
        movie_id,
        title,
        genres,
        round(avg(rating), 2) as avg_rating,
        count(*) as rating_count,
        round(sum(rating) / count(*), 2) as weighted_avg_rating
    from ratings_base
    group by movie_id, title, genres
),

-- =======================================================
-- Step 3: Polarization Score
-- Standard deviation of ratings per movie
-- =======================================================
polarization as (
    select
        movie_id,
        round(coalesce(stddev(rating), 0.0), 2) as polarization_score
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 4: Top Rating Percentage
-- Percent of ratings >= 4.5
-- =======================================================
top_rating_pct as (
    select
        movie_id,
        round(100.0 * sum(case when rating >= 4.5 then 1 else 0 end) / count(*), 2) as top_rating_percentage
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 5: Genre Ranking
-- Rank movies within each genre by weighted_avg_rating
-- =======================================================
genre_rank as (
    select
        movie_id,
        genres,
        weighted_avg_rating,
        rank() over (partition by genres order by weighted_avg_rating desc) as genre_rank
    from weighted_rating
),

-- =======================================================
-- Step 6: Tag Count
-- Count distinct tags per movie
-- =======================================================
tag_count as (
    select
        movie_id,
        count(distinct tag) as tag_count
    from `workspace`.`movielens_volume`.`silver_tags`
    group by movie_id
)

-- =======================================================
-- Step 7: Final Selection
-- Combine all metrics into the gold table
-- =======================================================
select
    w.movie_id,
    w.title,
    w.genres,
    w.weighted_avg_rating,
    w.rating_count,
    g.genre_rank,
    p.polarization_score,
    tr.top_rating_percentage,
    coalesce(tc.tag_count, 0) as tag_count
from weighted_rating w
join genre_rank g
    on w.movie_id = g.movie_id
join polarization p
    on w.movie_id = p.movie_id
join top_rating_pct tr
    on w.movie_id = tr.movie_id
left join tag_count tc
    on w.movie_id = tc.movie_id


-- Use NOT EXISTS for safer incremental inserts
where not exists (
    select 1 
    from `workspace`.`movielens_volume`.`movie_performance` t
    where t.movie_id = w.movie_id
)

  
[0m20:29:25.826788 [debug] [Thread-4  ]: SQL status: OK in 0.9200000166893005 seconds
[0m20:29:25.832256 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m20:29:25.833004 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m20:29:26.264811 [debug] [Thread-4  ]: SQL status: OK in 0.4300000071525574 seconds
[0m20:29:26.269395 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_performance"
[0m20:29:26.278328 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m20:29:26.279161 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`movie_performance` as DBT_INTERNAL_DEST
      using `movie_performance__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.movie_id = DBT_INTERNAL_DEST.movie_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m20:29:33.437539 [debug] [Thread-4  ]: SQL status: OK in 7.159999847412109 seconds
[0m20:29:33.566727 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (execute): 20:29:23.342581 => 20:29:33.566434
[0m20:29:33.567792 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: ROLLBACK
[0m20:29:33.568873 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m20:29:33.569554 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: Close
[0m20:29:33.687334 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da98f69a-b741-432b-bdb5-aecd8313cb38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c02b7f10>]}
[0m20:29:33.688632 [info ] [Thread-4  ]: 3 of 5 OK created sql incremental model movielens_volume.movie_performance ..... [[32mOK[0m in 10.37s]
[0m20:29:33.690129 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_performance
[0m20:29:33.691044 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_activity
[0m20:29:33.692096 [info ] [Thread-4  ]: 4 of 5 START sql incremental model movielens_volume.user_activity .............. [RUN]
[0m20:29:33.693527 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_performance, now model.my_dbt_project.user_activity)
[0m20:29:33.694399 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_activity
[0m20:29:33.702250 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_activity"
[0m20:29:33.711592 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (compile): 20:29:33.695017 => 20:29:33.711086
[0m20:29:33.712405 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_activity
[0m20:29:33.719536 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:33.720304 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m20:29:33.721006 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m20:29:33.721654 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m20:29:34.835298 [debug] [Thread-4  ]: SQL status: OK in 1.1100000143051147 seconds
[0m20:29:34.841273 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m20:29:34.842357 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

  
    create temporary view `user_activity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Join ratings with movies to attach genres and prepare timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_timestamp,
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: User Activity (General)
-- Calculate total ratings, average rating, first and last activity dates per user
-- =======================================================
user_activity as (
    select
        user_id,
        count(*) as user_rating_count,
        round(avg(rating), 2) as avg_user_rating,
        min(rating_timestamp) as first_activity_date,
        max(rating_timestamp) as last_activity_date
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 3: User Influence Score
-- Compute influence based on total ratings (more ratings = higher influence)
-- =======================================================
user_influence as (
    select
        user_id,
        user_rating_count as influence_score
    from user_activity
),

-- =======================================================
-- Step 4: Active Users by Genre
-- Count ratings per genre and rank genres per user
-- =======================================================
user_genre_activity as (
    select
        user_id,
        regexp_replace(genres, '\\|', ' | ') as genres,
        count(*) as genre_rating_count,
        rank() over (partition by user_id order by count(*) desc) as genre_rank
    from ratings_base
    group by user_id, genres
),

-- =======================================================
-- Step 5: New vs Loyal Users
-- Categorize users as new if first activity < 30 days ago, else loyal
-- =======================================================
user_type as (
    select
        user_id,
        case
            when min(rating_timestamp) >= date_sub(current_date(), 30)
                then 'new_user'
            else 'loyal_user'
        end as user_category
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 6: Top Genre per User
-- Identify the most active genre for each user
-- =======================================================
top_genre as (
    select
        user_id,
        max(genres) as top_genre
    from user_genre_activity
    group by user_id
)

-- =======================================================
-- Final Selection
-- Combine general activity, influence, type, and top genre per user
-- =======================================================
select
    ua.user_id,
    ua.user_rating_count,
    ua.avg_user_rating,
    ua.first_activity_date,
    ua.last_activity_date,
    ui.influence_score,
    ut.user_category,
    tg.top_genre
from user_activity ua
join user_influence ui on ua.user_id = ui.user_id
join user_type ut on ua.user_id = ut.user_id
left join top_genre tg on ua.user_id = tg.user_id


  where ua.user_id not in (select user_id from `workspace`.`movielens_volume`.`user_activity`)

  
[0m20:29:35.402114 [debug] [Thread-4  ]: SQL status: OK in 0.5600000023841858 seconds
[0m20:29:35.411720 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m20:29:35.412755 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m20:29:35.765820 [debug] [Thread-4  ]: SQL status: OK in 0.3499999940395355 seconds
[0m20:29:35.771454 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_activity"
[0m20:29:35.781205 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m20:29:35.782050 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_activity` as DBT_INTERNAL_DEST
      using `user_activity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m20:29:44.502871 [debug] [Thread-4  ]: SQL status: OK in 8.720000267028809 seconds
[0m20:29:44.637653 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (execute): 20:29:33.712874 => 20:29:44.637251
[0m20:29:44.638600 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: ROLLBACK
[0m20:29:44.639382 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m20:29:44.640076 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: Close
[0m20:29:44.761602 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da98f69a-b741-432b-bdb5-aecd8313cb38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c17b2880>]}
[0m20:29:44.763400 [info ] [Thread-4  ]: 4 of 5 OK created sql incremental model movielens_volume.user_activity ......... [[32mOK[0m in 11.07s]
[0m20:29:44.764917 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_activity
[0m20:29:44.765883 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_preferences
[0m20:29:44.766864 [info ] [Thread-4  ]: 5 of 5 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m20:29:44.768517 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.user_activity, now model.my_dbt_project.user_preferences)
[0m20:29:44.769342 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_preferences
[0m20:29:44.779107 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m20:29:44.788815 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (compile): 20:29:44.769968 => 20:29:44.788341
[0m20:29:44.789671 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_preferences
[0m20:29:44.798042 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:44.799100 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m20:29:44.799828 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m20:29:44.800533 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m20:29:46.020807 [debug] [Thread-4  ]: SQL status: OK in 1.2200000286102295 seconds
[0m20:29:46.029530 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m20:29:46.030970 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create temporary view `user_preferences__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score,  -- influence based on total ratings
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )

  
[0m20:29:46.611077 [debug] [Thread-4  ]: SQL status: OK in 0.5799999833106995 seconds
[0m20:29:46.617804 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m20:29:46.618638 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m20:29:47.008292 [debug] [Thread-4  ]: SQL status: OK in 0.38999998569488525 seconds
[0m20:29:47.013681 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m20:29:47.023892 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m20:29:47.024718 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
      using `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m20:29:55.654563 [debug] [Thread-4  ]: SQL status: OK in 8.630000114440918 seconds
[0m20:29:55.795779 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (execute): 20:29:44.790273 => 20:29:55.795506
[0m20:29:55.796664 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: ROLLBACK
[0m20:29:55.797500 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m20:29:55.798198 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: Close
[0m20:29:55.909486 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da98f69a-b741-432b-bdb5-aecd8313cb38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c02e9850>]}
[0m20:29:55.910721 [info ] [Thread-4  ]: 5 of 5 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 11.14s]
[0m20:29:55.912058 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_preferences
[0m20:29:55.914887 [debug] [MainThread]: On master: ROLLBACK
[0m20:29:55.915523 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:29:56.293667 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:29:56.294667 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:56.295486 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:29:56.296173 [debug] [MainThread]: On master: ROLLBACK
[0m20:29:56.296971 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:29:56.297657 [debug] [MainThread]: On master: Close
[0m20:29:56.419813 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:29:56.420613 [debug] [MainThread]: Connection 'model.my_dbt_project.user_preferences' was properly closed.
[0m20:29:56.423326 [info ] [MainThread]: 
[0m20:29:56.425180 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 54.64 seconds (54.64s).
[0m20:29:56.427328 [debug] [MainThread]: Command end result
[0m20:29:56.459386 [info ] [MainThread]: 
[0m20:29:56.460483 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:29:56.461243 [info ] [MainThread]: 
[0m20:29:56.462037 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m20:29:56.463221 [debug] [MainThread]: Command `dbt run` succeeded at 20:29:56.463051 after 58.05 seconds
[0m20:29:56.463950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8ee486670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c14bc910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8c177fa60>]}
[0m20:29:56.464652 [debug] [MainThread]: Flushing usage events
[0m20:30:00.232055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a5b2435b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a597af1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a597af910>]}


============================== 20:30:00.242786 | 2b254fd3-dce0-4f06-aaed-087340b148ce ==============================
[0m20:30:00.242786 [info ] [MainThread]: Running with dbt=1.5.2
[0m20:30:00.244051 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m20:30:01.327595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2b254fd3-dce0-4f06-aaed-087340b148ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a597af160>]}
[0m20:30:01.361390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2b254fd3-dce0-4f06-aaed-087340b148ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a2e4d7c40>]}
[0m20:30:01.362500 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m20:30:01.409694 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m20:30:02.694908 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:30:02.695661 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:30:02.709026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2b254fd3-dce0-4f06-aaed-087340b148ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a2e48d400>]}
[0m20:30:02.743740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2b254fd3-dce0-4f06-aaed-087340b148ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a2e2b3430>]}
[0m20:30:02.744664 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m20:30:02.745509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2b254fd3-dce0-4f06-aaed-087340b148ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a2e2b3490>]}
[0m20:30:02.749517 [info ] [MainThread]: 
[0m20:30:02.751082 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:30:02.753585 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m20:30:02.760192 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m20:30:02.760902 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m20:30:02.761531 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:30:03.442222 [debug] [ThreadPool]: SQL status: OK in 0.6800000071525574 seconds
[0m20:30:03.445725 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m20:30:03.571034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2b254fd3-dce0-4f06-aaed-087340b148ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a2e2b3310>]}
[0m20:30:03.572235 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:03.572864 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:30:03.574071 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:30:03.575047 [info ] [MainThread]: 
[0m20:30:03.592506 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m20:30:03.593641 [info ] [Thread-2  ]: 1 of 22 START test accepted_rating_range_silver_ratings_rating__5__0 ........... [RUN]
[0m20:30:03.595060 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff)
[0m20:30:03.595713 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m20:30:03.609198 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m20:30:03.622090 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (compile): 20:30:03.596276 => 20:30:03.621582
[0m20:30:03.623019 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m20:30:03.658602 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m20:30:03.671749 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:03.672522 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m20:30:03.673268 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`silver_ratings`
where rating < 0
   or rating > 5


      
    ) dbt_internal_test
[0m20:30:03.674063 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:05.161568 [debug] [Thread-2  ]: SQL status: OK in 1.4900000095367432 seconds
[0m20:30:05.170796 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (execute): 20:30:03.623632 => 20:30:05.170403
[0m20:30:05.171804 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: ROLLBACK
[0m20:30:05.172561 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:05.173269 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: Close
[0m20:30:05.290483 [info ] [Thread-2  ]: 1 of 22 PASS accepted_rating_range_silver_ratings_rating__5__0 ................. [[32mPASS[0m in 1.70s]
[0m20:30:05.291847 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m20:30:05.292722 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m20:30:05.293691 [info ] [Thread-2  ]: 2 of 22 START test not_null_silver_links_imdb_id ............................... [RUN]
[0m20:30:05.295028 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff, now test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51)
[0m20:30:05.295835 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m20:30:05.306570 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m20:30:05.316558 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (compile): 20:30:05.296559 => 20:30:05.316058
[0m20:30:05.317374 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m20:30:05.321701 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m20:30:05.331474 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:05.332143 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m20:30:05.332734 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



      
    ) dbt_internal_test
[0m20:30:05.333345 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:06.280173 [debug] [Thread-2  ]: SQL status: OK in 0.949999988079071 seconds
[0m20:30:06.284545 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (execute): 20:30:05.317891 => 20:30:06.284178
[0m20:30:06.285429 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: ROLLBACK
[0m20:30:06.286180 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:06.286930 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m20:30:06.395782 [info ] [Thread-2  ]: 2 of 22 PASS not_null_silver_links_imdb_id ..................................... [[32mPASS[0m in 1.10s]
[0m20:30:06.397065 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m20:30:06.397795 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m20:30:06.398452 [info ] [Thread-2  ]: 3 of 22 START test not_null_silver_links_movie_id .............................. [RUN]
[0m20:30:06.399629 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51, now test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d)
[0m20:30:06.400226 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m20:30:06.411229 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m20:30:06.423178 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (compile): 20:30:06.400732 => 20:30:06.422364
[0m20:30:06.424352 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m20:30:06.429039 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m20:30:06.439991 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:06.440823 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m20:30:06.441460 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



      
    ) dbt_internal_test
[0m20:30:06.442125 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:07.301334 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m20:30:07.305815 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (execute): 20:30:06.425029 => 20:30:07.305461
[0m20:30:07.306641 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: ROLLBACK
[0m20:30:07.307323 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:07.307967 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m20:30:07.426772 [info ] [Thread-2  ]: 3 of 22 PASS not_null_silver_links_movie_id .................................... [[32mPASS[0m in 1.03s]
[0m20:30:07.428229 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m20:30:07.429151 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m20:30:07.430050 [info ] [Thread-2  ]: 4 of 22 START test not_null_silver_links_tmdb_id ............................... [RUN]
[0m20:30:07.431623 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d, now test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f)
[0m20:30:07.432377 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m20:30:07.445815 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m20:30:07.456209 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (compile): 20:30:07.432976 => 20:30:07.455723
[0m20:30:07.457027 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m20:30:07.461341 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m20:30:07.471341 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:07.472020 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m20:30:07.472754 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



      
    ) dbt_internal_test
[0m20:30:07.473568 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:08.351619 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m20:30:08.356001 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (execute): 20:30:07.457489 => 20:30:08.355575
[0m20:30:08.356796 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: ROLLBACK
[0m20:30:08.357487 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:08.358128 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m20:30:08.468450 [info ] [Thread-2  ]: 4 of 22 PASS not_null_silver_links_tmdb_id ..................................... [[32mPASS[0m in 1.04s]
[0m20:30:08.469844 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m20:30:08.470700 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m20:30:08.471506 [info ] [Thread-2  ]: 5 of 22 START test not_null_silver_movies_genres ............................... [RUN]
[0m20:30:08.472899 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f, now test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63)
[0m20:30:08.473810 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m20:30:08.483072 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m20:30:08.492887 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (compile): 20:30:08.474350 => 20:30:08.492420
[0m20:30:08.493627 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m20:30:08.498099 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m20:30:08.507990 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:08.508649 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m20:30:08.509306 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



      
    ) dbt_internal_test
[0m20:30:08.510098 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:09.426781 [debug] [Thread-2  ]: SQL status: OK in 0.9200000166893005 seconds
[0m20:30:09.431343 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (execute): 20:30:08.494142 => 20:30:09.430961
[0m20:30:09.432156 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: ROLLBACK
[0m20:30:09.432883 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:09.433566 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m20:30:09.543634 [info ] [Thread-2  ]: 5 of 22 PASS not_null_silver_movies_genres ..................................... [[32mPASS[0m in 1.07s]
[0m20:30:09.545364 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m20:30:09.546329 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m20:30:09.547095 [info ] [Thread-2  ]: 6 of 22 START test not_null_silver_movies_movie_id ............................. [RUN]
[0m20:30:09.548367 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63, now test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e)
[0m20:30:09.549158 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m20:30:09.557795 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m20:30:09.566970 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (compile): 20:30:09.549775 => 20:30:09.566208
[0m20:30:09.568030 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m20:30:09.572725 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m20:30:09.582765 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:09.583697 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m20:30:09.584511 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



      
    ) dbt_internal_test
[0m20:30:09.585382 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:10.458256 [debug] [Thread-2  ]: SQL status: OK in 0.8700000047683716 seconds
[0m20:30:10.462596 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (execute): 20:30:09.568637 => 20:30:10.462223
[0m20:30:10.463493 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: ROLLBACK
[0m20:30:10.464157 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:10.464874 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m20:30:10.580222 [info ] [Thread-2  ]: 6 of 22 PASS not_null_silver_movies_movie_id ................................... [[32mPASS[0m in 1.03s]
[0m20:30:10.581782 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m20:30:10.582779 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m20:30:10.583816 [info ] [Thread-2  ]: 7 of 22 START test not_null_silver_movies_title ................................ [RUN]
[0m20:30:10.585238 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e, now test.my_dbt_project.not_null_silver_movies_title.3a779bd2de)
[0m20:30:10.585873 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m20:30:10.599477 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m20:30:10.608262 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (compile): 20:30:10.586421 => 20:30:10.607831
[0m20:30:10.609038 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m20:30:10.613700 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m20:30:10.622009 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:10.622704 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m20:30:10.623257 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



      
    ) dbt_internal_test
[0m20:30:10.623925 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:11.848114 [debug] [Thread-2  ]: SQL status: OK in 1.2200000286102295 seconds
[0m20:30:11.852780 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (execute): 20:30:10.609534 => 20:30:11.852409
[0m20:30:11.853592 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: ROLLBACK
[0m20:30:11.854480 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:11.855281 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m20:30:11.979327 [info ] [Thread-2  ]: 7 of 22 PASS not_null_silver_movies_title ...................................... [[32mPASS[0m in 1.39s]
[0m20:30:11.980830 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m20:30:11.981811 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m20:30:11.982675 [info ] [Thread-2  ]: 8 of 22 START test not_null_silver_ratings_movie_id ............................ [RUN]
[0m20:30:11.984147 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_title.3a779bd2de, now test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36)
[0m20:30:11.985025 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m20:30:11.991691 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m20:30:12.002111 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (compile): 20:30:11.985573 => 20:30:12.001621
[0m20:30:12.002891 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m20:30:12.007170 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m20:30:12.017251 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:12.017895 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m20:30:12.018441 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



      
    ) dbt_internal_test
[0m20:30:12.019132 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:12.918562 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m20:30:12.923812 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (execute): 20:30:12.003366 => 20:30:12.923422
[0m20:30:12.924687 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: ROLLBACK
[0m20:30:12.925380 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:12.926079 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m20:30:13.038641 [info ] [Thread-2  ]: 8 of 22 PASS not_null_silver_ratings_movie_id .................................. [[32mPASS[0m in 1.05s]
[0m20:30:13.039913 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m20:30:13.040707 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m20:30:13.041532 [info ] [Thread-2  ]: 9 of 22 START test not_null_silver_ratings_rating .............................. [RUN]
[0m20:30:13.042845 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36, now test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a)
[0m20:30:13.043600 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m20:30:13.049966 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m20:30:13.059915 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (compile): 20:30:13.044151 => 20:30:13.059483
[0m20:30:13.060790 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m20:30:13.065225 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m20:30:13.075231 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:13.075910 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m20:30:13.076854 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



      
    ) dbt_internal_test
[0m20:30:13.077595 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:14.004385 [debug] [Thread-2  ]: SQL status: OK in 0.9300000071525574 seconds
[0m20:30:14.008999 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (execute): 20:30:13.061353 => 20:30:14.008623
[0m20:30:14.009959 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: ROLLBACK
[0m20:30:14.010680 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:14.011384 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m20:30:14.126730 [info ] [Thread-2  ]: 9 of 22 PASS not_null_silver_ratings_rating .................................... [[32mPASS[0m in 1.08s]
[0m20:30:14.128244 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m20:30:14.129291 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m20:30:14.130082 [info ] [Thread-2  ]: 10 of 22 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m20:30:14.131545 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a, now test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f)
[0m20:30:14.132388 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m20:30:14.148255 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m20:30:14.159151 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (compile): 20:30:14.132995 => 20:30:14.158652
[0m20:30:14.159925 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m20:30:14.164322 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m20:30:14.174513 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:14.175203 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m20:30:14.175803 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



      
    ) dbt_internal_test
[0m20:30:14.176489 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:15.093694 [debug] [Thread-2  ]: SQL status: OK in 0.9200000166893005 seconds
[0m20:30:15.097936 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (execute): 20:30:14.160439 => 20:30:15.097591
[0m20:30:15.098742 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: ROLLBACK
[0m20:30:15.099487 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:15.100209 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m20:30:15.218497 [info ] [Thread-2  ]: 10 of 22 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 1.09s]
[0m20:30:15.219966 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m20:30:15.220812 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m20:30:15.221800 [info ] [Thread-2  ]: 11 of 22 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m20:30:15.223140 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f, now test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3)
[0m20:30:15.223965 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m20:30:15.233533 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m20:30:15.244660 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (compile): 20:30:15.224581 => 20:30:15.244184
[0m20:30:15.245460 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m20:30:15.249708 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m20:30:15.259594 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:15.260376 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m20:30:15.261027 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



      
    ) dbt_internal_test
[0m20:30:15.261771 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:16.230921 [debug] [Thread-2  ]: SQL status: OK in 0.9700000286102295 seconds
[0m20:30:16.235292 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (execute): 20:30:15.245947 => 20:30:16.234949
[0m20:30:16.236100 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: ROLLBACK
[0m20:30:16.236885 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:16.237695 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m20:30:16.346549 [info ] [Thread-2  ]: 11 of 22 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 1.12s]
[0m20:30:16.348012 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m20:30:16.348947 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m20:30:16.349875 [info ] [Thread-2  ]: 12 of 22 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m20:30:16.351170 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3, now test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3)
[0m20:30:16.351911 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m20:30:16.361943 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m20:30:16.372642 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (compile): 20:30:16.352643 => 20:30:16.372073
[0m20:30:16.373572 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m20:30:16.377754 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m20:30:16.387475 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:16.388233 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m20:30:16.388835 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



      
    ) dbt_internal_test
[0m20:30:16.389506 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:17.303656 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m20:30:17.307850 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (execute): 20:30:16.374076 => 20:30:17.307503
[0m20:30:17.308545 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: ROLLBACK
[0m20:30:17.309249 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:17.310006 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m20:30:17.426101 [info ] [Thread-2  ]: 12 of 22 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 1.08s]
[0m20:30:17.427664 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m20:30:17.428714 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m20:30:17.429492 [info ] [Thread-2  ]: 13 of 22 START test not_null_silver_tags_tag ................................... [RUN]
[0m20:30:17.431106 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3, now test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8)
[0m20:30:17.431981 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m20:30:17.442458 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m20:30:17.453073 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (compile): 20:30:17.432573 => 20:30:17.452590
[0m20:30:17.453846 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m20:30:17.457958 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m20:30:17.467538 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:17.468317 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m20:30:17.468873 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



      
    ) dbt_internal_test
[0m20:30:17.469551 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:18.279724 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m20:30:18.284255 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (execute): 20:30:17.454313 => 20:30:18.283870
[0m20:30:18.285190 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: ROLLBACK
[0m20:30:18.286013 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:18.286684 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m20:30:18.414378 [info ] [Thread-2  ]: 13 of 22 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 0.98s]
[0m20:30:18.415809 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m20:30:18.416741 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m20:30:18.417650 [info ] [Thread-2  ]: 14 of 22 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m20:30:18.419140 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8, now test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86)
[0m20:30:18.419922 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m20:30:18.426842 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m20:30:18.436981 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (compile): 20:30:18.420496 => 20:30:18.436541
[0m20:30:18.437741 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m20:30:18.441917 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m20:30:18.451841 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:18.452683 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m20:30:18.453453 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



      
    ) dbt_internal_test
[0m20:30:18.454162 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:19.279178 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m20:30:19.283725 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (execute): 20:30:18.438280 => 20:30:19.283346
[0m20:30:19.284656 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: ROLLBACK
[0m20:30:19.285578 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:19.286415 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m20:30:19.401954 [info ] [Thread-2  ]: 14 of 22 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 0.98s]
[0m20:30:19.403344 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m20:30:19.404297 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m20:30:19.405102 [info ] [Thread-2  ]: 15 of 22 START test not_null_silver_tags_user_id ............................... [RUN]
[0m20:30:19.406915 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86, now test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3)
[0m20:30:19.407859 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m20:30:19.414663 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m20:30:19.425618 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (compile): 20:30:19.408405 => 20:30:19.425142
[0m20:30:19.426459 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m20:30:19.430568 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m20:30:19.440703 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:19.441430 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m20:30:19.442073 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



      
    ) dbt_internal_test
[0m20:30:19.442702 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:20.281769 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m20:30:20.286166 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (execute): 20:30:19.426939 => 20:30:20.285765
[0m20:30:20.287065 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: ROLLBACK
[0m20:30:20.287984 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:20.288711 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m20:30:20.405853 [info ] [Thread-2  ]: 15 of 22 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 1.00s]
[0m20:30:20.407218 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m20:30:20.407984 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m20:30:20.408742 [info ] [Thread-2  ]: 16 of 22 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m20:30:20.409992 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m20:30:20.410624 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m20:30:20.434643 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m20:30:20.446977 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 20:30:20.411233 => 20:30:20.446325
[0m20:30:20.448031 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m20:30:20.452283 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m20:30:20.464101 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:20.464864 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m20:30:20.465473 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m20:30:20.466180 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:22.796291 [debug] [Thread-2  ]: SQL status: OK in 2.3299999237060547 seconds
[0m20:30:22.800997 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 20:30:20.448611 => 20:30:22.800609
[0m20:30:22.801860 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m20:30:22.802809 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:22.803533 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m20:30:22.918467 [info ] [Thread-2  ]: 16 of 22 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 2.51s]
[0m20:30:22.920241 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m20:30:22.921283 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m20:30:22.922272 [info ] [Thread-2  ]: 17 of 22 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m20:30:22.923804 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m20:30:22.924518 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m20:30:22.933134 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m20:30:22.943430 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 20:30:22.925111 => 20:30:22.942978
[0m20:30:22.944257 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m20:30:22.948424 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m20:30:22.958336 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:22.959022 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m20:30:22.959618 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m20:30:22.960247 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:24.188259 [debug] [Thread-2  ]: SQL status: OK in 1.2300000190734863 seconds
[0m20:30:24.192595 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 20:30:22.944822 => 20:30:24.192211
[0m20:30:24.193475 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m20:30:24.194260 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:24.194889 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m20:30:24.311395 [info ] [Thread-2  ]: 17 of 22 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.39s]
[0m20:30:24.312718 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m20:30:24.313792 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m20:30:24.314793 [info ] [Thread-2  ]: 18 of 22 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m20:30:24.316355 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32)
[0m20:30:24.317225 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m20:30:24.326569 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m20:30:24.336891 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (compile): 20:30:24.317838 => 20:30:24.336253
[0m20:30:24.337677 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m20:30:24.341584 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m20:30:24.351709 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:24.352595 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m20:30:24.353285 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m20:30:24.353988 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:26.189290 [debug] [Thread-2  ]: SQL status: OK in 1.840000033378601 seconds
[0m20:30:26.193700 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (execute): 20:30:24.338161 => 20:30:26.193251
[0m20:30:26.194643 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: ROLLBACK
[0m20:30:26.195513 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:26.196209 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m20:30:26.308971 [info ] [Thread-2  ]: 18 of 22 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.99s]
[0m20:30:26.310418 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m20:30:26.311297 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m20:30:26.312215 [info ] [Thread-2  ]: 19 of 22 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m20:30:26.313527 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32, now test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd)
[0m20:30:26.314236 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m20:30:25.985880 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m20:30:25.997523 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (compile): 20:30:26.314792 => 20:30:25.996978
[0m20:30:25.998313 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m20:30:26.002354 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m20:30:26.012269 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:26.013015 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m20:30:26.013646 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m20:30:26.014312 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:27.168927 [debug] [Thread-2  ]: SQL status: OK in 1.149999976158142 seconds
[0m20:30:27.174193 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (execute): 20:30:25.998776 => 20:30:27.173769
[0m20:30:27.175052 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: ROLLBACK
[0m20:30:27.175752 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:27.176566 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m20:30:27.298663 [info ] [Thread-2  ]: 19 of 22 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.99s]
[0m20:30:27.300108 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m20:30:27.301038 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m20:30:27.301984 [info ] [Thread-2  ]: 20 of 22 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m20:30:27.303553 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd, now test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719)
[0m20:30:27.304351 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m20:30:27.313254 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m20:30:27.323459 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (compile): 20:30:27.304889 => 20:30:27.322912
[0m20:30:27.324202 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m20:30:27.329155 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m20:30:27.340719 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:27.341562 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m20:30:27.342288 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m20:30:27.343009 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:29.252759 [debug] [Thread-2  ]: SQL status: OK in 1.909999966621399 seconds
[0m20:30:29.257299 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (execute): 20:30:27.324712 => 20:30:29.256801
[0m20:30:29.258264 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: ROLLBACK
[0m20:30:29.259180 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:29.259898 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m20:30:29.370375 [info ] [Thread-2  ]: 20 of 22 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 2.07s]
[0m20:30:29.371854 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m20:30:29.372855 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m20:30:29.373853 [info ] [Thread-2  ]: 21 of 22 START test unique_silver_links_movie_id ............................... [RUN]
[0m20:30:29.375144 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719, now test.my_dbt_project.unique_silver_links_movie_id.c422609054)
[0m20:30:29.375854 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m20:30:29.386559 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m20:30:29.396861 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (compile): 20:30:29.376365 => 20:30:29.396290
[0m20:30:29.397723 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m20:30:29.401825 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m20:30:29.412171 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:29.412935 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m20:30:29.413699 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:30:29.414441 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:30.533985 [debug] [Thread-2  ]: SQL status: OK in 1.1200000047683716 seconds
[0m20:30:30.538310 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (execute): 20:30:29.398204 => 20:30:30.537933
[0m20:30:30.539240 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: ROLLBACK
[0m20:30:30.540089 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:30.540848 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m20:30:30.658357 [info ] [Thread-2  ]: 21 of 22 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 1.28s]
[0m20:30:30.659790 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m20:30:30.660663 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m20:30:30.661436 [info ] [Thread-2  ]: 22 of 22 START test unique_silver_movies_movie_id .............................. [RUN]
[0m20:30:30.662830 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_silver_links_movie_id.c422609054, now test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085)
[0m20:30:30.663595 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m20:30:30.670270 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m20:30:30.680358 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (compile): 20:30:30.664110 => 20:30:30.679912
[0m20:30:30.681268 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m20:30:30.694587 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m20:30:30.704187 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:30.704905 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m20:30:30.705604 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:30:30.706255 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:31.840308 [debug] [Thread-2  ]: SQL status: OK in 1.1299999952316284 seconds
[0m20:30:31.844878 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (execute): 20:30:30.681802 => 20:30:31.844501
[0m20:30:31.845737 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: ROLLBACK
[0m20:30:31.846437 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:31.847076 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m20:30:31.965329 [info ] [Thread-2  ]: 22 of 22 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 1.30s]
[0m20:30:31.967082 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m20:30:31.969876 [debug] [MainThread]: On master: ROLLBACK
[0m20:30:31.970543 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:30:32.339538 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:30:32.340469 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:32.341195 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:30:32.341848 [debug] [MainThread]: On master: ROLLBACK
[0m20:30:32.342585 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:30:32.343267 [debug] [MainThread]: On master: Close
[0m20:30:32.461152 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:30:32.462081 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085' was properly closed.
[0m20:30:32.464823 [info ] [MainThread]: 
[0m20:30:32.466700 [info ] [MainThread]: Finished running 22 tests in 0 hours 0 minutes and 29.71 seconds (29.71s).
[0m20:30:32.473165 [debug] [MainThread]: Command end result
[0m20:30:32.506084 [info ] [MainThread]: 
[0m20:30:32.507127 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:30:32.508044 [info ] [MainThread]: 
[0m20:30:32.509018 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=0 SKIP=0 TOTAL=22
[0m20:30:32.510463 [debug] [MainThread]: Command `dbt test` succeeded at 20:30:32.510299 after 33.17 seconds
[0m20:30:32.511173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a5b2435b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a2e4d7c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a2e1d6eb0>]}
[0m20:30:32.511840 [debug] [MainThread]: Flushing usage events
[0m20:30:36.803998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcd41035b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcd266f220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcd266f940>]}


============================== 20:30:36.811959 | 3238187b-8d12-4687-862c-5840b7673d1d ==============================
[0m20:30:36.811959 [info ] [MainThread]: Running with dbt=1.5.2
[0m20:30:36.813403 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m20:30:37.819961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3238187b-8d12-4687-862c-5840b7673d1d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcd266f1c0>]}
[0m20:30:37.850752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3238187b-8d12-4687-862c-5840b7673d1d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efca73dac40>]}
[0m20:30:37.851869 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m20:30:37.898691 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m20:30:39.181614 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:30:39.182379 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:30:39.196015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3238187b-8d12-4687-862c-5840b7673d1d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efca7391400>]}
[0m20:30:39.234183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3238187b-8d12-4687-862c-5840b7673d1d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efca71b6430>]}
[0m20:30:39.235336 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m20:30:39.236459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3238187b-8d12-4687-862c-5840b7673d1d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efca71b6490>]}
[0m20:30:39.240895 [info ] [MainThread]: 
[0m20:30:39.242598 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:30:39.245384 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m20:30:39.252184 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m20:30:39.252992 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m20:30:39.253683 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:30:39.881138 [debug] [ThreadPool]: SQL status: OK in 0.6299999952316284 seconds
[0m20:30:39.884832 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m20:30:40.002677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3238187b-8d12-4687-862c-5840b7673d1d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efca71b6310>]}
[0m20:30:40.003834 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:40.004525 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:30:40.005681 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:30:40.006568 [info ] [MainThread]: 
[0m20:30:40.024577 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m20:30:40.025533 [info ] [Thread-2  ]: 1 of 35 START test accepted_rating_range_genre_popularity_avg_rating__5__0 ..... [RUN]
[0m20:30:40.026860 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413)
[0m20:30:40.027521 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m20:30:40.042443 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m20:30:40.053050 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (compile): 20:30:40.028026 => 20:30:40.052577
[0m20:30:40.053857 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m20:30:40.101256 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m20:30:40.115430 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:40.116226 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m20:30:40.117050 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m20:30:40.118138 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:41.257256 [debug] [Thread-2  ]: SQL status: OK in 1.1399999856948853 seconds
[0m20:30:41.266658 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (execute): 20:30:40.054365 => 20:30:41.266291
[0m20:30:41.267525 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: ROLLBACK
[0m20:30:41.268440 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:41.269088 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: Close
[0m20:30:41.380450 [info ] [Thread-2  ]: 1 of 35 PASS accepted_rating_range_genre_popularity_avg_rating__5__0 ........... [[32mPASS[0m in 1.35s]
[0m20:30:41.381943 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m20:30:41.382821 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m20:30:41.383668 [info ] [Thread-2  ]: 2 of 35 START test accepted_rating_range_movie_enriched_avg_rating__5__0 ....... [RUN]
[0m20:30:41.385430 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413, now test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8)
[0m20:30:41.386179 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m20:30:41.393792 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m20:30:41.404561 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (compile): 20:30:41.386707 => 20:30:41.404054
[0m20:30:41.405311 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m20:30:41.409992 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m20:30:41.419953 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:41.420787 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m20:30:41.421499 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m20:30:41.422158 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:42.266737 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m20:30:42.271093 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (execute): 20:30:41.406018 => 20:30:42.270746
[0m20:30:42.271945 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: ROLLBACK
[0m20:30:42.272752 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:42.273713 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: Close
[0m20:30:42.395116 [info ] [Thread-2  ]: 2 of 35 PASS accepted_rating_range_movie_enriched_avg_rating__5__0 ............. [[32mPASS[0m in 1.01s]
[0m20:30:42.396919 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m20:30:42.397833 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m20:30:42.398609 [info ] [Thread-2  ]: 3 of 35 START test accepted_rating_range_movie_performance_weighted_avg_rating__5__0  [RUN]
[0m20:30:42.399875 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8, now test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083)
[0m20:30:42.400517 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m20:30:42.409255 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m20:30:42.420460 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (compile): 20:30:42.400987 => 20:30:42.419960
[0m20:30:42.421288 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m20:30:42.425774 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m20:30:42.436724 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:42.437410 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m20:30:42.438079 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating < 0
   or weighted_avg_rating > 5


      
    ) dbt_internal_test
[0m20:30:42.438873 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:43.370104 [debug] [Thread-2  ]: SQL status: OK in 0.9300000071525574 seconds
[0m20:30:43.374608 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (execute): 20:30:42.421860 => 20:30:43.374231
[0m20:30:43.375531 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: ROLLBACK
[0m20:30:43.376417 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:43.377092 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: Close
[0m20:30:43.495012 [info ] [Thread-2  ]: 3 of 35 PASS accepted_rating_range_movie_performance_weighted_avg_rating__5__0 . [[32mPASS[0m in 1.10s]
[0m20:30:43.496388 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m20:30:43.497163 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m20:30:43.497913 [info ] [Thread-2  ]: 4 of 35 START test accepted_rating_range_user_activity_avg_user_rating__5__0 ... [RUN]
[0m20:30:43.499236 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083, now test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a)
[0m20:30:43.500064 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m20:30:43.513420 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m20:30:43.523801 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (compile): 20:30:43.500571 => 20:30:43.523340
[0m20:30:43.524617 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m20:30:43.528729 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m20:30:43.539092 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:43.539879 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m20:30:43.540571 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating < 0
   or avg_user_rating > 5


      
    ) dbt_internal_test
[0m20:30:43.541239 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:44.536658 [debug] [Thread-2  ]: SQL status: OK in 1.0 seconds
[0m20:30:44.541036 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (execute): 20:30:43.525116 => 20:30:44.540609
[0m20:30:44.542051 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: ROLLBACK
[0m20:30:44.543050 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:44.543806 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: Close
[0m20:30:44.657941 [info ] [Thread-2  ]: 4 of 35 PASS accepted_rating_range_user_activity_avg_user_rating__5__0 ......... [[32mPASS[0m in 1.16s]
[0m20:30:44.659514 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m20:30:44.660346 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m20:30:44.661146 [info ] [Thread-2  ]: 5 of 35 START test not_null_genre_popularity_avg_rating ........................ [RUN]
[0m20:30:44.662659 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a, now test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9)
[0m20:30:44.663389 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m20:30:44.673738 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m20:30:44.682122 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (compile): 20:30:44.663969 => 20:30:44.681618
[0m20:30:44.682946 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m20:30:44.687658 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m20:30:44.696059 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:44.696723 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m20:30:44.697349 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating is null



      
    ) dbt_internal_test
[0m20:30:44.697984 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:45.529594 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m20:30:45.534390 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (execute): 20:30:44.683497 => 20:30:45.533994
[0m20:30:45.535284 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: ROLLBACK
[0m20:30:45.536125 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:45.536873 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: Close
[0m20:30:45.661120 [info ] [Thread-2  ]: 5 of 35 PASS not_null_genre_popularity_avg_rating .............................. [[32mPASS[0m in 1.00s]
[0m20:30:45.662469 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m20:30:45.663431 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m20:30:45.664221 [info ] [Thread-2  ]: 6 of 35 START test not_null_genre_popularity_first_rating_date ................. [RUN]
[0m20:30:45.665459 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9, now test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32)
[0m20:30:45.666270 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m20:30:45.672529 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m20:30:45.680730 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (compile): 20:30:45.666817 => 20:30:45.680081
[0m20:30:45.681491 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m20:30:45.685627 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m20:30:45.693825 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:45.694452 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m20:30:45.695064 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where first_rating_date is null



      
    ) dbt_internal_test
[0m20:30:45.695774 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:46.539895 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m20:30:46.545079 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (execute): 20:30:45.681943 => 20:30:46.544698
[0m20:30:46.545988 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: ROLLBACK
[0m20:30:46.546693 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:46.547348 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: Close
[0m20:30:46.665525 [info ] [Thread-2  ]: 6 of 35 PASS not_null_genre_popularity_first_rating_date ....................... [[32mPASS[0m in 1.00s]
[0m20:30:46.667008 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m20:30:46.668165 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m20:30:46.670956 [info ] [Thread-2  ]: 7 of 35 START test not_null_genre_popularity_genres ............................ [RUN]
[0m20:30:46.673627 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32, now test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa)
[0m20:30:46.674692 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m20:30:46.700146 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m20:30:46.710962 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (compile): 20:30:46.675375 => 20:30:46.710434
[0m20:30:46.711745 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m20:30:46.716130 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m20:30:46.726570 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:46.727334 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m20:30:46.728111 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`genre_popularity`
where genres is null



      
    ) dbt_internal_test
[0m20:30:46.729008 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:47.610518 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m20:30:47.614888 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (execute): 20:30:46.712225 => 20:30:47.614533
[0m20:30:47.615759 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: ROLLBACK
[0m20:30:47.616510 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:47.617308 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: Close
[0m20:30:47.732034 [info ] [Thread-2  ]: 7 of 35 PASS not_null_genre_popularity_genres .................................. [[32mPASS[0m in 1.06s]
[0m20:30:47.733515 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m20:30:47.734382 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m20:30:47.735121 [info ] [Thread-2  ]: 8 of 35 START test not_null_genre_popularity_last_rating_date .................. [RUN]
[0m20:30:47.736328 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa, now test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432)
[0m20:30:47.737015 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m20:30:47.746144 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m20:30:47.757301 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (compile): 20:30:47.737595 => 20:30:47.756826
[0m20:30:47.758145 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m20:30:47.764018 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m20:30:47.774441 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:47.775206 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m20:30:47.775861 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where last_rating_date is null



      
    ) dbt_internal_test
[0m20:30:47.776629 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:48.630250 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m20:30:48.634912 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (execute): 20:30:47.758670 => 20:30:48.634591
[0m20:30:48.635953 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: ROLLBACK
[0m20:30:48.636727 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:48.637423 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: Close
[0m20:30:48.752333 [info ] [Thread-2  ]: 8 of 35 PASS not_null_genre_popularity_last_rating_date ........................ [[32mPASS[0m in 1.02s]
[0m20:30:48.753834 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m20:30:48.754912 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m20:30:48.755778 [info ] [Thread-2  ]: 9 of 35 START test not_null_genre_popularity_popularity_rank ................... [RUN]
[0m20:30:48.757206 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432, now test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc)
[0m20:30:48.757993 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m20:30:48.767357 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m20:30:48.777216 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (compile): 20:30:48.758690 => 20:30:48.776782
[0m20:30:48.777937 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m20:30:48.782113 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m20:30:48.791724 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:48.792400 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m20:30:48.792976 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select popularity_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where popularity_rank is null



      
    ) dbt_internal_test
[0m20:30:48.793704 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:49.577122 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m20:30:49.582269 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (execute): 20:30:48.778390 => 20:30:49.581894
[0m20:30:49.583392 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: ROLLBACK
[0m20:30:49.584566 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:49.586041 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: Close
[0m20:30:49.698507 [info ] [Thread-2  ]: 9 of 35 PASS not_null_genre_popularity_popularity_rank ......................... [[32mPASS[0m in 0.94s]
[0m20:30:49.699985 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m20:30:49.700899 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m20:30:49.701758 [info ] [Thread-2  ]: 10 of 35 START test not_null_genre_popularity_quality_rank ..................... [RUN]
[0m20:30:49.703267 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc, now test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a)
[0m20:30:49.704024 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m20:30:49.716038 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m20:30:49.726310 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (compile): 20:30:49.704665 => 20:30:49.725828
[0m20:30:49.727099 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m20:30:49.731138 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m20:30:49.741096 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:49.741996 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m20:30:49.742705 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select quality_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where quality_rank is null



      
    ) dbt_internal_test
[0m20:30:49.743483 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:50.600308 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m20:30:50.604528 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (execute): 20:30:49.727617 => 20:30:50.604193
[0m20:30:50.605364 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: ROLLBACK
[0m20:30:50.606140 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:50.606815 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: Close
[0m20:30:50.728944 [info ] [Thread-2  ]: 10 of 35 PASS not_null_genre_popularity_quality_rank ........................... [[32mPASS[0m in 1.03s]
[0m20:30:50.730401 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m20:30:50.731328 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m20:30:50.732258 [info ] [Thread-2  ]: 11 of 35 START test not_null_genre_popularity_rating_count ..................... [RUN]
[0m20:30:50.733780 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a, now test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856)
[0m20:30:50.734789 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m20:30:50.741758 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m20:30:50.752486 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (compile): 20:30:50.735339 => 20:30:50.752007
[0m20:30:50.753267 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m20:30:50.757414 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m20:30:50.767418 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:50.768066 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m20:30:50.768604 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`genre_popularity`
where rating_count is null



      
    ) dbt_internal_test
[0m20:30:50.769264 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:51.563419 [debug] [Thread-2  ]: SQL status: OK in 0.7900000214576721 seconds
[0m20:30:51.567745 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (execute): 20:30:50.753709 => 20:30:51.567353
[0m20:30:51.568593 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: ROLLBACK
[0m20:30:51.569489 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:51.570282 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: Close
[0m20:30:51.689945 [info ] [Thread-2  ]: 11 of 35 PASS not_null_genre_popularity_rating_count ........................... [[32mPASS[0m in 0.96s]
[0m20:30:51.691569 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m20:30:51.692696 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m20:30:51.693408 [info ] [Thread-2  ]: 12 of 35 START test not_null_movie_enriched_avg_rating ......................... [RUN]
[0m20:30:51.694672 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856, now test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd)
[0m20:30:51.695355 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m20:30:51.701874 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m20:30:51.712024 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (compile): 20:30:51.695915 => 20:30:51.711605
[0m20:30:51.712772 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m20:30:51.716992 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m20:30:51.727255 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:51.728217 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m20:30:51.728866 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating is null



      
    ) dbt_internal_test
[0m20:30:51.729559 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:52.584199 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m20:30:52.589054 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (execute): 20:30:51.713341 => 20:30:52.588684
[0m20:30:52.590026 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: ROLLBACK
[0m20:30:52.591001 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:52.591722 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: Close
[0m20:30:52.707395 [info ] [Thread-2  ]: 12 of 35 PASS not_null_movie_enriched_avg_rating ............................... [[32mPASS[0m in 1.01s]
[0m20:30:52.708895 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m20:30:52.709758 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m20:30:52.710584 [info ] [Thread-2  ]: 13 of 35 START test not_null_movie_enriched_genres ............................. [RUN]
[0m20:30:52.712081 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd, now test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27)
[0m20:30:52.712876 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m20:30:52.719737 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m20:30:52.730355 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (compile): 20:30:52.713662 => 20:30:52.729841
[0m20:30:52.731222 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m20:30:52.745056 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m20:30:52.755400 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:52.756123 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m20:30:52.756920 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`movie_enriched`
where genres is null



      
    ) dbt_internal_test
[0m20:30:52.757704 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:53.647957 [debug] [Thread-2  ]: SQL status: OK in 0.8899999856948853 seconds
[0m20:30:53.652526 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (execute): 20:30:52.731693 => 20:30:53.652147
[0m20:30:53.653457 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: ROLLBACK
[0m20:30:53.654223 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:53.654851 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: Close
[0m20:30:53.765609 [info ] [Thread-2  ]: 13 of 35 PASS not_null_movie_enriched_genres ................................... [[32mPASS[0m in 1.05s]
[0m20:30:53.767154 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m20:30:53.767978 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m20:30:53.768799 [info ] [Thread-2  ]: 14 of 35 START test not_null_movie_enriched_movie_id ........................... [RUN]
[0m20:30:53.770093 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27, now test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86)
[0m20:30:53.770789 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m20:30:53.777445 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m20:30:53.787847 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (compile): 20:30:53.771292 => 20:30:53.787379
[0m20:30:53.788642 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m20:30:53.792705 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m20:30:53.802864 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:53.803616 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m20:30:53.804355 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is null



      
    ) dbt_internal_test
[0m20:30:53.805093 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:54.636399 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m20:30:54.641323 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (execute): 20:30:53.789112 => 20:30:54.640861
[0m20:30:54.642215 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: ROLLBACK
[0m20:30:54.642982 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:54.643846 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: Close
[0m20:30:54.756375 [info ] [Thread-2  ]: 14 of 35 PASS not_null_movie_enriched_movie_id ................................. [[32mPASS[0m in 0.99s]
[0m20:30:54.757757 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m20:30:54.758605 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m20:30:54.759428 [info ] [Thread-2  ]: 15 of 35 START test not_null_movie_enriched_title .............................. [RUN]
[0m20:30:54.760792 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86, now test.my_dbt_project.not_null_movie_enriched_title.d20f00c292)
[0m20:30:54.761472 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m20:30:54.768228 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m20:30:54.776888 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (compile): 20:30:54.761985 => 20:30:54.776420
[0m20:30:54.777648 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m20:30:54.781730 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m20:30:54.790180 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:54.790932 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m20:30:54.791534 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`movie_enriched`
where title is null



      
    ) dbt_internal_test
[0m20:30:54.792234 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:54.759720 [debug] [Thread-2  ]: SQL status: OK in -0.029999999329447746 seconds
[0m20:30:54.764240 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (execute): 20:30:54.778081 => 20:30:54.763899
[0m20:30:54.765052 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: ROLLBACK
[0m20:30:54.765999 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:54.766823 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: Close
[0m20:30:54.892438 [info ] [Thread-2  ]: 15 of 35 PASS not_null_movie_enriched_title .................................... [[32mPASS[0m in 0.13s]
[0m20:30:54.894008 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m20:30:54.895048 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m20:30:54.895889 [info ] [Thread-2  ]: 16 of 35 START test not_null_movie_enriched_total_ratings ...................... [RUN]
[0m20:30:54.897292 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_title.d20f00c292, now test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04)
[0m20:30:54.898060 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m20:30:54.904650 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m20:30:54.914942 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (compile): 20:30:54.898612 => 20:30:54.914438
[0m20:30:54.915821 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m20:30:54.931029 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m20:30:54.941341 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:54.942101 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m20:30:54.942659 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select total_ratings
from `workspace`.`movielens_volume`.`movie_enriched`
where total_ratings is null



      
    ) dbt_internal_test
[0m20:30:54.943275 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:55.835292 [debug] [Thread-2  ]: SQL status: OK in 0.8899999856948853 seconds
[0m20:30:55.840196 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (execute): 20:30:54.916335 => 20:30:55.839763
[0m20:30:55.841116 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: ROLLBACK
[0m20:30:55.841839 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:55.842505 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: Close
[0m20:30:55.955820 [info ] [Thread-2  ]: 16 of 35 PASS not_null_movie_enriched_total_ratings ............................ [[32mPASS[0m in 1.06s]
[0m20:30:55.957314 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m20:30:55.958222 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m20:30:55.959046 [info ] [Thread-2  ]: 17 of 35 START test not_null_movie_performance_genre_rank ...................... [RUN]
[0m20:30:55.960531 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04, now test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e)
[0m20:30:55.961218 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m20:30:55.970257 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m20:30:55.981252 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (compile): 20:30:55.961808 => 20:30:55.980791
[0m20:30:55.982146 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m20:30:55.986391 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m20:30:55.997105 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:55.997854 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m20:30:55.998522 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genre_rank
from `workspace`.`movielens_volume`.`movie_performance`
where genre_rank is null



      
    ) dbt_internal_test
[0m20:30:55.999275 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:56.859131 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m20:30:56.863394 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (execute): 20:30:55.982635 => 20:30:56.863040
[0m20:30:56.864279 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: ROLLBACK
[0m20:30:56.864978 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:56.865572 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: Close
[0m20:30:56.988055 [info ] [Thread-2  ]: 17 of 35 PASS not_null_movie_performance_genre_rank ............................ [[32mPASS[0m in 1.03s]
[0m20:30:56.989602 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m20:30:56.990513 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m20:30:56.991218 [info ] [Thread-2  ]: 18 of 35 START test not_null_movie_performance_movie_id ........................ [RUN]
[0m20:30:56.992468 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e, now test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52)
[0m20:30:56.993200 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m20:30:57.004867 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m20:30:57.015871 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (compile): 20:30:56.993709 => 20:30:57.015337
[0m20:30:57.016752 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m20:30:57.021302 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m20:30:57.032561 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:57.033370 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m20:30:57.034239 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is null



      
    ) dbt_internal_test
[0m20:30:57.035401 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:57.820460 [debug] [Thread-2  ]: SQL status: OK in 0.7900000214576721 seconds
[0m20:30:57.824755 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (execute): 20:30:57.017344 => 20:30:57.824355
[0m20:30:57.825770 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: ROLLBACK
[0m20:30:57.826514 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:57.827126 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: Close
[0m20:30:57.940597 [info ] [Thread-2  ]: 18 of 35 PASS not_null_movie_performance_movie_id .............................. [[32mPASS[0m in 0.95s]
[0m20:30:57.942160 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m20:30:57.942980 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m20:30:57.943686 [info ] [Thread-2  ]: 19 of 35 START test not_null_movie_performance_polarization_score .............. [RUN]
[0m20:30:57.944965 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52, now test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6)
[0m20:30:57.945769 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m20:30:57.954969 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m20:30:57.965791 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (compile): 20:30:57.946262 => 20:30:57.965283
[0m20:30:57.966671 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m20:30:57.972675 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m20:30:57.983543 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:57.984422 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m20:30:57.985135 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select polarization_score
from `workspace`.`movielens_volume`.`movie_performance`
where polarization_score is null



      
    ) dbt_internal_test
[0m20:30:57.985878 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:58.808890 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m20:30:58.813597 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (execute): 20:30:57.967168 => 20:30:58.813153
[0m20:30:58.814561 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: ROLLBACK
[0m20:30:58.815296 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:58.815903 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: Close
[0m20:30:58.923401 [info ] [Thread-2  ]: 19 of 35 PASS not_null_movie_performance_polarization_score .................... [[32mPASS[0m in 0.98s]
[0m20:30:58.924734 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m20:30:58.925696 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m20:30:58.926680 [info ] [Thread-2  ]: 20 of 35 START test not_null_movie_performance_rating_count .................... [RUN]
[0m20:30:58.928009 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6, now test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac)
[0m20:30:58.928697 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m20:30:58.937649 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m20:30:58.948432 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (compile): 20:30:58.929172 => 20:30:58.947937
[0m20:30:58.949353 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m20:30:58.953620 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m20:30:58.963676 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:58.964482 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m20:30:58.965076 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`movie_performance`
where rating_count is null



      
    ) dbt_internal_test
[0m20:30:58.965723 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:30:59.787327 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m20:30:59.791567 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (execute): 20:30:58.949891 => 20:30:59.791070
[0m20:30:59.792440 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: ROLLBACK
[0m20:30:59.793247 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:59.793993 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: Close
[0m20:30:59.972223 [info ] [Thread-2  ]: 20 of 35 PASS not_null_movie_performance_rating_count .......................... [[32mPASS[0m in 1.04s]
[0m20:30:59.973674 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m20:30:59.974547 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m20:30:59.975282 [info ] [Thread-2  ]: 21 of 35 START test not_null_movie_performance_top_rating_percentage ........... [RUN]
[0m20:30:59.976680 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac, now test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb)
[0m20:30:59.977370 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m20:30:59.983814 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m20:30:59.994235 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (compile): 20:30:59.977865 => 20:30:59.993753
[0m20:30:59.995127 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m20:31:00.000194 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m20:31:00.010423 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:00.011233 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m20:31:00.011980 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_rating_percentage
from `workspace`.`movielens_volume`.`movie_performance`
where top_rating_percentage is null



      
    ) dbt_internal_test
[0m20:31:00.012768 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:00.854972 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m20:31:00.859636 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (execute): 20:30:59.995681 => 20:31:00.859274
[0m20:31:00.860513 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: ROLLBACK
[0m20:31:00.861257 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:00.861971 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: Close
[0m20:31:00.977293 [info ] [Thread-2  ]: 21 of 35 PASS not_null_movie_performance_top_rating_percentage ................. [[32mPASS[0m in 1.00s]
[0m20:31:00.978717 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m20:31:00.979503 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m20:31:00.980179 [info ] [Thread-2  ]: 22 of 35 START test not_null_movie_performance_weighted_avg_rating ............. [RUN]
[0m20:31:00.981428 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb, now test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5)
[0m20:31:00.982103 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m20:31:00.988797 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m20:31:01.000268 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (compile): 20:31:00.982648 => 20:31:00.999748
[0m20:31:01.001050 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m20:31:01.013552 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m20:31:01.023932 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:01.024634 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m20:31:01.025314 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select weighted_avg_rating
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating is null



      
    ) dbt_internal_test
[0m20:31:01.026344 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:01.886059 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m20:31:01.890418 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (execute): 20:31:01.001549 => 20:31:01.890065
[0m20:31:01.891161 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: ROLLBACK
[0m20:31:01.892036 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:01.892878 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: Close
[0m20:31:02.003804 [info ] [Thread-2  ]: 22 of 35 PASS not_null_movie_performance_weighted_avg_rating ................... [[32mPASS[0m in 1.02s]
[0m20:31:02.005152 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m20:31:02.006295 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m20:31:02.007163 [info ] [Thread-2  ]: 23 of 35 START test not_null_user_activity_avg_user_rating ..................... [RUN]
[0m20:31:02.008594 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5, now test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b)
[0m20:31:02.009224 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m20:31:02.015814 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m20:31:02.027245 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (compile): 20:31:02.009691 => 20:31:02.026720
[0m20:31:02.028243 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m20:31:02.032834 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m20:31:02.045953 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:02.046727 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m20:31:02.047447 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_user_rating
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating is null



      
    ) dbt_internal_test
[0m20:31:02.048174 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:02.845665 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m20:31:02.851650 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (execute): 20:31:02.028924 => 20:31:02.850936
[0m20:31:02.852647 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: ROLLBACK
[0m20:31:02.853435 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:02.854204 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: Close
[0m20:31:02.960309 [info ] [Thread-2  ]: 23 of 35 PASS not_null_user_activity_avg_user_rating ........................... [[32mPASS[0m in 0.95s]
[0m20:31:02.961726 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m20:31:02.962625 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m20:31:02.963434 [info ] [Thread-2  ]: 24 of 35 START test not_null_user_activity_first_activity_date ................. [RUN]
[0m20:31:02.964967 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b, now test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac)
[0m20:31:02.965768 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m20:31:02.972401 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m20:31:02.982922 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (compile): 20:31:02.966302 => 20:31:02.982398
[0m20:31:02.983693 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m20:31:02.988320 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m20:31:02.998708 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:02.999538 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m20:31:03.000725 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where first_activity_date is null



      
    ) dbt_internal_test
[0m20:31:03.001523 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:03.822866 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m20:31:03.827222 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (execute): 20:31:02.984272 => 20:31:03.826889
[0m20:31:03.828040 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: ROLLBACK
[0m20:31:03.828943 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:03.829840 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: Close
[0m20:31:03.946718 [info ] [Thread-2  ]: 24 of 35 PASS not_null_user_activity_first_activity_date ....................... [[32mPASS[0m in 0.98s]
[0m20:31:03.947937 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m20:31:03.948670 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m20:31:03.949431 [info ] [Thread-2  ]: 25 of 35 START test not_null_user_activity_influence_score ..................... [RUN]
[0m20:31:03.950664 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac, now test.my_dbt_project.not_null_user_activity_influence_score.758852d325)
[0m20:31:03.951558 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m20:31:03.957534 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m20:31:03.968082 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (compile): 20:31:03.952010 => 20:31:03.967624
[0m20:31:03.968814 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m20:31:03.982410 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m20:31:03.993393 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:03.994407 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m20:31:03.995080 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select influence_score
from `workspace`.`movielens_volume`.`user_activity`
where influence_score is null



      
    ) dbt_internal_test
[0m20:31:03.995803 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:04.775591 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m20:31:04.779485 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (execute): 20:31:03.969266 => 20:31:04.779101
[0m20:31:04.780340 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: ROLLBACK
[0m20:31:04.781027 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:04.781648 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: Close
[0m20:31:04.903325 [info ] [Thread-2  ]: 25 of 35 PASS not_null_user_activity_influence_score ........................... [[32mPASS[0m in 0.95s]
[0m20:31:04.904730 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m20:31:04.905633 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m20:31:04.906488 [info ] [Thread-2  ]: 26 of 35 START test not_null_user_activity_last_activity_date .................. [RUN]
[0m20:31:04.907685 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_influence_score.758852d325, now test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad)
[0m20:31:04.908565 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m20:31:04.917796 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m20:31:04.928234 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (compile): 20:31:04.909129 => 20:31:04.927758
[0m20:31:04.929002 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m20:31:04.933257 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m20:31:04.942798 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:04.943523 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m20:31:04.944508 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where last_activity_date is null



      
    ) dbt_internal_test
[0m20:31:04.945246 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:06.073581 [debug] [Thread-2  ]: SQL status: OK in 1.1299999952316284 seconds
[0m20:31:06.077837 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (execute): 20:31:04.929477 => 20:31:06.077495
[0m20:31:06.078619 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: ROLLBACK
[0m20:31:06.079352 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:06.080032 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: Close
[0m20:31:06.198841 [info ] [Thread-2  ]: 26 of 35 PASS not_null_user_activity_last_activity_date ........................ [[32mPASS[0m in 1.29s]
[0m20:31:06.200372 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m20:31:06.201186 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m20:31:06.202045 [info ] [Thread-2  ]: 27 of 35 START test not_null_user_activity_top_genre ........................... [RUN]
[0m20:31:06.203388 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad, now test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313)
[0m20:31:06.204304 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m20:31:06.213584 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m20:31:06.224011 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (compile): 20:31:06.204920 => 20:31:06.223469
[0m20:31:06.224789 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m20:31:06.229108 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m20:31:06.239463 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:06.240277 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m20:31:06.240878 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_genre
from `workspace`.`movielens_volume`.`user_activity`
where top_genre is null



      
    ) dbt_internal_test
[0m20:31:06.241520 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:07.045913 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m20:31:07.050504 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (execute): 20:31:06.225313 => 20:31:07.050108
[0m20:31:07.051381 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: ROLLBACK
[0m20:31:07.052208 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:07.053061 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: Close
[0m20:31:07.168330 [info ] [Thread-2  ]: 27 of 35 PASS not_null_user_activity_top_genre ................................. [[32mPASS[0m in 0.97s]
[0m20:31:07.169736 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m20:31:07.170540 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m20:31:07.171256 [info ] [Thread-2  ]: 28 of 35 START test not_null_user_activity_user_id ............................. [RUN]
[0m20:31:07.172551 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313, now test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36)
[0m20:31:07.173201 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m20:31:07.182488 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m20:31:07.192725 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (compile): 20:31:07.173678 => 20:31:07.192235
[0m20:31:07.193574 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m20:31:07.200270 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m20:31:07.210529 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:07.211361 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m20:31:07.212083 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`user_activity`
where user_id is null



      
    ) dbt_internal_test
[0m20:31:07.212760 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:08.009997 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m20:31:08.014425 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (execute): 20:31:07.194057 => 20:31:08.014056
[0m20:31:08.015234 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: ROLLBACK
[0m20:31:08.015898 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:08.016513 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: Close
[0m20:31:08.139010 [info ] [Thread-2  ]: 28 of 35 PASS not_null_user_activity_user_id ................................... [[32mPASS[0m in 0.97s]
[0m20:31:08.140550 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m20:31:08.141544 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m20:31:08.142324 [info ] [Thread-2  ]: 29 of 35 START test not_null_user_activity_user_rating_count ................... [RUN]
[0m20:31:08.143572 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36, now test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e)
[0m20:31:08.144275 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m20:31:08.154137 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m20:31:08.164644 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (compile): 20:31:08.144786 => 20:31:08.164137
[0m20:31:08.165387 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m20:31:08.169586 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m20:31:08.179473 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:08.180195 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m20:31:08.180876 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_rating_count
from `workspace`.`movielens_volume`.`user_activity`
where user_rating_count is null



      
    ) dbt_internal_test
[0m20:31:08.181522 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:09.000340 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m20:31:09.004867 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (execute): 20:31:08.165873 => 20:31:09.004449
[0m20:31:09.005803 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: ROLLBACK
[0m20:31:09.006561 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:09.007378 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: Close
[0m20:31:09.133519 [info ] [Thread-2  ]: 29 of 35 PASS not_null_user_activity_user_rating_count ......................... [[32mPASS[0m in 0.99s]
[0m20:31:09.135081 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m20:31:09.135977 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m20:31:09.136762 [info ] [Thread-2  ]: 30 of 35 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m20:31:09.138059 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m20:31:09.138742 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m20:31:09.150921 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m20:31:09.161167 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 20:31:09.139248 => 20:31:09.160747
[0m20:31:09.162075 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m20:31:09.166062 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m20:31:09.176210 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:09.177138 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m20:31:09.177929 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m20:31:09.178684 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:09.790227 [debug] [Thread-2  ]: SQL status: OK in 0.6100000143051147 seconds
[0m20:31:09.794668 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 20:31:09.162605 => 20:31:09.794286
[0m20:31:09.795520 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m20:31:09.796301 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:09.796955 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m20:31:09.907436 [info ] [Thread-2  ]: 30 of 35 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.77s]
[0m20:31:09.908837 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m20:31:09.909697 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m20:31:09.910546 [info ] [Thread-2  ]: 31 of 35 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m20:31:09.911867 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m20:31:09.912640 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m20:31:09.921202 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m20:31:09.931337 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 20:31:09.913154 => 20:31:09.930927
[0m20:31:09.932158 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m20:31:09.936403 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m20:31:09.946029 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:09.946710 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m20:31:09.947411 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m20:31:09.948247 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:10.565930 [debug] [Thread-2  ]: SQL status: OK in 0.6200000047683716 seconds
[0m20:31:10.570454 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 20:31:09.932651 => 20:31:10.570051
[0m20:31:10.571357 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m20:31:10.572085 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:10.572715 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m20:31:10.694086 [info ] [Thread-2  ]: 31 of 35 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.78s]
[0m20:31:10.695593 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m20:31:10.696574 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m20:31:10.697321 [info ] [Thread-2  ]: 32 of 35 START test unique_genre_popularity_genres ............................. [RUN]
[0m20:31:10.698560 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075)
[0m20:31:10.699278 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m20:31:10.709249 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m20:31:10.718951 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (compile): 20:31:10.699889 => 20:31:10.718526
[0m20:31:10.719774 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m20:31:10.723980 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m20:31:10.734021 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:10.734812 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m20:31:10.735452 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    genres as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`genre_popularity`
where genres is not null
group by genres
having count(*) > 1



      
    ) dbt_internal_test
[0m20:31:10.736155 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:11.602578 [debug] [Thread-2  ]: SQL status: OK in 0.8700000047683716 seconds
[0m20:31:11.606924 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (execute): 20:31:10.720429 => 20:31:11.606546
[0m20:31:11.607892 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: ROLLBACK
[0m20:31:11.608721 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:11.609376 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: Close
[0m20:31:11.719944 [info ] [Thread-2  ]: 32 of 35 PASS unique_genre_popularity_genres ................................... [[32mPASS[0m in 1.02s]
[0m20:31:11.721734 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m20:31:11.722632 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m20:31:11.723399 [info ] [Thread-2  ]: 33 of 35 START test unique_movie_enriched_movie_id ............................. [RUN]
[0m20:31:11.724640 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075, now test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a)
[0m20:31:11.725311 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m20:31:11.731899 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m20:31:11.742236 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (compile): 20:31:11.725820 => 20:31:11.741751
[0m20:31:11.743127 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m20:31:11.747185 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m20:31:11.757565 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:11.758299 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m20:31:11.758886 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:31:11.759530 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:12.580491 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m20:31:12.585109 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (execute): 20:31:11.743635 => 20:31:12.584773
[0m20:31:12.585948 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: ROLLBACK
[0m20:31:12.586906 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:12.587570 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m20:31:12.706279 [info ] [Thread-2  ]: 33 of 35 PASS unique_movie_enriched_movie_id ................................... [[32mPASS[0m in 0.98s]
[0m20:31:12.708283 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m20:31:12.709362 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m20:31:12.710180 [info ] [Thread-2  ]: 34 of 35 START test unique_movie_performance_movie_id .......................... [RUN]
[0m20:31:12.711466 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a, now test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22)
[0m20:31:12.712291 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m20:31:12.719096 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m20:31:12.729499 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (compile): 20:31:12.712865 => 20:31:12.729050
[0m20:31:12.730368 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m20:31:12.734572 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m20:31:12.744384 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:12.745121 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m20:31:12.745799 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:31:12.746488 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:13.581719 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m20:31:13.592936 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (execute): 20:31:12.730871 => 20:31:13.591348
[0m20:31:13.593878 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: ROLLBACK
[0m20:31:13.594664 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:13.595286 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: Close
[0m20:31:13.715276 [info ] [Thread-2  ]: 34 of 35 PASS unique_movie_performance_movie_id ................................ [[32mPASS[0m in 1.00s]
[0m20:31:13.717049 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m20:31:13.717838 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m20:31:13.718586 [info ] [Thread-2  ]: 35 of 35 START test unique_user_activity_user_id ............................... [RUN]
[0m20:31:13.719971 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22, now test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda)
[0m20:31:13.720782 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m20:31:13.727690 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m20:31:13.735878 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (compile): 20:31:13.721342 => 20:31:13.735412
[0m20:31:13.737000 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m20:31:13.741144 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m20:31:13.749085 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:13.749834 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m20:31:13.750472 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`user_activity`
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:31:13.751220 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m20:31:14.592818 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m20:31:14.597770 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (execute): 20:31:13.737563 => 20:31:14.597210
[0m20:31:14.598964 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: ROLLBACK
[0m20:31:14.599867 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m20:31:14.600606 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: Close
[0m20:31:14.728647 [info ] [Thread-2  ]: 35 of 35 PASS unique_user_activity_user_id ..................................... [[32mPASS[0m in 1.01s]
[0m20:31:14.730150 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m20:31:14.733133 [debug] [MainThread]: On master: ROLLBACK
[0m20:31:14.733844 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:31:15.099126 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:31:15.100081 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:15.100717 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:31:15.101454 [debug] [MainThread]: On master: ROLLBACK
[0m20:31:15.102219 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:31:15.102896 [debug] [MainThread]: On master: Close
[0m20:31:15.218544 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:31:15.219377 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda' was properly closed.
[0m20:31:15.221841 [info ] [MainThread]: 
[0m20:31:15.223279 [info ] [MainThread]: Finished running 35 tests in 0 hours 0 minutes and 35.98 seconds (35.98s).
[0m20:31:15.231221 [debug] [MainThread]: Command end result
[0m20:31:15.262306 [info ] [MainThread]: 
[0m20:31:15.263273 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:31:15.264100 [info ] [MainThread]: 
[0m20:31:15.264997 [info ] [MainThread]: Done. PASS=35 WARN=0 ERROR=0 SKIP=0 TOTAL=35
[0m20:31:15.266119 [debug] [MainThread]: Command `dbt test` succeeded at 20:31:15.265978 after 39.39 seconds
[0m20:31:15.266771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcd41035b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efca73dac40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efca741e280>]}
[0m20:31:15.267770 [debug] [MainThread]: Flushing usage events
[0m06:56:10.826837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e23d43520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e222ae190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e222ae8b0>]}


============================== 06:56:10.835397 | 929bf554-bac2-4764-8a06-a35c06c5fd03 ==============================
[0m06:56:10.835397 [info ] [MainThread]: Running with dbt=1.5.2
[0m06:56:10.836408 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/opt/airflow/dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m06:56:11.994800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '929bf554-bac2-4764-8a06-a35c06c5fd03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e222ae130>]}
[0m06:56:12.023181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '929bf554-bac2-4764-8a06-a35c06c5fd03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3df6fdb820>]}
[0m06:56:12.024162 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m06:56:12.066881 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m06:56:14.254938 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:56:14.255681 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:56:14.266986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '929bf554-bac2-4764-8a06-a35c06c5fd03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3df6f83370>]}
[0m06:56:14.302726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '929bf554-bac2-4764-8a06-a35c06c5fd03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3df6eb23a0>]}
[0m06:56:14.303727 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m06:56:14.304860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '929bf554-bac2-4764-8a06-a35c06c5fd03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3df6eb2400>]}
[0m06:56:14.307579 [info ] [MainThread]: 
[0m06:56:14.308946 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:56:14.311006 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:56:14.311769 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:56:14.312394 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m06:56:14.313224 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:57:18.181192 [debug] [ThreadPool]: SQL status: OK in 63.869998931884766 seconds
[0m06:57:18.187686 [debug] [ThreadPool]: On list_workspace: Close
[0m06:57:18.295875 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m06:57:18.297702 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m06:57:18.317464 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m06:57:18.318297 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m06:57:18.318831 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m06:57:18.319449 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:57:20.281347 [debug] [ThreadPool]: SQL status: OK in 1.9600000381469727 seconds
[0m06:57:20.282800 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m06:57:20.283418 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m06:57:20.284031 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m06:57:20.284622 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m06:57:20.397421 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m06:57:20.406175 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m06:57:20.406888 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m06:57:20.407392 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:57:21.205147 [debug] [ThreadPool]: SQL status: OK in 0.800000011920929 seconds
[0m06:57:21.208764 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m06:57:21.320950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '929bf554-bac2-4764-8a06-a35c06c5fd03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3df6eb2250>]}
[0m06:57:21.322149 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:57:21.322842 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:57:21.324392 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:57:21.325462 [info ] [MainThread]: 
[0m06:57:21.344427 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_links
[0m06:57:21.345514 [info ] [Thread-4  ]: 1 of 4 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m06:57:21.346726 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.silver_links)
[0m06:57:21.347389 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_links
[0m06:57:21.359091 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m06:57:21.367128 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (compile): 06:57:21.348018 => 06:57:21.366781
[0m06:57:21.367819 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_links
[0m06:57:21.386440 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:57:21.387053 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m06:57:21.387703 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

      describe extended `workspace`.`movielens_volume`.`silver_links`
  
[0m06:57:21.388377 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m06:57:24.854314 [debug] [Thread-4  ]: SQL status: OK in 3.4700000286102295 seconds
[0m06:57:24.936316 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m06:57:24.944729 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m06:57:24.945403 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
    using delta
      
      
      
      
      
      
      as
      

with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m06:57:35.058339 [debug] [Thread-4  ]: SQL status: OK in 10.109999656677246 seconds
[0m06:57:35.243725 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (execute): 06:57:21.368326 => 06:57:35.243271
[0m06:57:35.244730 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: ROLLBACK
[0m06:57:35.245843 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m06:57:35.246551 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: Close
[0m06:57:35.355213 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '929bf554-bac2-4764-8a06-a35c06c5fd03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3df6cdfe80>]}
[0m06:57:35.356870 [info ] [Thread-4  ]: 1 of 4 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 14.01s]
[0m06:57:35.358259 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_links
[0m06:57:35.359079 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_movies
[0m06:57:35.360338 [info ] [Thread-4  ]: 2 of 4 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m06:57:35.361596 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_links, now model.my_dbt_project.silver_movies)
[0m06:57:35.362286 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_movies
[0m06:57:35.368678 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m06:57:35.376957 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (compile): 06:57:35.362758 => 06:57:35.376602
[0m06:57:35.377768 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_movies
[0m06:57:35.388788 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:57:35.389603 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m06:57:35.390238 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

      describe extended `workspace`.`movielens_volume`.`silver_movies`
  
[0m06:57:35.391017 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m06:57:36.855572 [debug] [Thread-4  ]: SQL status: OK in 1.4600000381469727 seconds
[0m06:57:36.864764 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m06:57:36.872786 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m06:57:36.873446 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m06:57:40.487422 [debug] [Thread-4  ]: SQL status: OK in 3.609999895095825 seconds
[0m06:57:40.491070 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (execute): 06:57:35.378347 => 06:57:40.490728
[0m06:57:40.491771 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: ROLLBACK
[0m06:57:40.492464 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m06:57:40.493168 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: Close
[0m06:57:40.606003 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '929bf554-bac2-4764-8a06-a35c06c5fd03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3df6d0d610>]}
[0m06:57:40.607171 [info ] [Thread-4  ]: 2 of 4 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 5.24s]
[0m06:57:40.608473 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_movies
[0m06:57:40.609486 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_ratings
[0m06:57:40.610441 [info ] [Thread-4  ]: 3 of 4 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m06:57:40.611675 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_movies, now model.my_dbt_project.silver_ratings)
[0m06:57:40.612338 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_ratings
[0m06:57:40.617814 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m06:57:40.630713 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (compile): 06:57:40.612872 => 06:57:40.630386
[0m06:57:40.631483 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_ratings
[0m06:57:40.637147 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:57:40.637645 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m06:57:40.638145 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

      describe extended `workspace`.`movielens_volume`.`silver_ratings`
  
[0m06:57:40.638778 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m06:57:41.952292 [debug] [Thread-4  ]: SQL status: OK in 1.309999942779541 seconds
[0m06:57:41.959120 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m06:57:41.967861 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m06:57:41.968472 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m06:57:49.060414 [debug] [Thread-4  ]: SQL status: OK in 7.090000152587891 seconds
[0m06:57:49.181387 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (execute): 06:57:40.631990 => 06:57:49.181116
[0m06:57:49.182137 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: ROLLBACK
[0m06:57:49.182802 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m06:57:49.183486 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: Close
[0m06:57:49.287988 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '929bf554-bac2-4764-8a06-a35c06c5fd03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3df5b46490>]}
[0m06:57:49.289062 [info ] [Thread-4  ]: 3 of 4 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 8.68s]
[0m06:57:49.290190 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_ratings
[0m06:57:49.290995 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_tags
[0m06:57:49.291684 [info ] [Thread-4  ]: 4 of 4 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m06:57:49.292760 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_ratings, now model.my_dbt_project.silver_tags)
[0m06:57:49.293388 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_tags
[0m06:57:49.303662 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m06:57:49.312151 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (compile): 06:57:49.293840 => 06:57:49.311708
[0m06:57:49.312828 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_tags
[0m06:57:49.318813 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:57:49.319411 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m06:57:49.320068 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

      describe extended `workspace`.`movielens_volume`.`silver_tags`
  
[0m06:57:49.320722 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m06:57:50.663963 [debug] [Thread-4  ]: SQL status: OK in 1.340000033378601 seconds
[0m06:57:50.670312 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m06:57:50.678520 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m06:57:50.679171 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m06:57:55.358762 [debug] [Thread-4  ]: SQL status: OK in 4.679999828338623 seconds
[0m06:57:55.364255 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (execute): 06:57:49.313387 => 06:57:55.363662
[0m06:57:55.365139 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: ROLLBACK
[0m06:57:55.365853 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m06:57:55.366570 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: Close
[0m06:57:55.475094 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '929bf554-bac2-4764-8a06-a35c06c5fd03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3df5c42580>]}
[0m06:57:55.476205 [info ] [Thread-4  ]: 4 of 4 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 6.18s]
[0m06:57:55.477500 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_tags
[0m06:57:55.480465 [debug] [MainThread]: On master: ROLLBACK
[0m06:57:55.481038 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:57:55.830591 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m06:57:55.831395 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:57:55.831995 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:57:55.832586 [debug] [MainThread]: On master: ROLLBACK
[0m06:57:55.833123 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m06:57:55.833672 [debug] [MainThread]: On master: Close
[0m06:57:55.947296 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:57:55.948030 [debug] [MainThread]: Connection 'model.my_dbt_project.silver_tags' was properly closed.
[0m06:57:55.950457 [info ] [MainThread]: 
[0m06:57:55.951948 [info ] [MainThread]: Finished running 4 table models in 0 hours 1 minutes and 41.64 seconds (101.64s).
[0m06:57:55.954357 [debug] [MainThread]: Command end result
[0m06:57:55.990851 [info ] [MainThread]: 
[0m06:57:55.991956 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:57:55.992738 [info ] [MainThread]: 
[0m06:57:55.993437 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m06:57:55.994936 [debug] [MainThread]: Command `dbt run` succeeded at 06:57:55.994726 after 100.95 seconds
[0m06:57:55.995728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e23d43520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3df5c42580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3df7024970>]}
[0m06:57:55.996356 [debug] [MainThread]: Flushing usage events
[0m06:58:00.055729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0021f83640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00204ee250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00204eea00>]}


============================== 06:58:00.062302 | e1aa0c3c-50f4-4a6d-8e99-0ba808aea084 ==============================
[0m06:58:00.062302 [info ] [MainThread]: Running with dbt=1.5.2
[0m06:58:00.063265 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'profiles_dir': '/opt/airflow/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m06:58:01.002236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e1aa0c3c-50f4-4a6d-8e99-0ba808aea084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00204ee220>]}
[0m06:58:01.031548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e1aa0c3c-50f4-4a6d-8e99-0ba808aea084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff9257a30>]}
[0m06:58:01.032620 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m06:58:01.077474 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m06:58:02.219413 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:58:02.220083 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:58:02.231258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e1aa0c3c-50f4-4a6d-8e99-0ba808aea084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff8fa1490>]}
[0m06:58:02.262253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e1aa0c3c-50f4-4a6d-8e99-0ba808aea084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff910b490>]}
[0m06:58:02.263132 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m06:58:02.263985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e1aa0c3c-50f4-4a6d-8e99-0ba808aea084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff910b460>]}
[0m06:58:02.266416 [info ] [MainThread]: 
[0m06:58:02.267614 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:58:02.269576 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m06:58:02.270238 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m06:58:02.270763 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m06:58:02.271304 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:58:02.830086 [debug] [ThreadPool]: SQL status: OK in 0.5600000023841858 seconds
[0m06:58:02.832115 [debug] [ThreadPool]: On list_workspace: Close
[0m06:58:02.958532 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m06:58:02.959915 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m06:58:02.973136 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m06:58:02.973713 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m06:58:02.974344 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m06:58:02.974923 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:58:03.656554 [debug] [ThreadPool]: SQL status: OK in 0.6800000071525574 seconds
[0m06:58:03.658522 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m06:58:03.659231 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m06:58:03.659847 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m06:58:03.660532 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m06:58:03.775457 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m06:58:03.784397 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m06:58:03.785176 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m06:58:03.785932 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:58:04.371802 [debug] [ThreadPool]: SQL status: OK in 0.5899999737739563 seconds
[0m06:58:04.375092 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m06:58:04.485327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e1aa0c3c-50f4-4a6d-8e99-0ba808aea084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff9112940>]}
[0m06:58:04.486263 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:58:04.486840 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:58:04.487849 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:58:04.488556 [info ] [MainThread]: 
[0m06:58:04.501835 [debug] [Thread-4  ]: Began running node model.my_dbt_project.genre_popularity
[0m06:58:04.502776 [info ] [Thread-4  ]: 1 of 5 START sql incremental model movielens_volume.genre_popularity ........... [RUN]
[0m06:58:04.503893 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.genre_popularity)
[0m06:58:04.504477 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.genre_popularity
[0m06:58:04.522182 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.genre_popularity"
[0m06:58:04.530361 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (compile): 06:58:04.504939 => 06:58:04.529896
[0m06:58:04.531163 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.genre_popularity
[0m06:58:04.575073 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:58:04.575790 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m06:58:04.576337 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m06:58:04.576919 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m06:58:05.883961 [debug] [Thread-4  ]: SQL status: OK in 1.309999942779541 seconds
[0m06:58:05.911305 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m06:58:05.912304 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

  
    create temporary view `genre_popularity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings with Genres
-- Join silver ratings with silver movies to attach genres
-- Prepare rating timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        r.rating,
        r.rating_date as rating_timestamp,  
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Aggregate Genre Metrics
-- For each genre, calculate total ratings, avg rating,
-- and distribution insights
-- =======================================================
genre_stats as (
    select
        genres,  -- genres already formatted with spacing
        count(*) as rating_count,
        round(avg(rating), 2) as avg_rating,  
        to_date(min(rating_timestamp)) as first_rating_date,  
        to_date(max(rating_timestamp)) as last_rating_date   
    from ratings_base
    group by genres
),

-- =======================================================
-- Step 3: Ranking Genres
-- Rank genres by popularity (# of ratings)
-- and quality (avg rating)
-- =======================================================
ranked_genres as (
    select
        genres,
        rating_count,
        avg_rating,
        first_rating_date,
        last_rating_date,
        rank() over (order by rating_count desc) as popularity_rank,
        rank() over (order by avg_rating desc) as quality_rank
    from genre_stats
)

-- =======================================================
-- Step 4: Final Selection
-- One row per genre with all metrics
-- =======================================================
select
    genres,
    rating_count,
    avg_rating,
    popularity_rank,
    quality_rank,
    first_rating_date,
    last_rating_date
from ranked_genres


  -- Only insert new genres on incremental runs
  where genres not in (select genres from `workspace`.`movielens_volume`.`genre_popularity`)

  
[0m06:58:06.543695 [debug] [Thread-4  ]: SQL status: OK in 0.6299999952316284 seconds
[0m06:58:06.569462 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m06:58:06.570219 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m06:58:06.983775 [debug] [Thread-4  ]: SQL status: OK in 0.4099999964237213 seconds
[0m06:58:06.995584 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.genre_popularity"
[0m06:58:07.003394 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m06:58:07.003963 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`genre_popularity` as DBT_INTERNAL_DEST
      using `genre_popularity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.genres = DBT_INTERNAL_DEST.genres
          

      when matched then update set
         * 

      when not matched then insert *

[0m06:58:18.065059 [debug] [Thread-4  ]: SQL status: OK in 11.0600004196167 seconds
[0m06:58:18.242389 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (execute): 06:58:04.531628 => 06:58:18.242133
[0m06:58:18.243183 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: ROLLBACK
[0m06:58:18.244074 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m06:58:18.244873 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: Close
[0m06:58:18.362025 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e1aa0c3c-50f4-4a6d-8e99-0ba808aea084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff85adb80>]}
[0m06:58:18.363211 [info ] [Thread-4  ]: 1 of 5 OK created sql incremental model movielens_volume.genre_popularity ...... [[32mOK[0m in 13.86s]
[0m06:58:18.364538 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.genre_popularity
[0m06:58:18.365366 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_enriched
[0m06:58:18.366188 [info ] [Thread-4  ]: 2 of 5 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m06:58:18.367279 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.genre_popularity, now model.my_dbt_project.movie_enriched)
[0m06:58:18.367851 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_enriched
[0m06:58:18.373244 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m06:58:18.381310 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (compile): 06:58:18.368280 => 06:58:18.380897
[0m06:58:18.382111 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_enriched
[0m06:58:18.397760 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:58:18.398444 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m06:58:18.399035 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

      describe extended `workspace`.`movielens_volume`.`movie_enriched`
  
[0m06:58:18.399737 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m06:58:19.967916 [debug] [Thread-4  ]: SQL status: OK in 1.5700000524520874 seconds
[0m06:58:20.010637 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m06:58:20.019194 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m06:58:20.019970 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m06:58:23.950899 [debug] [Thread-4  ]: SQL status: OK in 3.930000066757202 seconds
[0m06:58:23.958329 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (execute): 06:58:18.382571 => 06:58:23.958033
[0m06:58:23.959113 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: ROLLBACK
[0m06:58:23.959808 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m06:58:23.960372 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: Close
[0m06:58:24.070292 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e1aa0c3c-50f4-4a6d-8e99-0ba808aea084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff8f9d8b0>]}
[0m06:58:24.071525 [info ] [Thread-4  ]: 2 of 5 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 5.70s]
[0m06:58:24.072642 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_enriched
[0m06:58:24.073376 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_performance
[0m06:58:24.074239 [info ] [Thread-4  ]: 3 of 5 START sql incremental model movielens_volume.movie_performance .......... [RUN]
[0m06:58:24.075687 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_enriched, now model.my_dbt_project.movie_performance)
[0m06:58:24.076404 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_performance
[0m06:58:24.083489 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_performance"
[0m06:58:24.091803 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (compile): 06:58:24.076887 => 06:58:24.091497
[0m06:58:24.092510 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_performance
[0m06:58:24.102050 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:58:24.102655 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m06:58:24.103157 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m06:58:24.103711 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m06:58:25.326297 [debug] [Thread-4  ]: SQL status: OK in 1.2200000286102295 seconds
[0m06:58:25.331278 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m06:58:25.332081 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

  
    create temporary view `movie_performance__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings Join
-- Attach movie metadata (title, genres) to ratings
-- =======================================================
with ratings_base as (
    select
        r.movie_id,
        m.title,
        regexp_replace(m.genres, '\\|', ' | ') as genres,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_date
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Weighted Rating
-- Compute avg rating, rating count, weighted avg
-- =======================================================
weighted_rating as (
    select
        movie_id,
        title,
        genres,
        round(avg(rating), 2) as avg_rating,
        count(*) as rating_count,
        round(sum(rating) / count(*), 2) as weighted_avg_rating
    from ratings_base
    group by movie_id, title, genres
),

-- =======================================================
-- Step 3: Polarization Score
-- Standard deviation of ratings per movie
-- =======================================================
polarization as (
    select
        movie_id,
        round(coalesce(stddev(rating), 0.0), 2) as polarization_score
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 4: Top Rating Percentage
-- Percent of ratings >= 4.5
-- =======================================================
top_rating_pct as (
    select
        movie_id,
        round(100.0 * sum(case when rating >= 4.5 then 1 else 0 end) / count(*), 2) as top_rating_percentage
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 5: Genre Ranking
-- Rank movies within each genre by weighted_avg_rating
-- =======================================================
genre_rank as (
    select
        movie_id,
        genres,
        weighted_avg_rating,
        rank() over (partition by genres order by weighted_avg_rating desc) as genre_rank
    from weighted_rating
),

-- =======================================================
-- Step 6: Tag Count
-- Count distinct tags per movie
-- =======================================================
tag_count as (
    select
        movie_id,
        count(distinct tag) as tag_count
    from `workspace`.`movielens_volume`.`silver_tags`
    group by movie_id
)

-- =======================================================
-- Step 7: Final Selection
-- Combine all metrics into the gold table
-- =======================================================
select
    w.movie_id,
    w.title,
    w.genres,
    w.weighted_avg_rating,
    w.rating_count,
    g.genre_rank,
    p.polarization_score,
    tr.top_rating_percentage,
    coalesce(tc.tag_count, 0) as tag_count
from weighted_rating w
join genre_rank g
    on w.movie_id = g.movie_id
join polarization p
    on w.movie_id = p.movie_id
join top_rating_pct tr
    on w.movie_id = tr.movie_id
left join tag_count tc
    on w.movie_id = tc.movie_id


-- Use NOT EXISTS for safer incremental inserts
where not exists (
    select 1 
    from `workspace`.`movielens_volume`.`movie_performance` t
    where t.movie_id = w.movie_id
)

  
[0m06:58:26.311874 [debug] [Thread-4  ]: SQL status: OK in 0.9800000190734863 seconds
[0m06:58:26.316951 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m06:58:26.317675 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m06:58:26.694064 [debug] [Thread-4  ]: SQL status: OK in 0.3799999952316284 seconds
[0m06:58:26.699044 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_performance"
[0m06:58:26.707617 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m06:58:26.708316 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`movie_performance` as DBT_INTERNAL_DEST
      using `movie_performance__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.movie_id = DBT_INTERNAL_DEST.movie_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m06:58:33.740496 [debug] [Thread-4  ]: SQL status: OK in 7.03000020980835 seconds
[0m06:58:33.897892 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (execute): 06:58:24.093001 => 06:58:33.897269
[0m06:58:33.899140 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: ROLLBACK
[0m06:58:33.900016 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m06:58:33.900758 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: Close
[0m06:58:34.009055 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e1aa0c3c-50f4-4a6d-8e99-0ba808aea084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff86c0e20>]}
[0m06:58:34.010454 [info ] [Thread-4  ]: 3 of 5 OK created sql incremental model movielens_volume.movie_performance ..... [[32mOK[0m in 9.93s]
[0m06:58:34.011671 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_performance
[0m06:58:34.012442 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_activity
[0m06:58:34.013420 [info ] [Thread-4  ]: 4 of 5 START sql incremental model movielens_volume.user_activity .............. [RUN]
[0m06:58:34.014600 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_performance, now model.my_dbt_project.user_activity)
[0m06:58:34.015184 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_activity
[0m06:58:34.024789 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_activity"
[0m06:58:34.033156 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (compile): 06:58:34.015665 => 06:58:34.032658
[0m06:58:34.033910 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_activity
[0m06:58:34.040619 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:58:34.041152 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m06:58:34.041676 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m06:58:34.042417 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m06:58:35.209400 [debug] [Thread-4  ]: SQL status: OK in 1.1699999570846558 seconds
[0m06:58:35.216658 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m06:58:35.217688 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

  
    create temporary view `user_activity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Join ratings with movies to attach genres and prepare timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_timestamp,
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: User Activity (General)
-- Calculate total ratings, average rating, first and last activity dates per user
-- =======================================================
user_activity as (
    select
        user_id,
        count(*) as user_rating_count,
        round(avg(rating), 2) as avg_user_rating,
        min(rating_timestamp) as first_activity_date,
        max(rating_timestamp) as last_activity_date
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 3: User Influence Score
-- Compute influence based on total ratings (more ratings = higher influence)
-- =======================================================
user_influence as (
    select
        user_id,
        user_rating_count as influence_score
    from user_activity
),

-- =======================================================
-- Step 4: Active Users by Genre
-- Count ratings per genre and rank genres per user
-- =======================================================
user_genre_activity as (
    select
        user_id,
        regexp_replace(genres, '\\|', ' | ') as genres,
        count(*) as genre_rating_count,
        rank() over (partition by user_id order by count(*) desc) as genre_rank
    from ratings_base
    group by user_id, genres
),

-- =======================================================
-- Step 5: New vs Loyal Users
-- Categorize users as new if first activity < 30 days ago, else loyal
-- =======================================================
user_type as (
    select
        user_id,
        case
            when min(rating_timestamp) >= date_sub(current_date(), 30)
                then 'new_user'
            else 'loyal_user'
        end as user_category
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 6: Top Genre per User
-- Identify the most active genre for each user
-- =======================================================
top_genre as (
    select
        user_id,
        max(genres) as top_genre
    from user_genre_activity
    group by user_id
)

-- =======================================================
-- Final Selection
-- Combine general activity, influence, type, and top genre per user
-- =======================================================
select
    ua.user_id,
    ua.user_rating_count,
    ua.avg_user_rating,
    ua.first_activity_date,
    ua.last_activity_date,
    ui.influence_score,
    ut.user_category,
    tg.top_genre
from user_activity ua
join user_influence ui on ua.user_id = ui.user_id
join user_type ut on ua.user_id = ut.user_id
left join top_genre tg on ua.user_id = tg.user_id


  where ua.user_id not in (select user_id from `workspace`.`movielens_volume`.`user_activity`)

  
[0m06:58:35.662919 [debug] [Thread-4  ]: SQL status: OK in 0.4399999976158142 seconds
[0m06:58:35.673100 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m06:58:35.673948 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m06:58:36.057318 [debug] [Thread-4  ]: SQL status: OK in 0.3799999952316284 seconds
[0m06:58:36.061831 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_activity"
[0m06:58:36.070176 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m06:58:36.070742 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_activity` as DBT_INTERNAL_DEST
      using `user_activity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m06:58:41.362973 [debug] [Thread-4  ]: SQL status: OK in 5.289999961853027 seconds
[0m06:58:41.498085 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (execute): 06:58:34.034487 => 06:58:41.497756
[0m06:58:41.498890 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: ROLLBACK
[0m06:58:41.499627 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m06:58:41.500394 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: Close
[0m06:58:41.609060 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e1aa0c3c-50f4-4a6d-8e99-0ba808aea084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff8583220>]}
[0m06:58:41.610238 [info ] [Thread-4  ]: 4 of 5 OK created sql incremental model movielens_volume.user_activity ......... [[32mOK[0m in 7.59s]
[0m06:58:41.611336 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_activity
[0m06:58:41.612006 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_preferences
[0m06:58:41.612705 [info ] [Thread-4  ]: 5 of 5 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m06:58:41.613863 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.user_activity, now model.my_dbt_project.user_preferences)
[0m06:58:41.614550 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_preferences
[0m06:58:41.621326 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m06:58:41.629680 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (compile): 06:58:41.615072 => 06:58:41.629283
[0m06:58:41.630361 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_preferences
[0m06:58:41.636841 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:58:41.637490 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m06:58:41.638152 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m06:58:41.638791 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m06:58:42.739518 [debug] [Thread-4  ]: SQL status: OK in 1.100000023841858 seconds
[0m06:58:42.744237 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m06:58:42.745092 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create temporary view `user_preferences__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score,  -- influence based on total ratings
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )

  
[0m06:58:43.290404 [debug] [Thread-4  ]: SQL status: OK in 0.5400000214576721 seconds
[0m06:58:43.295071 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m06:58:43.295674 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m06:58:43.622675 [debug] [Thread-4  ]: SQL status: OK in 0.33000001311302185 seconds
[0m06:58:43.627150 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m06:58:43.635313 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m06:58:43.636050 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
      using `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m06:58:52.786351 [debug] [Thread-4  ]: SQL status: OK in 9.149999618530273 seconds
[0m06:58:52.921276 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (execute): 06:58:41.630822 => 06:58:52.920944
[0m06:58:52.922168 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: ROLLBACK
[0m06:58:52.922866 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m06:58:52.923624 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: Close
[0m06:58:53.037779 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e1aa0c3c-50f4-4a6d-8e99-0ba808aea084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff8646b20>]}
[0m06:58:53.039101 [info ] [Thread-4  ]: 5 of 5 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 11.42s]
[0m06:58:53.040440 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_preferences
[0m06:58:53.043457 [debug] [MainThread]: On master: ROLLBACK
[0m06:58:53.044107 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:58:53.392592 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m06:58:53.393545 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:58:53.394267 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:58:53.394942 [debug] [MainThread]: On master: ROLLBACK
[0m06:58:53.395594 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m06:58:53.396270 [debug] [MainThread]: On master: Close
[0m06:58:53.505862 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:58:53.506609 [debug] [MainThread]: Connection 'model.my_dbt_project.user_preferences' was properly closed.
[0m06:58:53.508992 [info ] [MainThread]: 
[0m06:58:53.510509 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 51.24 seconds (51.24s).
[0m06:58:53.512739 [debug] [MainThread]: Command end result
[0m06:58:53.542311 [info ] [MainThread]: 
[0m06:58:53.543217 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:58:53.543891 [info ] [MainThread]: 
[0m06:58:53.544722 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m06:58:53.546212 [debug] [MainThread]: Command `dbt run` succeeded at 06:58:53.546068 after 51.29 seconds
[0m06:58:53.547048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0021f83640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff92576d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efff856bca0>]}
[0m06:58:53.547776 [debug] [MainThread]: Flushing usage events
[0m06:58:57.645076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab131c35b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab1176f1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab1176f910>]}


============================== 06:58:57.651750 | 6ef563fb-86e0-4e68-b019-272deeeb6bad ==============================
[0m06:58:57.651750 [info ] [MainThread]: Running with dbt=1.5.2
[0m06:58:57.652944 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/opt/airflow/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m06:58:58.607508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6ef563fb-86e0-4e68-b019-272deeeb6bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab1176f160>]}
[0m06:58:58.636320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6ef563fb-86e0-4e68-b019-272deeeb6bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faae64d7c40>]}
[0m06:58:58.637168 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m06:58:58.676735 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m06:58:59.760758 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:58:59.761396 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:58:59.772367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6ef563fb-86e0-4e68-b019-272deeeb6bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faae648e400>]}
[0m06:58:59.802560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6ef563fb-86e0-4e68-b019-272deeeb6bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faae63b8430>]}
[0m06:58:59.803505 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m06:58:59.804373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6ef563fb-86e0-4e68-b019-272deeeb6bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faae63b8490>]}
[0m06:58:59.807723 [info ] [MainThread]: 
[0m06:58:59.809081 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:58:59.811369 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m06:58:59.817722 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m06:58:59.818317 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m06:58:59.818897 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:59:00.445704 [debug] [ThreadPool]: SQL status: OK in 0.6299999952316284 seconds
[0m06:59:00.451903 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m06:59:00.557138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6ef563fb-86e0-4e68-b019-272deeeb6bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faae63b8310>]}
[0m06:59:00.558658 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:00.559429 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:59:00.560911 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:59:00.561875 [info ] [MainThread]: 
[0m06:59:00.579899 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m06:59:00.580625 [info ] [Thread-2  ]: 1 of 22 START test accepted_rating_range_silver_ratings_rating__5__0 ........... [RUN]
[0m06:59:00.581827 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff)
[0m06:59:00.582452 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m06:59:00.594861 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m06:59:00.604781 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (compile): 06:59:00.582935 => 06:59:00.604356
[0m06:59:00.605666 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m06:59:00.634822 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m06:59:00.644541 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:00.645185 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m06:59:00.645776 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`silver_ratings`
where rating < 0
   or rating > 5


      
    ) dbt_internal_test
[0m06:59:00.646386 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:01.974243 [debug] [Thread-2  ]: SQL status: OK in 1.3300000429153442 seconds
[0m06:59:01.987013 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (execute): 06:59:00.606155 => 06:59:01.986725
[0m06:59:01.987698 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: ROLLBACK
[0m06:59:01.988350 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:01.988970 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: Close
[0m06:59:02.095157 [info ] [Thread-2  ]: 1 of 22 PASS accepted_rating_range_silver_ratings_rating__5__0 ................. [[32mPASS[0m in 1.51s]
[0m06:59:02.096702 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m06:59:02.097643 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m06:59:02.098529 [info ] [Thread-2  ]: 2 of 22 START test not_null_silver_links_imdb_id ............................... [RUN]
[0m06:59:02.099773 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff, now test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51)
[0m06:59:02.100391 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m06:59:02.110186 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m06:59:02.119243 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (compile): 06:59:02.100862 => 06:59:02.118962
[0m06:59:02.119845 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m06:59:02.123413 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m06:59:02.132528 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:02.133177 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m06:59:02.133968 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



      
    ) dbt_internal_test
[0m06:59:02.134645 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:03.052641 [debug] [Thread-2  ]: SQL status: OK in 0.9200000166893005 seconds
[0m06:59:03.056944 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (execute): 06:59:02.120257 => 06:59:03.056624
[0m06:59:03.057776 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: ROLLBACK
[0m06:59:03.058668 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:03.059340 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m06:59:03.171339 [info ] [Thread-2  ]: 2 of 22 PASS not_null_silver_links_imdb_id ..................................... [[32mPASS[0m in 1.07s]
[0m06:59:03.172697 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m06:59:03.173608 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m06:59:03.174395 [info ] [Thread-2  ]: 3 of 22 START test not_null_silver_links_movie_id .............................. [RUN]
[0m06:59:03.175751 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51, now test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d)
[0m06:59:03.176406 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m06:59:03.185012 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m06:59:03.193716 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (compile): 06:59:03.177007 => 06:59:03.193366
[0m06:59:03.194353 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m06:59:03.198151 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m06:59:03.206300 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:03.206789 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m06:59:03.207320 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



      
    ) dbt_internal_test
[0m06:59:03.207913 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:04.039824 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m06:59:04.044123 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (execute): 06:59:03.194906 => 06:59:04.043772
[0m06:59:04.045150 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: ROLLBACK
[0m06:59:04.045944 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:04.046682 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m06:59:04.168087 [info ] [Thread-2  ]: 3 of 22 PASS not_null_silver_links_movie_id .................................... [[32mPASS[0m in 0.99s]
[0m06:59:04.169565 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m06:59:04.170476 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m06:59:04.171271 [info ] [Thread-2  ]: 4 of 22 START test not_null_silver_links_tmdb_id ............................... [RUN]
[0m06:59:04.172436 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d, now test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f)
[0m06:59:04.173186 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m06:59:04.184738 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m06:59:04.204256 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (compile): 06:59:04.173831 => 06:59:04.203861
[0m06:59:04.205088 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m06:59:04.209093 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m06:59:04.218555 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:04.219133 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m06:59:04.219674 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



      
    ) dbt_internal_test
[0m06:59:04.220266 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:05.125252 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m06:59:05.129359 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (execute): 06:59:04.205583 => 06:59:05.129050
[0m06:59:05.130175 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: ROLLBACK
[0m06:59:05.131011 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:05.131709 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m06:59:05.239017 [info ] [Thread-2  ]: 4 of 22 PASS not_null_silver_links_tmdb_id ..................................... [[32mPASS[0m in 1.07s]
[0m06:59:05.240649 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m06:59:05.241499 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m06:59:05.242278 [info ] [Thread-2  ]: 5 of 22 START test not_null_silver_movies_genres ............................... [RUN]
[0m06:59:05.243486 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f, now test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63)
[0m06:59:05.244128 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m06:59:05.252351 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m06:59:05.261437 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (compile): 06:59:05.244663 => 06:59:05.261105
[0m06:59:05.262116 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m06:59:05.265746 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m06:59:05.274168 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:05.274806 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m06:59:05.275379 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



      
    ) dbt_internal_test
[0m06:59:05.276007 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:06.178620 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m06:59:06.182721 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (execute): 06:59:05.262623 => 06:59:06.182411
[0m06:59:06.183465 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: ROLLBACK
[0m06:59:06.184113 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:06.184812 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m06:59:06.293757 [info ] [Thread-2  ]: 5 of 22 PASS not_null_silver_movies_genres ..................................... [[32mPASS[0m in 1.05s]
[0m06:59:06.295305 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m06:59:06.296194 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m06:59:06.296954 [info ] [Thread-2  ]: 6 of 22 START test not_null_silver_movies_movie_id ............................. [RUN]
[0m06:59:06.298258 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63, now test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e)
[0m06:59:06.298951 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m06:59:06.305223 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m06:59:06.314079 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (compile): 06:59:06.299495 => 06:59:06.313752
[0m06:59:06.314674 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m06:59:06.318149 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m06:59:06.326895 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:06.327458 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m06:59:06.328084 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



      
    ) dbt_internal_test
[0m06:59:06.328772 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:07.177508 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m06:59:07.181495 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (execute): 06:59:06.315127 => 06:59:07.181193
[0m06:59:07.182128 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: ROLLBACK
[0m06:59:07.182740 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:07.183428 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m06:59:07.292021 [info ] [Thread-2  ]: 6 of 22 PASS not_null_silver_movies_movie_id ................................... [[32mPASS[0m in 0.99s]
[0m06:59:07.293404 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m06:59:07.294307 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m06:59:07.295143 [info ] [Thread-2  ]: 7 of 22 START test not_null_silver_movies_title ................................ [RUN]
[0m06:59:07.296468 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e, now test.my_dbt_project.not_null_silver_movies_title.3a779bd2de)
[0m06:59:07.297206 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m06:59:07.308665 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m06:59:07.318098 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (compile): 06:59:07.297719 => 06:59:07.317720
[0m06:59:07.318790 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m06:59:07.322668 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m06:59:07.331658 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:07.332396 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m06:59:07.332945 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



      
    ) dbt_internal_test
[0m06:59:07.333624 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:08.186609 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m06:59:08.191183 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (execute): 06:59:07.319234 => 06:59:08.190803
[0m06:59:08.191983 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: ROLLBACK
[0m06:59:08.192672 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:08.193299 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m06:59:08.296016 [info ] [Thread-2  ]: 7 of 22 PASS not_null_silver_movies_title ...................................... [[32mPASS[0m in 1.00s]
[0m06:59:08.297459 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m06:59:08.298300 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m06:59:08.299009 [info ] [Thread-2  ]: 8 of 22 START test not_null_silver_ratings_movie_id ............................ [RUN]
[0m06:59:08.300133 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_title.3a779bd2de, now test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36)
[0m06:59:08.300822 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m06:59:08.307058 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m06:59:08.319801 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (compile): 06:59:08.301447 => 06:59:08.319523
[0m06:59:08.320470 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m06:59:08.324127 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m06:59:08.332555 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:08.333046 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m06:59:08.333657 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



      
    ) dbt_internal_test
[0m06:59:08.334338 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:09.160125 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m06:59:09.164582 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (execute): 06:59:08.320994 => 06:59:09.164238
[0m06:59:09.165431 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: ROLLBACK
[0m06:59:09.166299 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:09.167135 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m06:59:09.275115 [info ] [Thread-2  ]: 8 of 22 PASS not_null_silver_ratings_movie_id .................................. [[32mPASS[0m in 0.97s]
[0m06:59:09.276523 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m06:59:09.277324 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m06:59:09.278066 [info ] [Thread-2  ]: 9 of 22 START test not_null_silver_ratings_rating .............................. [RUN]
[0m06:59:09.279214 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36, now test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a)
[0m06:59:09.279908 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m06:59:09.286062 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m06:59:09.295205 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (compile): 06:59:09.280534 => 06:59:09.294874
[0m06:59:09.295959 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m06:59:09.299737 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m06:59:09.307897 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:09.308433 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m06:59:09.309077 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



      
    ) dbt_internal_test
[0m06:59:09.309792 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:10.169507 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m06:59:10.173539 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (execute): 06:59:09.296496 => 06:59:10.173235
[0m06:59:10.174249 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: ROLLBACK
[0m06:59:10.175071 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:10.175697 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m06:59:10.288126 [info ] [Thread-2  ]: 9 of 22 PASS not_null_silver_ratings_rating .................................... [[32mPASS[0m in 1.01s]
[0m06:59:10.289602 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m06:59:10.290477 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m06:59:10.291209 [info ] [Thread-2  ]: 10 of 22 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m06:59:10.292506 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a, now test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f)
[0m06:59:10.293195 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m06:59:10.306575 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m06:59:10.313687 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (compile): 06:59:10.293720 => 06:59:10.313351
[0m06:59:10.314352 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m06:59:10.318193 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m06:59:10.324696 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:10.325286 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m06:59:10.325789 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



      
    ) dbt_internal_test
[0m06:59:10.326390 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:11.206113 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m06:59:11.210220 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (execute): 06:59:10.314836 => 06:59:11.209892
[0m06:59:11.210978 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: ROLLBACK
[0m06:59:11.211800 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:11.212543 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m06:59:11.319094 [info ] [Thread-2  ]: 10 of 22 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 1.03s]
[0m06:59:11.320455 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m06:59:11.321368 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m06:59:11.322153 [info ] [Thread-2  ]: 11 of 22 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m06:59:11.323613 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f, now test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3)
[0m06:59:11.324275 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m06:59:11.332526 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m06:59:11.341565 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (compile): 06:59:11.324783 => 06:59:11.341185
[0m06:59:11.342286 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m06:59:11.346056 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m06:59:11.364176 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:11.364806 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m06:59:11.365417 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



      
    ) dbt_internal_test
[0m06:59:11.366060 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:12.141569 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m06:59:12.145646 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (execute): 06:59:11.342783 => 06:59:12.145340
[0m06:59:12.146444 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: ROLLBACK
[0m06:59:12.147205 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:12.147902 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m06:59:12.250291 [info ] [Thread-2  ]: 11 of 22 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 0.93s]
[0m06:59:12.251685 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m06:59:12.252571 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m06:59:12.253372 [info ] [Thread-2  ]: 12 of 22 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m06:59:12.254689 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3, now test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3)
[0m06:59:12.255572 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m06:59:12.263886 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m06:59:12.273568 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (compile): 06:59:12.256129 => 06:59:12.273260
[0m06:59:12.274219 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m06:59:12.278061 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m06:59:12.286887 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:12.287443 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m06:59:12.287997 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



      
    ) dbt_internal_test
[0m06:59:12.288671 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:13.114932 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m06:59:13.119015 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (execute): 06:59:12.274751 => 06:59:13.118629
[0m06:59:13.119880 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: ROLLBACK
[0m06:59:13.120590 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:13.121261 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m06:59:13.229154 [info ] [Thread-2  ]: 12 of 22 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 0.97s]
[0m06:59:13.230470 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m06:59:13.231264 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m06:59:13.232094 [info ] [Thread-2  ]: 13 of 22 START test not_null_silver_tags_tag ................................... [RUN]
[0m06:59:13.233630 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3, now test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8)
[0m06:59:13.234382 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m06:59:13.248585 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m06:59:13.257698 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (compile): 06:59:13.234960 => 06:59:13.257253
[0m06:59:13.258622 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m06:59:13.263363 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m06:59:13.272503 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:13.273312 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m06:59:13.273966 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



      
    ) dbt_internal_test
[0m06:59:13.274717 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:14.158774 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m06:59:14.162880 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (execute): 06:59:13.259124 => 06:59:14.162480
[0m06:59:14.163667 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: ROLLBACK
[0m06:59:14.164273 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:14.164951 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m06:59:14.269347 [info ] [Thread-2  ]: 13 of 22 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 1.04s]
[0m06:59:14.270651 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m06:59:14.271451 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m06:59:14.272172 [info ] [Thread-2  ]: 14 of 22 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m06:59:14.273279 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8, now test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86)
[0m06:59:14.273883 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m06:59:14.280119 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m06:59:14.290033 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (compile): 06:59:14.274387 => 06:59:14.289506
[0m06:59:14.290839 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m06:59:14.295073 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m06:59:14.303583 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:14.304199 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m06:59:14.304772 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



      
    ) dbt_internal_test
[0m06:59:14.305406 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:15.127154 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m06:59:15.131001 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (execute): 06:59:14.291327 => 06:59:15.130681
[0m06:59:15.131708 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: ROLLBACK
[0m06:59:15.132371 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:15.133034 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m06:59:15.230780 [info ] [Thread-2  ]: 14 of 22 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 0.96s]
[0m06:59:15.232144 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m06:59:15.232941 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m06:59:15.233593 [info ] [Thread-2  ]: 15 of 22 START test not_null_silver_tags_user_id ............................... [RUN]
[0m06:59:15.235125 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86, now test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3)
[0m06:59:15.235839 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m06:59:15.241496 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m06:59:15.249535 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (compile): 06:59:15.236316 => 06:59:15.249060
[0m06:59:15.250283 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m06:59:15.254340 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m06:59:15.262147 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:15.262771 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m06:59:15.263288 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



      
    ) dbt_internal_test
[0m06:59:15.263897 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:16.158447 [debug] [Thread-2  ]: SQL status: OK in 0.8899999856948853 seconds
[0m06:59:16.162558 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (execute): 06:59:15.250777 => 06:59:16.162215
[0m06:59:16.163272 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: ROLLBACK
[0m06:59:16.163893 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:16.164450 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m06:59:16.274015 [info ] [Thread-2  ]: 15 of 22 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 1.04s]
[0m06:59:16.275341 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m06:59:16.276098 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m06:59:16.276741 [info ] [Thread-2  ]: 16 of 22 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m06:59:16.277951 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m06:59:16.278618 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m06:59:16.297414 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m06:59:16.307440 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 06:59:16.279176 => 06:59:16.307058
[0m06:59:16.308172 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m06:59:16.311826 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m06:59:16.320847 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:16.321421 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m06:59:16.321963 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m06:59:16.322548 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:17.934509 [debug] [Thread-2  ]: SQL status: OK in 1.6100000143051147 seconds
[0m06:59:17.938688 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 06:59:16.308641 => 06:59:17.938357
[0m06:59:17.939450 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m06:59:17.940298 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:17.940916 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m06:59:18.051387 [info ] [Thread-2  ]: 16 of 22 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.77s]
[0m06:59:18.052816 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m06:59:18.053833 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m06:59:18.054804 [info ] [Thread-2  ]: 17 of 22 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m06:59:18.056255 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m06:59:18.056938 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m06:59:18.065057 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m06:59:18.074554 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 06:59:18.057464 => 06:59:18.074194
[0m06:59:18.075325 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m06:59:18.079085 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m06:59:18.088025 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:18.088549 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m06:59:18.089143 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m06:59:18.089744 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:20.958594 [debug] [Thread-2  ]: SQL status: OK in 2.869999885559082 seconds
[0m06:59:20.962363 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 06:59:18.075870 => 06:59:20.962018
[0m06:59:20.963090 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m06:59:20.963755 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:20.964376 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m06:59:21.071112 [info ] [Thread-2  ]: 17 of 22 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 3.02s]
[0m06:59:21.072539 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m06:59:21.073354 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m06:59:21.074141 [info ] [Thread-2  ]: 18 of 22 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m06:59:21.075469 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32)
[0m06:59:21.076204 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m06:59:21.084802 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m06:59:21.095215 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (compile): 06:59:21.076751 => 06:59:21.094691
[0m06:59:21.096041 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m06:59:21.099825 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m06:59:21.109242 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:21.109949 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m06:59:21.110569 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m06:59:21.111238 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:22.182380 [debug] [Thread-2  ]: SQL status: OK in 1.0700000524520874 seconds
[0m06:59:22.186228 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (execute): 06:59:21.096497 => 06:59:22.185900
[0m06:59:22.187056 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: ROLLBACK
[0m06:59:22.187704 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:22.188248 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m06:59:22.308459 [info ] [Thread-2  ]: 18 of 22 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.23s]
[0m06:59:22.309878 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m06:59:22.310652 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m06:59:22.311349 [info ] [Thread-2  ]: 19 of 22 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m06:59:22.312610 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32, now test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd)
[0m06:59:22.313381 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m06:59:22.329387 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m06:59:22.338556 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (compile): 06:59:22.313859 => 06:59:22.338117
[0m06:59:22.339345 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m06:59:22.343070 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m06:59:22.351168 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:22.351725 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m06:59:22.352261 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m06:59:22.352921 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:23.663164 [debug] [Thread-2  ]: SQL status: OK in 1.309999942779541 seconds
[0m06:59:23.666972 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (execute): 06:59:22.339850 => 06:59:23.666686
[0m06:59:23.667719 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: ROLLBACK
[0m06:59:23.668455 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:23.669027 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m06:59:23.779026 [info ] [Thread-2  ]: 19 of 22 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.47s]
[0m06:59:23.780425 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m06:59:23.781271 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m06:59:23.782193 [info ] [Thread-2  ]: 20 of 22 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m06:59:23.783511 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd, now test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719)
[0m06:59:23.784236 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m06:59:23.792606 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m06:59:23.801349 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (compile): 06:59:23.784758 => 06:59:23.800997
[0m06:59:23.802038 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m06:59:23.805751 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m06:59:23.813982 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:23.814473 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m06:59:23.815031 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m06:59:23.815727 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:25.228242 [debug] [Thread-2  ]: SQL status: OK in 1.409999966621399 seconds
[0m06:59:25.232124 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (execute): 06:59:23.802569 => 06:59:25.231801
[0m06:59:25.232899 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: ROLLBACK
[0m06:59:25.233789 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:25.234504 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m06:59:25.353883 [info ] [Thread-2  ]: 20 of 22 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 1.57s]
[0m06:59:25.355290 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m06:59:25.356081 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m06:59:25.356789 [info ] [Thread-2  ]: 21 of 22 START test unique_silver_links_movie_id ............................... [RUN]
[0m06:59:25.357971 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719, now test.my_dbt_project.unique_silver_links_movie_id.c422609054)
[0m06:59:25.358672 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m06:59:25.367662 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m06:59:25.377242 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (compile): 06:59:25.359238 => 06:59:25.376937
[0m06:59:25.377843 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m06:59:25.381408 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m06:59:25.390046 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:25.390559 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m06:59:25.391101 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m06:59:25.391669 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:26.631586 [debug] [Thread-2  ]: SQL status: OK in 1.2400000095367432 seconds
[0m06:59:26.639511 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (execute): 06:59:25.378282 => 06:59:26.638984
[0m06:59:26.640441 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: ROLLBACK
[0m06:59:26.641384 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:26.642018 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m06:59:26.762566 [info ] [Thread-2  ]: 21 of 22 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 1.40s]
[0m06:59:26.764058 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m06:59:26.765049 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m06:59:26.765877 [info ] [Thread-2  ]: 22 of 22 START test unique_silver_movies_movie_id .............................. [RUN]
[0m06:59:26.767175 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_silver_links_movie_id.c422609054, now test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085)
[0m06:59:26.767820 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m06:59:26.775580 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m06:59:26.784988 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (compile): 06:59:26.768281 => 06:59:26.784586
[0m06:59:26.785768 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m06:59:26.800974 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m06:59:26.810282 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:26.810948 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m06:59:26.811599 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m06:59:26.812204 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:27.692873 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m06:59:27.696835 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (execute): 06:59:26.786230 => 06:59:27.696525
[0m06:59:27.697604 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: ROLLBACK
[0m06:59:27.698383 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:27.698998 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m06:59:27.803053 [info ] [Thread-2  ]: 22 of 22 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 1.04s]
[0m06:59:27.804298 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m06:59:27.807143 [debug] [MainThread]: On master: ROLLBACK
[0m06:59:27.807851 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:59:28.140782 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m06:59:28.141692 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:28.142364 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:59:28.143039 [debug] [MainThread]: On master: ROLLBACK
[0m06:59:28.143684 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m06:59:28.144309 [debug] [MainThread]: On master: Close
[0m06:59:28.250914 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:59:28.251689 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085' was properly closed.
[0m06:59:28.254138 [info ] [MainThread]: 
[0m06:59:28.255417 [info ] [MainThread]: Finished running 22 tests in 0 hours 0 minutes and 28.45 seconds (28.45s).
[0m06:59:28.260543 [debug] [MainThread]: Command end result
[0m06:59:28.292028 [info ] [MainThread]: 
[0m06:59:28.293016 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:59:28.293829 [info ] [MainThread]: 
[0m06:59:28.294650 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=0 SKIP=0 TOTAL=22
[0m06:59:28.296111 [debug] [MainThread]: Command `dbt test` succeeded at 06:59:28.295912 after 29.46 seconds
[0m06:59:28.296881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab131c35b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faae64d7c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faae661e400>]}
[0m06:59:28.297723 [debug] [MainThread]: Flushing usage events
[0m06:59:32.449278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9fe6035e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9fcbaf1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9fcbaf940>]}


============================== 06:59:32.456338 | 9e7232c8-7e8b-4da7-b94f-d59c901ebcc0 ==============================
[0m06:59:32.456338 [info ] [MainThread]: Running with dbt=1.5.2
[0m06:59:32.457284 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/opt/airflow/dbt', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m06:59:33.326629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9e7232c8-7e8b-4da7-b94f-d59c901ebcc0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9fcbaf190>]}
[0m06:59:33.354074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9e7232c8-7e8b-4da7-b94f-d59c901ebcc0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9d18ed9a0>]}
[0m06:59:33.355087 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m06:59:33.397419 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m06:59:34.498305 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:59:34.498944 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:59:34.509104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9e7232c8-7e8b-4da7-b94f-d59c901ebcc0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9d189e430>]}
[0m06:59:34.538730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9e7232c8-7e8b-4da7-b94f-d59c901ebcc0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9d17c8460>]}
[0m06:59:34.539666 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m06:59:34.540361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e7232c8-7e8b-4da7-b94f-d59c901ebcc0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9d17c84c0>]}
[0m06:59:34.544556 [info ] [MainThread]: 
[0m06:59:34.546002 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m06:59:34.548240 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m06:59:34.554056 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m06:59:34.554564 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m06:59:34.555053 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:59:35.139494 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m06:59:35.142555 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m06:59:35.250116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e7232c8-7e8b-4da7-b94f-d59c901ebcc0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9d17c8340>]}
[0m06:59:35.251129 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:35.251928 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:59:35.253025 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:59:35.253958 [info ] [MainThread]: 
[0m06:59:35.272432 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m06:59:35.273432 [info ] [Thread-2  ]: 1 of 35 START test accepted_rating_range_genre_popularity_avg_rating__5__0 ..... [RUN]
[0m06:59:35.274781 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413)
[0m06:59:35.275405 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m06:59:35.289525 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m06:59:35.298681 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (compile): 06:59:35.275929 => 06:59:35.298359
[0m06:59:35.299335 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m06:59:35.338921 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m06:59:35.351234 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:35.351937 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m06:59:35.352569 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m06:59:35.353278 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:36.456102 [debug] [Thread-2  ]: SQL status: OK in 1.100000023841858 seconds
[0m06:59:36.463981 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (execute): 06:59:35.299855 => 06:59:36.463609
[0m06:59:36.464846 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: ROLLBACK
[0m06:59:36.465578 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:36.466220 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: Close
[0m06:59:36.573295 [info ] [Thread-2  ]: 1 of 35 PASS accepted_rating_range_genre_popularity_avg_rating__5__0 ........... [[32mPASS[0m in 1.30s]
[0m06:59:36.574689 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m06:59:36.575491 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m06:59:36.576346 [info ] [Thread-2  ]: 2 of 35 START test accepted_rating_range_movie_enriched_avg_rating__5__0 ....... [RUN]
[0m06:59:36.577754 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413, now test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8)
[0m06:59:36.578478 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m06:59:36.584617 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m06:59:36.591447 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (compile): 06:59:36.579076 => 06:59:36.591171
[0m06:59:36.592099 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m06:59:36.595719 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m06:59:36.602408 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:36.602891 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m06:59:36.603411 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m06:59:36.603951 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:37.458863 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m06:59:37.462943 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (execute): 06:59:36.592562 => 06:59:37.462592
[0m06:59:37.463693 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: ROLLBACK
[0m06:59:37.464596 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:37.465319 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: Close
[0m06:59:37.569304 [info ] [Thread-2  ]: 2 of 35 PASS accepted_rating_range_movie_enriched_avg_rating__5__0 ............. [[32mPASS[0m in 0.99s]
[0m06:59:37.570818 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m06:59:37.571727 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m06:59:37.572464 [info ] [Thread-2  ]: 3 of 35 START test accepted_rating_range_movie_performance_weighted_avg_rating__5__0  [RUN]
[0m06:59:37.573870 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8, now test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083)
[0m06:59:37.574504 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m06:59:37.582258 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m06:59:37.589597 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (compile): 06:59:37.574899 => 06:59:37.589320
[0m06:59:37.590360 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m06:59:37.593930 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m06:59:37.600667 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:37.601288 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m06:59:37.601761 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating < 0
   or weighted_avg_rating > 5


      
    ) dbt_internal_test
[0m06:59:37.602261 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:38.351512 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m06:59:38.355644 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (execute): 06:59:37.590820 => 06:59:38.355274
[0m06:59:38.356441 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: ROLLBACK
[0m06:59:38.357204 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:38.357885 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: Close
[0m06:59:38.467833 [info ] [Thread-2  ]: 3 of 35 PASS accepted_rating_range_movie_performance_weighted_avg_rating__5__0 . [[32mPASS[0m in 0.89s]
[0m06:59:38.469189 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m06:59:38.470014 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m06:59:38.470778 [info ] [Thread-2  ]: 4 of 35 START test accepted_rating_range_user_activity_avg_user_rating__5__0 ... [RUN]
[0m06:59:38.472140 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083, now test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a)
[0m06:59:38.472864 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m06:59:38.483927 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m06:59:38.493052 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (compile): 06:59:38.473335 => 06:59:38.492662
[0m06:59:38.493716 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m06:59:38.497546 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m06:59:38.507377 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:38.508052 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m06:59:38.508643 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating < 0
   or avg_user_rating > 5


      
    ) dbt_internal_test
[0m06:59:38.509330 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:39.391427 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m06:59:39.395661 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (execute): 06:59:38.494162 => 06:59:39.395279
[0m06:59:39.396472 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: ROLLBACK
[0m06:59:39.397179 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:39.397861 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: Close
[0m06:59:39.501179 [info ] [Thread-2  ]: 4 of 35 PASS accepted_rating_range_user_activity_avg_user_rating__5__0 ......... [[32mPASS[0m in 1.03s]
[0m06:59:39.502572 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m06:59:39.503487 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m06:59:39.504271 [info ] [Thread-2  ]: 5 of 35 START test not_null_genre_popularity_avg_rating ........................ [RUN]
[0m06:59:39.505341 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a, now test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9)
[0m06:59:39.506051 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m06:59:39.515252 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m06:59:39.525320 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (compile): 06:59:39.506513 => 06:59:39.524966
[0m06:59:39.525969 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m06:59:39.529666 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m06:59:39.539439 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:39.539989 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m06:59:39.540501 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating is null



      
    ) dbt_internal_test
[0m06:59:39.541059 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:40.308302 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m06:59:40.312561 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (execute): 06:59:39.526423 => 06:59:40.312166
[0m06:59:40.313389 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: ROLLBACK
[0m06:59:40.314084 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:40.314738 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: Close
[0m06:59:40.421917 [info ] [Thread-2  ]: 5 of 35 PASS not_null_genre_popularity_avg_rating .............................. [[32mPASS[0m in 0.92s]
[0m06:59:40.423089 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m06:59:40.423853 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m06:59:40.424888 [info ] [Thread-2  ]: 6 of 35 START test not_null_genre_popularity_first_rating_date ................. [RUN]
[0m06:59:40.426504 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9, now test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32)
[0m06:59:40.427355 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m06:59:40.433500 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m06:59:40.442401 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (compile): 06:59:40.427843 => 06:59:40.442056
[0m06:59:40.443194 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m06:59:40.447018 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m06:59:40.455374 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:40.456073 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m06:59:40.456696 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where first_rating_date is null



      
    ) dbt_internal_test
[0m06:59:40.457374 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:41.265047 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m06:59:41.269246 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (execute): 06:59:40.443666 => 06:59:41.268940
[0m06:59:41.269999 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: ROLLBACK
[0m06:59:41.270763 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:41.271488 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: Close
[0m06:59:41.382699 [info ] [Thread-2  ]: 6 of 35 PASS not_null_genre_popularity_first_rating_date ....................... [[32mPASS[0m in 0.96s]
[0m06:59:41.384195 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m06:59:41.385077 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m06:59:41.385886 [info ] [Thread-2  ]: 7 of 35 START test not_null_genre_popularity_genres ............................ [RUN]
[0m06:59:41.387266 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32, now test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa)
[0m06:59:41.387961 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m06:59:41.402219 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m06:59:41.411728 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (compile): 06:59:41.388483 => 06:59:41.411298
[0m06:59:41.412487 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m06:59:41.416705 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m06:59:41.425790 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:41.426433 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m06:59:41.427011 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`genre_popularity`
where genres is null



      
    ) dbt_internal_test
[0m06:59:41.427727 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:42.194311 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m06:59:42.203545 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (execute): 06:59:41.413020 => 06:59:42.202889
[0m06:59:42.204448 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: ROLLBACK
[0m06:59:42.205414 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:42.206088 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: Close
[0m06:59:42.315465 [info ] [Thread-2  ]: 7 of 35 PASS not_null_genre_popularity_genres .................................. [[32mPASS[0m in 0.93s]
[0m06:59:42.316989 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m06:59:42.317966 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m06:59:42.318865 [info ] [Thread-2  ]: 8 of 35 START test not_null_genre_popularity_last_rating_date .................. [RUN]
[0m06:59:42.320513 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa, now test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432)
[0m06:59:42.321373 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m06:59:42.332458 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m06:59:42.341704 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (compile): 06:59:42.322028 => 06:59:42.341268
[0m06:59:42.342436 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m06:59:42.346639 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m06:59:42.355401 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:42.356064 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m06:59:42.356666 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where last_rating_date is null



      
    ) dbt_internal_test
[0m06:59:42.357341 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:43.101210 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m06:59:43.105174 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (execute): 06:59:42.342905 => 06:59:43.104853
[0m06:59:43.105957 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: ROLLBACK
[0m06:59:43.106725 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:43.107405 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: Close
[0m06:59:43.216914 [info ] [Thread-2  ]: 8 of 35 PASS not_null_genre_popularity_last_rating_date ........................ [[32mPASS[0m in 0.90s]
[0m06:59:43.218340 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m06:59:43.219120 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m06:59:43.219972 [info ] [Thread-2  ]: 9 of 35 START test not_null_genre_popularity_popularity_rank ................... [RUN]
[0m06:59:43.221266 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432, now test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc)
[0m06:59:43.222030 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m06:59:43.230117 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m06:59:43.238561 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (compile): 06:59:43.222533 => 06:59:43.238286
[0m06:59:43.239156 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m06:59:43.242651 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m06:59:43.250839 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:43.251458 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m06:59:43.251999 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select popularity_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where popularity_rank is null



      
    ) dbt_internal_test
[0m06:59:43.252641 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:44.087381 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m06:59:44.091706 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (execute): 06:59:43.239581 => 06:59:44.091356
[0m06:59:44.092470 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: ROLLBACK
[0m06:59:44.093209 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:44.094013 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: Close
[0m06:59:44.228613 [info ] [Thread-2  ]: 9 of 35 PASS not_null_genre_popularity_popularity_rank ......................... [[32mPASS[0m in 1.01s]
[0m06:59:44.230018 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m06:59:44.230867 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m06:59:44.231741 [info ] [Thread-2  ]: 10 of 35 START test not_null_genre_popularity_quality_rank ..................... [RUN]
[0m06:59:44.233151 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc, now test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a)
[0m06:59:44.233882 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m06:59:44.243902 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m06:59:44.256697 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (compile): 06:59:44.234413 => 06:59:44.256322
[0m06:59:44.257327 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m06:59:44.260755 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m06:59:44.269377 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:44.269913 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m06:59:44.270448 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select quality_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where quality_rank is null



      
    ) dbt_internal_test
[0m06:59:44.271023 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:45.027660 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m06:59:45.031959 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (execute): 06:59:44.257751 => 06:59:45.031650
[0m06:59:45.032671 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: ROLLBACK
[0m06:59:45.033344 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:45.034041 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: Close
[0m06:59:45.152014 [info ] [Thread-2  ]: 10 of 35 PASS not_null_genre_popularity_quality_rank ........................... [[32mPASS[0m in 0.92s]
[0m06:59:45.153346 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m06:59:45.154169 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m06:59:45.154944 [info ] [Thread-2  ]: 11 of 35 START test not_null_genre_popularity_rating_count ..................... [RUN]
[0m06:59:45.156162 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a, now test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856)
[0m06:59:45.156815 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m06:59:45.163078 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m06:59:45.171708 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (compile): 06:59:45.157325 => 06:59:45.171208
[0m06:59:45.172447 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m06:59:45.176577 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m06:59:45.185850 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:45.186530 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m06:59:45.187167 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`genre_popularity`
where rating_count is null



      
    ) dbt_internal_test
[0m06:59:45.187964 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:46.008812 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m06:59:46.012868 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (execute): 06:59:45.172873 => 06:59:46.012541
[0m06:59:46.013597 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: ROLLBACK
[0m06:59:46.014207 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:46.014841 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: Close
[0m06:59:46.133383 [info ] [Thread-2  ]: 11 of 35 PASS not_null_genre_popularity_rating_count ........................... [[32mPASS[0m in 0.98s]
[0m06:59:46.134555 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m06:59:46.135273 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m06:59:46.135993 [info ] [Thread-2  ]: 12 of 35 START test not_null_movie_enriched_avg_rating ......................... [RUN]
[0m06:59:46.137050 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856, now test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd)
[0m06:59:46.137696 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m06:59:46.143577 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m06:59:46.152848 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (compile): 06:59:46.138202 => 06:59:46.152409
[0m06:59:46.153578 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m06:59:46.157772 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m06:59:46.166855 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:46.167552 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m06:59:46.168141 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating is null



      
    ) dbt_internal_test
[0m06:59:46.168833 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:46.902412 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m06:59:46.906605 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (execute): 06:59:46.154139 => 06:59:46.906257
[0m06:59:46.907364 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: ROLLBACK
[0m06:59:46.907972 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:46.908498 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: Close
[0m06:59:47.025387 [info ] [Thread-2  ]: 12 of 35 PASS not_null_movie_enriched_avg_rating ............................... [[32mPASS[0m in 0.89s]
[0m06:59:47.026814 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m06:59:47.027630 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m06:59:47.028277 [info ] [Thread-2  ]: 13 of 35 START test not_null_movie_enriched_genres ............................. [RUN]
[0m06:59:47.029745 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd, now test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27)
[0m06:59:47.030322 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m06:59:47.036512 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m06:59:47.043770 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (compile): 06:59:47.030704 => 06:59:47.043324
[0m06:59:47.044581 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m06:59:47.057796 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m06:59:47.065129 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:47.065797 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m06:59:47.066402 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`movie_enriched`
where genres is null



      
    ) dbt_internal_test
[0m06:59:47.066999 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:47.934036 [debug] [Thread-2  ]: SQL status: OK in 0.8700000047683716 seconds
[0m06:59:47.938169 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (execute): 06:59:47.045098 => 06:59:47.937857
[0m06:59:47.938909 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: ROLLBACK
[0m06:59:47.939601 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:47.940205 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: Close
[0m06:59:48.050091 [info ] [Thread-2  ]: 13 of 35 PASS not_null_movie_enriched_genres ................................... [[32mPASS[0m in 1.02s]
[0m06:59:48.051411 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m06:59:48.052192 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m06:59:48.052844 [info ] [Thread-2  ]: 14 of 35 START test not_null_movie_enriched_movie_id ........................... [RUN]
[0m06:59:48.053985 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27, now test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86)
[0m06:59:48.054591 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m06:59:48.060517 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m06:59:48.070282 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (compile): 06:59:48.055108 => 06:59:48.069680
[0m06:59:48.070994 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m06:59:48.075179 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m06:59:48.084995 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:48.085724 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m06:59:48.086333 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is null



      
    ) dbt_internal_test
[0m06:59:48.087021 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:48.889351 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m06:59:48.893263 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (execute): 06:59:48.071526 => 06:59:48.892949
[0m06:59:48.894001 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: ROLLBACK
[0m06:59:48.894708 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:48.895366 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: Close
[0m06:59:48.996673 [info ] [Thread-2  ]: 14 of 35 PASS not_null_movie_enriched_movie_id ................................. [[32mPASS[0m in 0.94s]
[0m06:59:48.998054 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m06:59:48.998867 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m06:59:48.999626 [info ] [Thread-2  ]: 15 of 35 START test not_null_movie_enriched_title .............................. [RUN]
[0m06:59:49.000713 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86, now test.my_dbt_project.not_null_movie_enriched_title.d20f00c292)
[0m06:59:49.001428 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m06:59:49.007182 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m06:59:49.015338 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (compile): 06:59:49.001894 => 06:59:49.014891
[0m06:59:49.016002 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m06:59:49.019782 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m06:59:49.027301 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:49.027981 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m06:59:49.028505 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`movie_enriched`
where title is null



      
    ) dbt_internal_test
[0m06:59:49.029191 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:49.794686 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m06:59:49.798987 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (execute): 06:59:49.016381 => 06:59:49.798593
[0m06:59:49.799734 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: ROLLBACK
[0m06:59:49.800432 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:49.801131 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: Close
[0m06:59:50.214280 [info ] [Thread-2  ]: 15 of 35 PASS not_null_movie_enriched_title .................................... [[32mPASS[0m in 1.21s]
[0m06:59:50.215543 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m06:59:50.216274 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m06:59:50.216996 [info ] [Thread-2  ]: 16 of 35 START test not_null_movie_enriched_total_ratings ...................... [RUN]
[0m06:59:50.218225 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_title.d20f00c292, now test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04)
[0m06:59:50.218930 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m06:59:50.224741 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m06:59:50.234307 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (compile): 06:59:50.219392 => 06:59:50.233876
[0m06:59:50.235070 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m06:59:50.250710 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m06:59:50.261447 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:50.262467 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m06:59:50.263196 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select total_ratings
from `workspace`.`movielens_volume`.`movie_enriched`
where total_ratings is null



      
    ) dbt_internal_test
[0m06:59:50.264073 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:51.997781 [debug] [Thread-2  ]: SQL status: OK in 1.7300000190734863 seconds
[0m06:59:52.001582 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (execute): 06:59:50.235513 => 06:59:52.001251
[0m06:59:52.002220 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: ROLLBACK
[0m06:59:52.002791 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:52.003363 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: Close
[0m06:59:52.145243 [info ] [Thread-2  ]: 16 of 35 PASS not_null_movie_enriched_total_ratings ............................ [[32mPASS[0m in 1.93s]
[0m06:59:52.146469 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m06:59:52.147261 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m06:59:52.147981 [info ] [Thread-2  ]: 17 of 35 START test not_null_movie_performance_genre_rank ...................... [RUN]
[0m06:59:52.149256 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04, now test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e)
[0m06:59:52.149971 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m06:59:52.158286 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m06:59:52.167104 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (compile): 06:59:52.150546 => 06:59:52.166752
[0m06:59:52.167765 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m06:59:52.171588 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m06:59:52.180637 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:52.181289 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m06:59:52.181936 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genre_rank
from `workspace`.`movielens_volume`.`movie_performance`
where genre_rank is null



      
    ) dbt_internal_test
[0m06:59:52.182521 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:53.012629 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m06:59:53.016693 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (execute): 06:59:52.168194 => 06:59:53.016375
[0m06:59:53.017452 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: ROLLBACK
[0m06:59:53.018215 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:53.018844 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: Close
[0m06:59:53.144154 [info ] [Thread-2  ]: 17 of 35 PASS not_null_movie_performance_genre_rank ............................ [[32mPASS[0m in 1.00s]
[0m06:59:53.145659 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m06:59:53.146593 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m06:59:53.147369 [info ] [Thread-2  ]: 18 of 35 START test not_null_movie_performance_movie_id ........................ [RUN]
[0m06:59:53.148696 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e, now test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52)
[0m06:59:53.149443 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m06:59:53.157694 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m06:59:53.170430 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (compile): 06:59:53.150043 => 06:59:53.170154
[0m06:59:53.171062 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m06:59:53.174549 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m06:59:53.183256 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:53.183773 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m06:59:53.184345 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is null



      
    ) dbt_internal_test
[0m06:59:53.184979 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:53.964811 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m06:59:53.968597 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (execute): 06:59:53.171469 => 06:59:53.968283
[0m06:59:53.969415 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: ROLLBACK
[0m06:59:53.970147 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:53.970873 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: Close
[0m06:59:54.078457 [info ] [Thread-2  ]: 18 of 35 PASS not_null_movie_performance_movie_id .............................. [[32mPASS[0m in 0.93s]
[0m06:59:54.079802 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m06:59:54.080644 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m06:59:54.081455 [info ] [Thread-2  ]: 19 of 35 START test not_null_movie_performance_polarization_score .............. [RUN]
[0m06:59:54.082798 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52, now test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6)
[0m06:59:54.083505 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m06:59:54.091915 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m06:59:54.100748 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (compile): 06:59:54.084083 => 06:59:54.100444
[0m06:59:54.101347 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m06:59:54.106289 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m06:59:54.124922 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:54.125513 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m06:59:54.126187 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select polarization_score
from `workspace`.`movielens_volume`.`movie_performance`
where polarization_score is null



      
    ) dbt_internal_test
[0m06:59:54.126897 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:54.902803 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m06:59:54.906600 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (execute): 06:59:54.101856 => 06:59:54.906254
[0m06:59:54.907492 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: ROLLBACK
[0m06:59:54.908410 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:54.909128 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: Close
[0m06:59:55.030982 [info ] [Thread-2  ]: 19 of 35 PASS not_null_movie_performance_polarization_score .................... [[32mPASS[0m in 0.95s]
[0m06:59:55.032515 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m06:59:55.033293 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m06:59:55.034161 [info ] [Thread-2  ]: 20 of 35 START test not_null_movie_performance_rating_count .................... [RUN]
[0m06:59:55.035344 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6, now test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac)
[0m06:59:55.036091 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m06:59:55.044301 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m06:59:55.053879 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (compile): 06:59:55.036636 => 06:59:55.053566
[0m06:59:55.054507 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m06:59:55.058202 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m06:59:55.066876 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:55.067450 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m06:59:55.068066 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`movie_performance`
where rating_count is null



      
    ) dbt_internal_test
[0m06:59:55.068755 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:55.917511 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m06:59:55.921866 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (execute): 06:59:55.054982 => 06:59:55.921551
[0m06:59:55.922717 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: ROLLBACK
[0m06:59:55.923533 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:55.924273 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: Close
[0m06:59:56.032060 [info ] [Thread-2  ]: 20 of 35 PASS not_null_movie_performance_rating_count .......................... [[32mPASS[0m in 1.00s]
[0m06:59:56.033628 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m06:59:56.034509 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m06:59:56.035212 [info ] [Thread-2  ]: 21 of 35 START test not_null_movie_performance_top_rating_percentage ........... [RUN]
[0m06:59:56.036446 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac, now test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb)
[0m06:59:56.037138 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m06:59:56.043609 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m06:59:56.053344 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (compile): 06:59:56.037688 => 06:59:56.053036
[0m06:59:56.053987 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m06:59:56.057664 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m06:59:56.066642 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:56.067185 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m06:59:56.067791 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_rating_percentage
from `workspace`.`movielens_volume`.`movie_performance`
where top_rating_percentage is null



      
    ) dbt_internal_test
[0m06:59:56.068402 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:56.923403 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m06:59:56.927556 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (execute): 06:59:56.054512 => 06:59:56.927245
[0m06:59:56.928310 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: ROLLBACK
[0m06:59:56.929175 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:56.929869 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: Close
[0m06:59:57.041573 [info ] [Thread-2  ]: 21 of 35 PASS not_null_movie_performance_top_rating_percentage ................. [[32mPASS[0m in 1.01s]
[0m06:59:57.042907 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m06:59:57.043679 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m06:59:57.044377 [info ] [Thread-2  ]: 22 of 35 START test not_null_movie_performance_weighted_avg_rating ............. [RUN]
[0m06:59:57.045503 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb, now test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5)
[0m06:59:57.046179 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m06:59:57.052914 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m06:59:57.072680 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (compile): 06:59:57.046705 => 06:59:57.072258
[0m06:59:57.073488 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m06:59:57.082901 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m06:59:57.091834 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:57.092465 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m06:59:57.093098 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select weighted_avg_rating
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating is null



      
    ) dbt_internal_test
[0m06:59:57.093795 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:57.903248 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m06:59:57.907731 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (execute): 06:59:57.074053 => 06:59:57.907351
[0m06:59:57.908539 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: ROLLBACK
[0m06:59:57.909390 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:57.910163 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: Close
[0m06:59:58.019079 [info ] [Thread-2  ]: 22 of 35 PASS not_null_movie_performance_weighted_avg_rating ................... [[32mPASS[0m in 0.97s]
[0m06:59:58.020681 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m06:59:58.021662 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m06:59:58.022411 [info ] [Thread-2  ]: 23 of 35 START test not_null_user_activity_avg_user_rating ..................... [RUN]
[0m06:59:58.023846 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5, now test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b)
[0m06:59:58.024575 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m06:59:58.030742 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m06:59:58.039896 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (compile): 06:59:58.025152 => 06:59:58.039607
[0m06:59:58.040537 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m06:59:58.044374 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m06:59:58.053433 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:58.054039 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m06:59:58.054612 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_user_rating
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating is null



      
    ) dbt_internal_test
[0m06:59:58.055523 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:58.801944 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m06:59:58.806147 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (execute): 06:59:58.040969 => 06:59:58.805836
[0m06:59:58.807014 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: ROLLBACK
[0m06:59:58.807796 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:58.808563 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: Close
[0m06:59:58.921443 [info ] [Thread-2  ]: 23 of 35 PASS not_null_user_activity_avg_user_rating ........................... [[32mPASS[0m in 0.90s]
[0m06:59:58.922828 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m06:59:58.923732 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m06:59:58.924660 [info ] [Thread-2  ]: 24 of 35 START test not_null_user_activity_first_activity_date ................. [RUN]
[0m06:59:58.925928 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b, now test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac)
[0m06:59:58.926634 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m06:59:58.932744 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m06:59:58.941169 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (compile): 06:59:58.927107 => 06:59:58.940837
[0m06:59:58.941747 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m06:59:58.945357 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m06:59:58.953974 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:58.954468 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m06:59:58.955051 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where first_activity_date is null



      
    ) dbt_internal_test
[0m06:59:58.955626 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m06:59:59.814145 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m06:59:59.818482 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (execute): 06:59:58.942168 => 06:59:59.818150
[0m06:59:59.819189 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: ROLLBACK
[0m06:59:59.819912 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m06:59:59.820617 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: Close
[0m06:59:59.933268 [info ] [Thread-2  ]: 24 of 35 PASS not_null_user_activity_first_activity_date ....................... [[32mPASS[0m in 1.01s]
[0m06:59:59.934820 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m06:59:59.935735 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m06:59:59.936492 [info ] [Thread-2  ]: 25 of 35 START test not_null_user_activity_influence_score ..................... [RUN]
[0m06:59:59.937658 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac, now test.my_dbt_project.not_null_user_activity_influence_score.758852d325)
[0m06:59:59.938286 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m06:59:59.944585 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m06:59:59.954108 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (compile): 06:59:59.938796 => 06:59:59.953661
[0m06:59:59.954856 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m06:59:59.966982 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m06:59:59.976345 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m06:59:59.976974 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m06:59:59.977473 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select influence_score
from `workspace`.`movielens_volume`.`user_activity`
where influence_score is null



      
    ) dbt_internal_test
[0m06:59:59.978083 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m07:00:00.795406 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m07:00:00.799586 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (execute): 06:59:59.955365 => 07:00:00.799155
[0m07:00:00.800365 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: ROLLBACK
[0m07:00:00.801070 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m07:00:00.801711 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: Close
[0m07:00:00.916811 [info ] [Thread-2  ]: 25 of 35 PASS not_null_user_activity_influence_score ........................... [[32mPASS[0m in 0.98s]
[0m07:00:00.918177 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m07:00:00.919119 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m07:00:00.920030 [info ] [Thread-2  ]: 26 of 35 START test not_null_user_activity_last_activity_date .................. [RUN]
[0m07:00:00.921186 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_influence_score.758852d325, now test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad)
[0m07:00:00.921871 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m07:00:00.930164 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m07:00:00.938975 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (compile): 07:00:00.922527 => 07:00:00.938622
[0m07:00:00.939722 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m07:00:00.943520 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m07:00:00.952348 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m07:00:00.952995 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m07:00:00.953616 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where last_activity_date is null



      
    ) dbt_internal_test
[0m07:00:00.954468 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m07:00:01.765688 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m07:00:01.769974 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (execute): 07:00:00.940226 => 07:00:01.769630
[0m07:00:01.770768 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: ROLLBACK
[0m07:00:01.771579 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m07:00:01.772466 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: Close
[0m07:00:01.881398 [info ] [Thread-2  ]: 26 of 35 PASS not_null_user_activity_last_activity_date ........................ [[32mPASS[0m in 0.96s]
[0m07:00:01.882572 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m07:00:01.883331 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m07:00:01.884117 [info ] [Thread-2  ]: 27 of 35 START test not_null_user_activity_top_genre ........................... [RUN]
[0m07:00:01.885395 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad, now test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313)
[0m07:00:01.886139 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m07:00:01.895375 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m07:00:01.904465 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (compile): 07:00:01.886683 => 07:00:01.904144
[0m07:00:01.905126 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m07:00:01.909058 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m07:00:01.917954 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m07:00:01.918550 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m07:00:01.919092 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_genre
from `workspace`.`movielens_volume`.`user_activity`
where top_genre is null



      
    ) dbt_internal_test
[0m07:00:01.919821 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m07:00:02.742208 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m07:00:02.746076 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (execute): 07:00:01.905626 => 07:00:02.745754
[0m07:00:02.746840 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: ROLLBACK
[0m07:00:02.747563 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m07:00:02.748223 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: Close
[0m07:00:02.848549 [info ] [Thread-2  ]: 27 of 35 PASS not_null_user_activity_top_genre ................................. [[32mPASS[0m in 0.96s]
[0m07:00:02.849978 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m07:00:02.850918 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m07:00:02.851815 [info ] [Thread-2  ]: 28 of 35 START test not_null_user_activity_user_id ............................. [RUN]
[0m07:00:02.853002 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313, now test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36)
[0m07:00:02.853703 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m07:00:02.861133 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m07:00:02.869422 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (compile): 07:00:02.854170 => 07:00:02.869092
[0m07:00:02.870058 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m07:00:02.874923 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m07:00:02.883749 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m07:00:02.884265 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m07:00:02.884784 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`user_activity`
where user_id is null



      
    ) dbt_internal_test
[0m07:00:02.885405 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m07:00:03.630820 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m07:00:03.634661 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (execute): 07:00:02.870480 => 07:00:03.634347
[0m07:00:03.635352 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: ROLLBACK
[0m07:00:03.636074 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m07:00:03.636733 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: Close
[0m07:00:03.733179 [info ] [Thread-2  ]: 28 of 35 PASS not_null_user_activity_user_id ................................... [[32mPASS[0m in 0.88s]
[0m07:00:03.734558 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m07:00:03.735380 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m07:00:03.736211 [info ] [Thread-2  ]: 29 of 35 START test not_null_user_activity_user_rating_count ................... [RUN]
[0m07:00:03.737337 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36, now test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e)
[0m07:00:03.738021 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m07:00:03.746707 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m07:00:03.755069 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (compile): 07:00:03.738580 => 07:00:03.754804
[0m07:00:03.755670 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m07:00:03.759035 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m07:00:03.767162 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m07:00:03.767696 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m07:00:03.768191 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_rating_count
from `workspace`.`movielens_volume`.`user_activity`
where user_rating_count is null



      
    ) dbt_internal_test
[0m07:00:03.768763 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m07:00:04.513409 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m07:00:04.517310 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (execute): 07:00:03.756079 => 07:00:04.517020
[0m07:00:04.517974 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: ROLLBACK
[0m07:00:04.518644 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m07:00:04.519306 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: Close
[0m07:00:04.616849 [info ] [Thread-2  ]: 29 of 35 PASS not_null_user_activity_user_rating_count ......................... [[32mPASS[0m in 0.88s]
[0m07:00:04.618118 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m07:00:04.618845 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m07:00:04.619770 [info ] [Thread-2  ]: 30 of 35 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m07:00:04.620957 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m07:00:04.621639 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m07:00:04.632235 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m07:00:04.640522 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 07:00:04.622091 => 07:00:04.640192
[0m07:00:04.641241 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m07:00:04.644549 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m07:00:04.652468 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m07:00:04.653019 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m07:00:04.653587 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m07:00:04.654170 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m07:00:05.213411 [debug] [Thread-2  ]: SQL status: OK in 0.5600000023841858 seconds
[0m07:00:05.217269 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 07:00:04.641753 => 07:00:05.216964
[0m07:00:05.218015 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m07:00:05.218741 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m07:00:05.219400 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m07:00:05.321994 [info ] [Thread-2  ]: 30 of 35 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.70s]
[0m07:00:05.323413 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m07:00:05.324297 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m07:00:05.325111 [info ] [Thread-2  ]: 31 of 35 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m07:00:05.326272 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m07:00:05.326873 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m07:00:05.334385 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m07:00:05.342925 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 07:00:05.327352 => 07:00:05.342621
[0m07:00:05.343564 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m07:00:05.346953 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m07:00:05.354949 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m07:00:05.355550 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m07:00:05.356050 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m07:00:05.356607 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m07:00:05.905427 [debug] [Thread-2  ]: SQL status: OK in 0.550000011920929 seconds
[0m07:00:05.909234 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 07:00:05.344001 => 07:00:05.908863
[0m07:00:05.909964 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m07:00:05.910718 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m07:00:05.911293 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m07:00:06.014251 [info ] [Thread-2  ]: 31 of 35 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.69s]
[0m07:00:06.015644 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m07:00:06.016396 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m07:00:06.017130 [info ] [Thread-2  ]: 32 of 35 START test unique_genre_popularity_genres ............................. [RUN]
[0m07:00:06.018313 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075)
[0m07:00:06.018899 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m07:00:06.027755 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m07:00:06.036065 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (compile): 07:00:06.019391 => 07:00:06.035755
[0m07:00:06.036685 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m07:00:06.040372 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m07:00:06.048935 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m07:00:06.049441 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m07:00:06.049915 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    genres as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`genre_popularity`
where genres is not null
group by genres
having count(*) > 1



      
    ) dbt_internal_test
[0m07:00:06.050408 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m07:00:06.861156 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m07:00:06.864949 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (execute): 07:00:06.037199 => 07:00:06.864640
[0m07:00:06.865744 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: ROLLBACK
[0m07:00:06.866404 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m07:00:06.867078 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: Close
[0m07:00:06.975511 [info ] [Thread-2  ]: 32 of 35 PASS unique_genre_popularity_genres ................................... [[32mPASS[0m in 0.96s]
[0m07:00:06.976579 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m07:00:06.977192 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m07:00:06.977812 [info ] [Thread-2  ]: 33 of 35 START test unique_movie_enriched_movie_id ............................. [RUN]
[0m07:00:06.978970 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075, now test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a)
[0m07:00:06.979656 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m07:00:06.984799 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m07:00:06.993572 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (compile): 07:00:06.980117 => 07:00:06.993274
[0m07:00:06.994189 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m07:00:06.997755 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m07:00:07.007901 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m07:00:07.008612 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m07:00:07.009184 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m07:00:07.009899 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m07:00:07.801221 [debug] [Thread-2  ]: SQL status: OK in 0.7900000214576721 seconds
[0m07:00:07.805419 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (execute): 07:00:06.994640 => 07:00:07.805088
[0m07:00:07.806150 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: ROLLBACK
[0m07:00:07.806762 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m07:00:07.807386 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m07:00:07.913010 [info ] [Thread-2  ]: 33 of 35 PASS unique_movie_enriched_movie_id ................................... [[32mPASS[0m in 0.93s]
[0m07:00:07.914810 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m07:00:07.915891 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m07:00:07.916678 [info ] [Thread-2  ]: 34 of 35 START test unique_movie_performance_movie_id .......................... [RUN]
[0m07:00:07.918006 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a, now test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22)
[0m07:00:07.918658 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m07:00:07.926865 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m07:00:07.938098 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (compile): 07:00:07.919609 => 07:00:07.937530
[0m07:00:07.939025 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m07:00:07.944554 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m07:00:07.956833 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m07:00:07.957695 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m07:00:07.958333 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m07:00:07.959147 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m07:00:08.740733 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m07:00:08.751119 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (execute): 07:00:07.939664 => 07:00:08.749681
[0m07:00:08.751909 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: ROLLBACK
[0m07:00:08.752522 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m07:00:08.753178 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: Close
[0m07:00:08.859034 [info ] [Thread-2  ]: 34 of 35 PASS unique_movie_performance_movie_id ................................ [[32mPASS[0m in 0.94s]
[0m07:00:08.860417 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m07:00:08.861169 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m07:00:08.861928 [info ] [Thread-2  ]: 35 of 35 START test unique_user_activity_user_id ............................... [RUN]
[0m07:00:08.863368 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22, now test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda)
[0m07:00:08.864038 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m07:00:08.870089 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m07:00:08.878982 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (compile): 07:00:08.864495 => 07:00:08.878649
[0m07:00:08.879667 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m07:00:08.883336 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m07:00:08.892001 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m07:00:08.892532 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m07:00:08.893119 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`user_activity`
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m07:00:08.893766 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m07:00:09.733229 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m07:00:09.737115 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (execute): 07:00:08.880141 => 07:00:09.736775
[0m07:00:09.737873 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: ROLLBACK
[0m07:00:09.738753 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m07:00:09.739575 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: Close
[0m07:00:09.850490 [info ] [Thread-2  ]: 35 of 35 PASS unique_user_activity_user_id ..................................... [[32mPASS[0m in 0.99s]
[0m07:00:09.851983 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m07:00:09.855221 [debug] [MainThread]: On master: ROLLBACK
[0m07:00:09.855844 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:00:10.201662 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m07:00:10.202830 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m07:00:10.203574 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m07:00:10.204209 [debug] [MainThread]: On master: ROLLBACK
[0m07:00:10.204810 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m07:00:10.205462 [debug] [MainThread]: On master: Close
[0m07:00:10.318128 [debug] [MainThread]: Connection 'master' was properly closed.
[0m07:00:10.318949 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda' was properly closed.
[0m07:00:10.321510 [info ] [MainThread]: 
[0m07:00:10.322830 [info ] [MainThread]: Finished running 35 tests in 0 hours 0 minutes and 35.78 seconds (35.78s).
[0m07:00:10.331166 [debug] [MainThread]: Command end result
[0m07:00:10.359547 [info ] [MainThread]: 
[0m07:00:10.360566 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:00:10.361344 [info ] [MainThread]: 
[0m07:00:10.362022 [info ] [MainThread]: Done. PASS=35 WARN=0 ERROR=0 SKIP=0 TOTAL=35
[0m07:00:10.363296 [debug] [MainThread]: Command `dbt test` succeeded at 07:00:10.363158 after 36.77 seconds
[0m07:00:10.363948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9fe6035e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9d15d04c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9d16830a0>]}
[0m07:00:10.364782 [debug] [MainThread]: Flushing usage events
[0m08:15:28.865088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfb54825b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfb3a2f220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfb3a2f940>]}


============================== 08:15:28.875372 | 083a842b-2ca5-4945-b47f-6eba68a2b841 ==============================
[0m08:15:28.875372 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:15:28.876752 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/opt/airflow/dbt', 'log_path': '/opt/airflow/dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:15:30.541197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '083a842b-2ca5-4945-b47f-6eba68a2b841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfb3a2f1c0>]}
[0m08:15:30.578676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '083a842b-2ca5-4945-b47f-6eba68a2b841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf8c788b80>]}
[0m08:15:30.579849 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:15:30.651029 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:15:32.870007 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:15:32.870782 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:15:32.886229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '083a842b-2ca5-4945-b47f-6eba68a2b841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf8c733400>]}
[0m08:15:32.925770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '083a842b-2ca5-4945-b47f-6eba68a2b841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf8c662430>]}
[0m08:15:32.926901 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:15:32.928190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '083a842b-2ca5-4945-b47f-6eba68a2b841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf8c662490>]}
[0m08:15:32.930917 [info ] [MainThread]: 
[0m08:15:32.932291 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:15:32.936864 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:15:32.937846 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:15:32.938455 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m08:15:32.939154 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:16:38.186230 [debug] [ThreadPool]: SQL status: OK in 65.25 seconds
[0m08:16:38.194931 [debug] [ThreadPool]: On list_workspace: Close
[0m08:16:38.326457 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m08:16:38.328969 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m08:16:38.358836 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m08:16:38.360090 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m08:16:38.361075 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m08:16:38.362397 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:16:40.216737 [debug] [ThreadPool]: SQL status: OK in 1.850000023841858 seconds
[0m08:16:40.219152 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m08:16:40.220255 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m08:16:40.221272 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m08:16:40.228768 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m08:16:40.351259 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m08:16:40.366807 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:16:40.367997 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:16:40.369488 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:16:41.236535 [debug] [ThreadPool]: SQL status: OK in 0.8700000047683716 seconds
[0m08:16:41.242720 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:16:41.363312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '083a842b-2ca5-4945-b47f-6eba68a2b841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf8c662160>]}
[0m08:16:41.364762 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:16:41.366559 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:16:41.368921 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:16:41.370240 [info ] [MainThread]: 
[0m08:16:41.407671 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_links
[0m08:16:41.409791 [info ] [Thread-4  ]: 1 of 4 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m08:16:41.415176 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.silver_links)
[0m08:16:41.418084 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_links
[0m08:16:41.454056 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m08:16:41.475849 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (compile): 08:16:41.421009 => 08:16:41.474773
[0m08:16:41.477377 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_links
[0m08:16:41.523579 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:16:41.524853 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m08:16:41.526069 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

      describe extended `workspace`.`movielens_volume`.`silver_links`
  
[0m08:16:41.527216 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:16:43.527330 [debug] [Thread-4  ]: SQL status: OK in 2.0 seconds
[0m08:16:43.660771 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m08:16:43.680811 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m08:16:43.682222 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
    using delta
      
      
      
      
      
      
      as
      

with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m08:16:53.400253 [debug] [Thread-4  ]: SQL status: OK in 9.720000267028809 seconds
[0m08:16:53.598897 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (execute): 08:16:41.479425 => 08:16:53.598526
[0m08:16:53.600235 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: ROLLBACK
[0m08:16:53.601846 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:16:53.603374 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: Close
[0m08:16:53.714970 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '083a842b-2ca5-4945-b47f-6eba68a2b841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf8c311f40>]}
[0m08:16:53.716817 [info ] [Thread-4  ]: 1 of 4 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 12.30s]
[0m08:16:53.719079 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_links
[0m08:16:53.720451 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_movies
[0m08:16:53.722006 [info ] [Thread-4  ]: 2 of 4 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m08:16:53.724061 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_links, now model.my_dbt_project.silver_movies)
[0m08:16:53.725652 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_movies
[0m08:16:53.733887 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m08:16:53.747027 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (compile): 08:16:53.726791 => 08:16:53.746405
[0m08:16:53.748593 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_movies
[0m08:16:53.764153 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:16:53.765273 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m08:16:53.766019 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

      describe extended `workspace`.`movielens_volume`.`silver_movies`
  
[0m08:16:53.766822 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:16:54.990543 [debug] [Thread-4  ]: SQL status: OK in 1.2200000286102295 seconds
[0m08:16:54.999143 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m08:16:55.009600 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m08:16:55.010710 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m08:16:58.552854 [debug] [Thread-4  ]: SQL status: OK in 3.5399999618530273 seconds
[0m08:16:58.559090 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (execute): 08:16:53.749582 => 08:16:58.558584
[0m08:16:58.560287 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: ROLLBACK
[0m08:16:58.561568 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:16:58.562811 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: Close
[0m08:16:58.676775 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '083a842b-2ca5-4945-b47f-6eba68a2b841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf8c4de2e0>]}
[0m08:16:58.678940 [info ] [Thread-4  ]: 2 of 4 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 4.95s]
[0m08:16:58.680860 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_movies
[0m08:16:58.682320 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_ratings
[0m08:16:58.683621 [info ] [Thread-4  ]: 3 of 4 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m08:16:58.685735 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_movies, now model.my_dbt_project.silver_ratings)
[0m08:16:58.686724 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_ratings
[0m08:16:58.695297 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m08:16:58.712402 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (compile): 08:16:58.687878 => 08:16:58.711580
[0m08:16:58.713917 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_ratings
[0m08:16:58.724665 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:16:58.725759 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m08:16:58.726692 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

      describe extended `workspace`.`movielens_volume`.`silver_ratings`
  
[0m08:16:58.727775 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:17:00.427039 [debug] [Thread-4  ]: SQL status: OK in 1.7000000476837158 seconds
[0m08:17:01.247038 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m08:17:01.261391 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m08:17:01.262795 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m08:17:07.093247 [debug] [Thread-4  ]: SQL status: OK in 5.829999923706055 seconds
[0m08:17:07.290418 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (execute): 08:16:58.714905 => 08:17:07.290014
[0m08:17:07.291977 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: ROLLBACK
[0m08:17:07.292936 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:17:07.293949 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: Close
[0m08:17:07.405909 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '083a842b-2ca5-4945-b47f-6eba68a2b841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf8c4bbd90>]}
[0m08:17:07.407795 [info ] [Thread-4  ]: 3 of 4 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 8.72s]
[0m08:17:07.409768 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_ratings
[0m08:17:07.410997 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_tags
[0m08:17:07.412782 [info ] [Thread-4  ]: 4 of 4 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m08:17:07.414622 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_ratings, now model.my_dbt_project.silver_tags)
[0m08:17:07.415642 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_tags
[0m08:17:07.438648 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m08:17:07.452547 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (compile): 08:17:07.416562 => 08:17:07.451969
[0m08:17:07.453668 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_tags
[0m08:17:07.463948 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:17:07.464946 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m08:17:07.465781 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

      describe extended `workspace`.`movielens_volume`.`silver_tags`
  
[0m08:17:07.466689 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:17:08.207088 [debug] [Thread-4  ]: SQL status: OK in 0.7400000095367432 seconds
[0m08:17:08.217381 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m08:17:08.237128 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m08:17:08.238360 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m08:17:12.433225 [debug] [Thread-4  ]: SQL status: OK in 4.190000057220459 seconds
[0m08:17:12.439028 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (execute): 08:17:07.454324 => 08:17:12.438399
[0m08:17:12.440464 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: ROLLBACK
[0m08:17:12.441835 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:17:12.442828 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: Close
[0m08:17:12.551400 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '083a842b-2ca5-4945-b47f-6eba68a2b841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf8c311f40>]}
[0m08:17:12.552989 [info ] [Thread-4  ]: 4 of 4 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 5.14s]
[0m08:17:12.554782 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_tags
[0m08:17:12.558361 [debug] [MainThread]: On master: ROLLBACK
[0m08:17:12.559254 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:17:12.949901 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:17:12.951026 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:17:12.952048 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:17:12.952917 [debug] [MainThread]: On master: ROLLBACK
[0m08:17:12.953736 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:17:12.954622 [debug] [MainThread]: On master: Close
[0m08:17:13.064178 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:17:13.065374 [debug] [MainThread]: Connection 'model.my_dbt_project.silver_tags' was properly closed.
[0m08:17:13.069308 [info ] [MainThread]: 
[0m08:17:13.071914 [info ] [MainThread]: Finished running 4 table models in 0 hours 1 minutes and 40.14 seconds (100.14s).
[0m08:17:13.075102 [debug] [MainThread]: Command end result
[0m08:17:13.120687 [info ] [MainThread]: 
[0m08:17:13.122294 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:17:13.124181 [info ] [MainThread]: 
[0m08:17:13.125984 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m08:17:13.129884 [debug] [MainThread]: Command `dbt run` succeeded at 08:17:13.128995 after 99.69 seconds
[0m08:17:13.131430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfb54825b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf8c3f95b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf8c4bbd90>]}
[0m08:17:13.132598 [debug] [MainThread]: Flushing usage events
[0m08:17:20.225333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a2b6c35e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a29c6f190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a29c6f8e0>]}


============================== 08:17:20.237145 | d817e773-40f3-4163-8bc6-4f44aa0b0bdd ==============================
[0m08:17:20.237145 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:17:20.238812 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'profiles_dir': '/opt/airflow/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:17:21.996947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd817e773-40f3-4163-8bc6-4f44aa0b0bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a29c6f130>]}
[0m08:17:22.050931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd817e773-40f3-4163-8bc6-4f44aa0b0bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fe9db790>]}
[0m08:17:22.052616 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:17:22.171908 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:17:23.965997 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:17:23.967126 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:17:23.979820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd817e773-40f3-4163-8bc6-4f44aa0b0bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fe9853a0>]}
[0m08:17:24.018462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd817e773-40f3-4163-8bc6-4f44aa0b0bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fe8ad3d0>]}
[0m08:17:24.019623 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:17:24.020484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd817e773-40f3-4163-8bc6-4f44aa0b0bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fe8ad430>]}
[0m08:17:24.023937 [info ] [MainThread]: 
[0m08:17:24.025804 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:17:24.028345 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:17:24.029152 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:17:24.029824 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m08:17:24.030457 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:17:24.608402 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m08:17:24.611138 [debug] [ThreadPool]: On list_workspace: Close
[0m08:17:24.726281 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m08:17:24.727822 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m08:17:24.743053 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m08:17:24.744315 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m08:17:24.745200 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m08:17:24.745951 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:17:25.501828 [debug] [ThreadPool]: SQL status: OK in 0.7599999904632568 seconds
[0m08:17:25.504669 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m08:17:25.505858 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m08:17:25.506899 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m08:17:25.508113 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m08:17:25.620156 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m08:17:25.634484 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:17:25.635821 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:17:25.636832 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:17:26.388665 [debug] [ThreadPool]: SQL status: OK in 0.75 seconds
[0m08:17:26.393753 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:17:26.511923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd817e773-40f3-4163-8bc6-4f44aa0b0bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fe8ad0a0>]}
[0m08:17:26.513257 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:17:26.514214 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:17:26.515684 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:17:26.517071 [info ] [MainThread]: 
[0m08:17:26.541860 [debug] [Thread-4  ]: Began running node model.my_dbt_project.genre_popularity
[0m08:17:26.543492 [info ] [Thread-4  ]: 1 of 5 START sql incremental model movielens_volume.genre_popularity ........... [RUN]
[0m08:17:26.545390 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.genre_popularity)
[0m08:17:26.546471 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.genre_popularity
[0m08:17:26.574654 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.genre_popularity"
[0m08:17:26.591503 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (compile): 08:17:26.547295 => 08:17:26.590913
[0m08:17:26.592695 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.genre_popularity
[0m08:17:26.669507 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:17:26.670585 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m08:17:26.671502 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m08:17:26.672689 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:17:27.850668 [debug] [Thread-4  ]: SQL status: OK in 1.1799999475479126 seconds
[0m08:17:27.891088 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m08:17:27.892415 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

  
    create temporary view `genre_popularity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings with Genres
-- Join silver ratings with silver movies to attach genres
-- Prepare rating timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        r.rating,
        r.rating_date as rating_timestamp,  
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Aggregate Genre Metrics
-- For each genre, calculate total ratings, avg rating,
-- and distribution insights
-- =======================================================
genre_stats as (
    select
        genres,  -- genres already formatted with spacing
        count(*) as rating_count,
        round(avg(rating), 2) as avg_rating,  
        to_date(min(rating_timestamp)) as first_rating_date,  
        to_date(max(rating_timestamp)) as last_rating_date   
    from ratings_base
    group by genres
),

-- =======================================================
-- Step 3: Ranking Genres
-- Rank genres by popularity (# of ratings)
-- and quality (avg rating)
-- =======================================================
ranked_genres as (
    select
        genres,
        rating_count,
        avg_rating,
        first_rating_date,
        last_rating_date,
        rank() over (order by rating_count desc) as popularity_rank,
        rank() over (order by avg_rating desc) as quality_rank
    from genre_stats
)

-- =======================================================
-- Step 4: Final Selection
-- One row per genre with all metrics
-- =======================================================
select
    genres,
    rating_count,
    avg_rating,
    popularity_rank,
    quality_rank,
    first_rating_date,
    last_rating_date
from ranked_genres


  -- Only insert new genres on incremental runs
  where genres not in (select genres from `workspace`.`movielens_volume`.`genre_popularity`)

  
[0m08:17:28.466372 [debug] [Thread-4  ]: SQL status: OK in 0.5699999928474426 seconds
[0m08:17:28.510870 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m08:17:28.512421 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m08:17:28.915470 [debug] [Thread-4  ]: SQL status: OK in 0.4000000059604645 seconds
[0m08:17:28.933699 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.genre_popularity"
[0m08:17:28.949274 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m08:17:28.950302 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`genre_popularity` as DBT_INTERNAL_DEST
      using `genre_popularity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.genres = DBT_INTERNAL_DEST.genres
          

      when matched then update set
         * 

      when not matched then insert *

[0m08:17:39.956800 [debug] [Thread-4  ]: SQL status: OK in 11.010000228881836 seconds
[0m08:17:40.140643 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (execute): 08:17:26.593424 => 08:17:40.140037
[0m08:17:40.142052 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: ROLLBACK
[0m08:17:40.143629 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:17:40.144921 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: Close
[0m08:17:40.259728 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd817e773-40f3-4163-8bc6-4f44aa0b0bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fd574eb0>]}
[0m08:17:40.261824 [info ] [Thread-4  ]: 1 of 5 OK created sql incremental model movielens_volume.genre_popularity ...... [[32mOK[0m in 13.71s]
[0m08:17:40.263986 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.genre_popularity
[0m08:17:40.265667 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_enriched
[0m08:17:40.267146 [info ] [Thread-4  ]: 2 of 5 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m08:17:40.269415 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.genre_popularity, now model.my_dbt_project.movie_enriched)
[0m08:17:40.270580 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_enriched
[0m08:17:40.281923 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m08:17:40.298826 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (compile): 08:17:40.271567 => 08:17:40.298114
[0m08:17:40.300130 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_enriched
[0m08:17:40.333835 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:17:40.334983 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m08:17:40.336033 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

      describe extended `workspace`.`movielens_volume`.`movie_enriched`
  
[0m08:17:40.337293 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:17:41.290994 [debug] [Thread-4  ]: SQL status: OK in 0.949999988079071 seconds
[0m08:17:41.385149 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m08:17:41.401027 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m08:17:41.402226 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m08:17:44.963744 [debug] [Thread-4  ]: SQL status: OK in 3.559999942779541 seconds
[0m08:17:44.976461 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (execute): 08:17:40.300840 => 08:17:44.976102
[0m08:17:44.977936 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: ROLLBACK
[0m08:17:44.979241 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:17:44.980230 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: Close
[0m08:17:45.089733 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd817e773-40f3-4163-8bc6-4f44aa0b0bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fd569e50>]}
[0m08:17:45.091238 [info ] [Thread-4  ]: 2 of 5 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 4.82s]
[0m08:17:45.092974 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_enriched
[0m08:17:45.094371 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_performance
[0m08:17:45.095824 [info ] [Thread-4  ]: 3 of 5 START sql incremental model movielens_volume.movie_performance .......... [RUN]
[0m08:17:45.098008 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_enriched, now model.my_dbt_project.movie_performance)
[0m08:17:45.099071 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_performance
[0m08:17:45.109381 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_performance"
[0m08:17:45.124006 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (compile): 08:17:45.099851 => 08:17:45.123573
[0m08:17:45.125248 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_performance
[0m08:17:45.142985 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:17:45.144156 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m08:17:45.145080 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m08:17:45.146170 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:17:45.867824 [debug] [Thread-4  ]: SQL status: OK in 0.7200000286102295 seconds
[0m08:17:45.876113 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m08:17:45.877420 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

  
    create temporary view `movie_performance__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings Join
-- Attach movie metadata (title, genres) to ratings
-- =======================================================
with ratings_base as (
    select
        r.movie_id,
        m.title,
        regexp_replace(m.genres, '\\|', ' | ') as genres,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_date
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Weighted Rating
-- Compute avg rating, rating count, weighted avg
-- =======================================================
weighted_rating as (
    select
        movie_id,
        title,
        genres,
        round(avg(rating), 2) as avg_rating,
        count(*) as rating_count,
        round(sum(rating) / count(*), 2) as weighted_avg_rating
    from ratings_base
    group by movie_id, title, genres
),

-- =======================================================
-- Step 3: Polarization Score
-- Standard deviation of ratings per movie
-- =======================================================
polarization as (
    select
        movie_id,
        round(coalesce(stddev(rating), 0.0), 2) as polarization_score
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 4: Top Rating Percentage
-- Percent of ratings >= 4.5
-- =======================================================
top_rating_pct as (
    select
        movie_id,
        round(100.0 * sum(case when rating >= 4.5 then 1 else 0 end) / count(*), 2) as top_rating_percentage
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 5: Genre Ranking
-- Rank movies within each genre by weighted_avg_rating
-- =======================================================
genre_rank as (
    select
        movie_id,
        genres,
        weighted_avg_rating,
        rank() over (partition by genres order by weighted_avg_rating desc) as genre_rank
    from weighted_rating
),

-- =======================================================
-- Step 6: Tag Count
-- Count distinct tags per movie
-- =======================================================
tag_count as (
    select
        movie_id,
        count(distinct tag) as tag_count
    from `workspace`.`movielens_volume`.`silver_tags`
    group by movie_id
)

-- =======================================================
-- Step 7: Final Selection
-- Combine all metrics into the gold table
-- =======================================================
select
    w.movie_id,
    w.title,
    w.genres,
    w.weighted_avg_rating,
    w.rating_count,
    g.genre_rank,
    p.polarization_score,
    tr.top_rating_percentage,
    coalesce(tc.tag_count, 0) as tag_count
from weighted_rating w
join genre_rank g
    on w.movie_id = g.movie_id
join polarization p
    on w.movie_id = p.movie_id
join top_rating_pct tr
    on w.movie_id = tr.movie_id
left join tag_count tc
    on w.movie_id = tc.movie_id


-- Use NOT EXISTS for safer incremental inserts
where not exists (
    select 1 
    from `workspace`.`movielens_volume`.`movie_performance` t
    where t.movie_id = w.movie_id
)

  
[0m08:17:46.772551 [debug] [Thread-4  ]: SQL status: OK in 0.8899999856948853 seconds
[0m08:17:46.780713 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m08:17:46.781876 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m08:17:47.065621 [debug] [Thread-4  ]: SQL status: OK in 0.2800000011920929 seconds
[0m08:17:47.073593 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_performance"
[0m08:17:47.089275 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m08:17:47.090830 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`movie_performance` as DBT_INTERNAL_DEST
      using `movie_performance__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.movie_id = DBT_INTERNAL_DEST.movie_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m08:17:54.173794 [debug] [Thread-4  ]: SQL status: OK in 7.079999923706055 seconds
[0m08:17:54.308228 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (execute): 08:17:45.126062 => 08:17:54.307808
[0m08:17:54.309570 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: ROLLBACK
[0m08:17:54.310756 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:17:54.311843 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: Close
[0m08:17:54.424395 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd817e773-40f3-4163-8bc6-4f44aa0b0bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fd569d60>]}
[0m08:17:54.426392 [info ] [Thread-4  ]: 3 of 5 OK created sql incremental model movielens_volume.movie_performance ..... [[32mOK[0m in 9.33s]
[0m08:17:54.428140 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_performance
[0m08:17:54.429597 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_activity
[0m08:17:54.431009 [info ] [Thread-4  ]: 4 of 5 START sql incremental model movielens_volume.user_activity .............. [RUN]
[0m08:17:54.432771 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_performance, now model.my_dbt_project.user_activity)
[0m08:17:54.434123 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_activity
[0m08:17:54.449480 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_activity"
[0m08:17:54.465857 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (compile): 08:17:54.435247 => 08:17:54.464710
[0m08:17:54.467675 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_activity
[0m08:17:54.479850 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:17:54.481514 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m08:17:54.482709 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m08:17:54.483853 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:17:55.729453 [debug] [Thread-4  ]: SQL status: OK in 1.25 seconds
[0m08:17:55.737793 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m08:17:55.739771 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

  
    create temporary view `user_activity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Join ratings with movies to attach genres and prepare timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_timestamp,
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: User Activity (General)
-- Calculate total ratings, average rating, first and last activity dates per user
-- =======================================================
user_activity as (
    select
        user_id,
        count(*) as user_rating_count,
        round(avg(rating), 2) as avg_user_rating,
        min(rating_timestamp) as first_activity_date,
        max(rating_timestamp) as last_activity_date
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 3: User Influence Score
-- Compute influence based on total ratings (more ratings = higher influence)
-- =======================================================
user_influence as (
    select
        user_id,
        user_rating_count as influence_score
    from user_activity
),

-- =======================================================
-- Step 4: Active Users by Genre
-- Count ratings per genre and rank genres per user
-- =======================================================
user_genre_activity as (
    select
        user_id,
        regexp_replace(genres, '\\|', ' | ') as genres,
        count(*) as genre_rating_count,
        rank() over (partition by user_id order by count(*) desc) as genre_rank
    from ratings_base
    group by user_id, genres
),

-- =======================================================
-- Step 5: New vs Loyal Users
-- Categorize users as new if first activity < 30 days ago, else loyal
-- =======================================================
user_type as (
    select
        user_id,
        case
            when min(rating_timestamp) >= date_sub(current_date(), 30)
                then 'new_user'
            else 'loyal_user'
        end as user_category
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 6: Top Genre per User
-- Identify the most active genre for each user
-- =======================================================
top_genre as (
    select
        user_id,
        max(genres) as top_genre
    from user_genre_activity
    group by user_id
)

-- =======================================================
-- Final Selection
-- Combine general activity, influence, type, and top genre per user
-- =======================================================
select
    ua.user_id,
    ua.user_rating_count,
    ua.avg_user_rating,
    ua.first_activity_date,
    ua.last_activity_date,
    ui.influence_score,
    ut.user_category,
    tg.top_genre
from user_activity ua
join user_influence ui on ua.user_id = ui.user_id
join user_type ut on ua.user_id = ut.user_id
left join top_genre tg on ua.user_id = tg.user_id


  where ua.user_id not in (select user_id from `workspace`.`movielens_volume`.`user_activity`)

  
[0m08:17:56.317372 [debug] [Thread-4  ]: SQL status: OK in 0.5799999833106995 seconds
[0m08:17:56.337654 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m08:17:56.339334 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m08:17:56.689164 [debug] [Thread-4  ]: SQL status: OK in 0.3499999940395355 seconds
[0m08:17:56.696796 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_activity"
[0m08:17:56.711594 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m08:17:56.712690 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_activity` as DBT_INTERNAL_DEST
      using `user_activity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m08:18:06.077290 [debug] [Thread-4  ]: SQL status: OK in 9.359999656677246 seconds
[0m08:18:06.221898 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (execute): 08:17:54.468618 => 08:18:06.221151
[0m08:18:06.231073 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: ROLLBACK
[0m08:18:06.232806 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:06.233931 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: Close
[0m08:18:06.344410 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd817e773-40f3-4163-8bc6-4f44aa0b0bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fd569d60>]}
[0m08:18:06.345807 [info ] [Thread-4  ]: 4 of 5 OK created sql incremental model movielens_volume.user_activity ......... [[32mOK[0m in 11.91s]
[0m08:18:06.347359 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_activity
[0m08:18:06.348440 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_preferences
[0m08:18:06.349641 [info ] [Thread-4  ]: 5 of 5 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m08:18:06.351831 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.user_activity, now model.my_dbt_project.user_preferences)
[0m08:18:06.352862 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_preferences
[0m08:18:06.363951 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m08:18:06.377652 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (compile): 08:18:06.353516 => 08:18:06.377062
[0m08:18:06.378669 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_preferences
[0m08:18:06.386682 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:06.387643 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:18:06.388465 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m08:18:06.389297 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:18:07.283661 [debug] [Thread-4  ]: SQL status: OK in 0.8899999856948853 seconds
[0m08:18:07.289349 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:18:07.290515 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create temporary view `user_preferences__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score,  -- influence based on total ratings
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )

  
[0m08:18:07.787975 [debug] [Thread-4  ]: SQL status: OK in 0.5 seconds
[0m08:18:07.802871 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:18:07.804463 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m08:18:08.260512 [debug] [Thread-4  ]: SQL status: OK in 0.44999998807907104 seconds
[0m08:18:08.266697 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m08:18:08.276657 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:18:08.277498 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
      using `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m08:18:16.478467 [debug] [Thread-4  ]: SQL status: OK in 8.199999809265137 seconds
[0m08:18:16.617299 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (execute): 08:18:06.379250 => 08:18:16.616885
[0m08:18:16.618571 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: ROLLBACK
[0m08:18:16.619656 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:16.620852 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: Close
[0m08:18:16.737440 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd817e773-40f3-4163-8bc6-4f44aa0b0bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fd5e7970>]}
[0m08:18:16.739428 [info ] [Thread-4  ]: 5 of 5 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 10.39s]
[0m08:18:16.741329 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_preferences
[0m08:18:16.745345 [debug] [MainThread]: On master: ROLLBACK
[0m08:18:16.746384 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:18:17.128503 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:18:17.129762 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:17.130796 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:18:17.131727 [debug] [MainThread]: On master: ROLLBACK
[0m08:18:17.132661 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:18:17.133652 [debug] [MainThread]: On master: Close
[0m08:18:17.243224 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:18:17.244594 [debug] [MainThread]: Connection 'model.my_dbt_project.user_preferences' was properly closed.
[0m08:18:17.248645 [info ] [MainThread]: 
[0m08:18:17.250501 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 53.22 seconds (53.22s).
[0m08:18:17.253746 [debug] [MainThread]: Command end result
[0m08:18:17.319446 [info ] [MainThread]: 
[0m08:18:17.321621 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:18:17.323386 [info ] [MainThread]: 
[0m08:18:17.325345 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m08:18:17.327999 [debug] [MainThread]: Command `dbt run` succeeded at 08:18:17.327489 after 54.13 seconds
[0m08:18:17.329358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a2b6c35e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fe9db790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09fe6f96d0>]}
[0m08:18:17.330844 [debug] [MainThread]: Flushing usage events
[0m08:18:24.986092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fdb702520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd9c70190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd9c708b0>]}


============================== 08:18:24.996549 | 51832567-132e-45b3-914c-b9bed4060532 ==============================
[0m08:18:24.996549 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:18:24.997973 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/opt/airflow/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:18:26.539054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '51832567-132e-45b3-914c-b9bed4060532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd9c70130>]}
[0m08:18:26.586532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '51832567-132e-45b3-914c-b9bed4060532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fae9c1910>]}
[0m08:18:26.587807 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:18:26.659945 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:18:28.819875 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:18:28.821164 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:18:28.867686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '51832567-132e-45b3-914c-b9bed4060532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fae977370>]}
[0m08:18:28.937894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '51832567-132e-45b3-914c-b9bed4060532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fae79d3a0>]}
[0m08:18:28.939415 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:18:28.941125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '51832567-132e-45b3-914c-b9bed4060532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fae79d400>]}
[0m08:18:28.947477 [info ] [MainThread]: 
[0m08:18:28.950616 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:18:28.954163 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m08:18:28.963359 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:18:28.964314 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:18:28.965458 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:18:29.697693 [debug] [ThreadPool]: SQL status: OK in 0.7300000190734863 seconds
[0m08:18:29.702493 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:18:29.813900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '51832567-132e-45b3-914c-b9bed4060532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fae79d070>]}
[0m08:18:29.815409 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:29.816418 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:18:29.817896 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:18:29.819127 [info ] [MainThread]: 
[0m08:18:29.847499 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:18:29.848951 [info ] [Thread-2  ]: 1 of 22 START test accepted_rating_range_silver_ratings_rating__5__0 ........... [RUN]
[0m08:18:29.851190 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff)
[0m08:18:29.852247 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:18:29.873214 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m08:18:29.890639 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (compile): 08:18:29.853033 => 08:18:29.889763
[0m08:18:29.892138 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:18:29.941269 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m08:18:29.963609 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:29.965138 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m08:18:29.966252 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`silver_ratings`
where rating < 0
   or rating > 5


      
    ) dbt_internal_test
[0m08:18:29.967303 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:31.443432 [debug] [Thread-2  ]: SQL status: OK in 1.4800000190734863 seconds
[0m08:18:31.456997 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (execute): 08:18:29.892887 => 08:18:31.456478
[0m08:18:31.458229 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: ROLLBACK
[0m08:18:31.459342 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:31.460417 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: Close
[0m08:18:31.572349 [info ] [Thread-2  ]: 1 of 22 PASS accepted_rating_range_silver_ratings_rating__5__0 ................. [[32mPASS[0m in 1.72s]
[0m08:18:31.574580 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:18:31.575864 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:18:31.577225 [info ] [Thread-2  ]: 2 of 22 START test not_null_silver_links_imdb_id ............................... [RUN]
[0m08:18:31.579121 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff, now test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51)
[0m08:18:31.580369 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:18:31.596503 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m08:18:31.614642 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (compile): 08:18:31.581311 => 08:18:31.614135
[0m08:18:31.615698 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:18:31.623033 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m08:18:31.639373 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:31.640545 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m08:18:31.641533 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



      
    ) dbt_internal_test
[0m08:18:31.642559 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:32.805519 [debug] [Thread-2  ]: SQL status: OK in 1.159999966621399 seconds
[0m08:18:32.812747 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (execute): 08:18:31.616503 => 08:18:32.812156
[0m08:18:32.814013 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: ROLLBACK
[0m08:18:32.815099 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:32.816033 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m08:18:32.916755 [info ] [Thread-2  ]: 2 of 22 PASS not_null_silver_links_imdb_id ..................................... [[32mPASS[0m in 1.34s]
[0m08:18:32.919135 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:18:32.920361 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:18:32.921698 [info ] [Thread-2  ]: 3 of 22 START test not_null_silver_links_movie_id .............................. [RUN]
[0m08:18:32.923471 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51, now test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d)
[0m08:18:32.924657 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:18:32.938283 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m08:18:32.973323 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (compile): 08:18:32.925480 => 08:18:32.972323
[0m08:18:32.974821 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:18:32.982479 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m08:18:32.999061 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:33.000315 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m08:18:33.001595 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



      
    ) dbt_internal_test
[0m08:18:33.002740 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:34.070710 [debug] [Thread-2  ]: SQL status: OK in 1.0700000524520874 seconds
[0m08:18:34.077464 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (execute): 08:18:32.975711 => 08:18:34.077017
[0m08:18:34.078682 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: ROLLBACK
[0m08:18:34.079998 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:34.081376 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m08:18:34.994764 [info ] [Thread-2  ]: 3 of 22 PASS not_null_silver_links_movie_id .................................... [[32mPASS[0m in 2.07s]
[0m08:18:34.998281 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:18:35.000213 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:18:35.002416 [info ] [Thread-2  ]: 4 of 22 START test not_null_silver_links_tmdb_id ............................... [RUN]
[0m08:18:35.004817 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d, now test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f)
[0m08:18:35.006098 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:18:35.034967 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m08:18:35.862832 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (compile): 08:18:35.007328 => 08:18:35.861923
[0m08:18:35.864378 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:18:35.872289 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m08:18:35.890823 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:35.892157 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m08:18:35.893140 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



      
    ) dbt_internal_test
[0m08:18:35.894394 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:36.926997 [debug] [Thread-2  ]: SQL status: OK in 1.0299999713897705 seconds
[0m08:18:36.933917 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (execute): 08:18:35.865199 => 08:18:36.933279
[0m08:18:36.935231 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: ROLLBACK
[0m08:18:36.936502 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:36.937678 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m08:18:37.049564 [info ] [Thread-2  ]: 4 of 22 PASS not_null_silver_links_tmdb_id ..................................... [[32mPASS[0m in 2.05s]
[0m08:18:37.051785 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:18:37.053031 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:18:37.054279 [info ] [Thread-2  ]: 5 of 22 START test not_null_silver_movies_genres ............................... [RUN]
[0m08:18:37.056118 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f, now test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63)
[0m08:18:37.057398 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:18:37.071612 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m08:18:37.091470 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (compile): 08:18:37.058367 => 08:18:37.090885
[0m08:18:37.092603 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:18:37.098974 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m08:18:37.117784 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:37.118998 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m08:18:37.120168 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



      
    ) dbt_internal_test
[0m08:18:37.121542 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:38.096874 [debug] [Thread-2  ]: SQL status: OK in 0.9800000190734863 seconds
[0m08:18:38.104142 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (execute): 08:18:37.093694 => 08:18:38.103628
[0m08:18:38.105237 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: ROLLBACK
[0m08:18:38.106444 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:38.107412 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m08:18:38.226575 [info ] [Thread-2  ]: 5 of 22 PASS not_null_silver_movies_genres ..................................... [[32mPASS[0m in 1.17s]
[0m08:18:38.229317 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:18:38.231346 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:18:38.232706 [info ] [Thread-2  ]: 6 of 22 START test not_null_silver_movies_movie_id ............................. [RUN]
[0m08:18:38.235271 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63, now test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e)
[0m08:18:38.236335 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:18:38.246428 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m08:18:38.263626 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (compile): 08:18:38.237497 => 08:18:38.263087
[0m08:18:38.264759 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:18:38.272166 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m08:18:38.289344 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:38.290455 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m08:18:38.291413 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



      
    ) dbt_internal_test
[0m08:18:38.292485 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:39.202344 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m08:18:39.214576 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (execute): 08:18:38.265548 => 08:18:39.213902
[0m08:18:39.218891 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: ROLLBACK
[0m08:18:39.220246 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:39.221357 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m08:18:39.337247 [info ] [Thread-2  ]: 6 of 22 PASS not_null_silver_movies_movie_id ................................... [[32mPASS[0m in 1.10s]
[0m08:18:39.339635 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:18:39.341286 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:18:39.342446 [info ] [Thread-2  ]: 7 of 22 START test not_null_silver_movies_title ................................ [RUN]
[0m08:18:39.344502 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e, now test.my_dbt_project.not_null_silver_movies_title.3a779bd2de)
[0m08:18:39.345600 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:18:39.365564 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m08:18:39.385642 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (compile): 08:18:39.346456 => 08:18:39.385124
[0m08:18:39.386871 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:18:39.393709 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m08:18:39.413611 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:39.414763 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m08:18:39.415826 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



      
    ) dbt_internal_test
[0m08:18:39.417020 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:40.292104 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m08:18:40.300165 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (execute): 08:18:39.387706 => 08:18:40.299284
[0m08:18:40.301966 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: ROLLBACK
[0m08:18:40.303189 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:40.304370 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m08:18:40.447110 [info ] [Thread-2  ]: 7 of 22 PASS not_null_silver_movies_title ...................................... [[32mPASS[0m in 1.10s]
[0m08:18:40.450289 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:18:40.451849 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:18:40.453016 [info ] [Thread-2  ]: 8 of 22 START test not_null_silver_ratings_movie_id ............................ [RUN]
[0m08:18:40.455042 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_title.3a779bd2de, now test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36)
[0m08:18:40.456188 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:18:40.469934 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m08:18:40.491132 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (compile): 08:18:40.457293 => 08:18:40.490043
[0m08:18:40.492541 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:18:40.499192 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m08:18:40.517166 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:40.518973 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m08:18:40.520060 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



      
    ) dbt_internal_test
[0m08:18:40.521406 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:41.447929 [debug] [Thread-2  ]: SQL status: OK in 0.9300000071525574 seconds
[0m08:18:41.455105 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (execute): 08:18:40.493392 => 08:18:41.454614
[0m08:18:41.456272 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: ROLLBACK
[0m08:18:41.457332 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:41.458501 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m08:18:41.568421 [info ] [Thread-2  ]: 8 of 22 PASS not_null_silver_ratings_movie_id .................................. [[32mPASS[0m in 1.11s]
[0m08:18:41.570488 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:18:41.571821 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:18:41.572905 [info ] [Thread-2  ]: 9 of 22 START test not_null_silver_ratings_rating .............................. [RUN]
[0m08:18:41.574706 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36, now test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a)
[0m08:18:41.575681 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:18:41.585768 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m08:18:41.601973 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (compile): 08:18:41.576404 => 08:18:41.601030
[0m08:18:41.603260 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:18:41.612449 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m08:18:41.628162 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:41.629497 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m08:18:41.630761 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



      
    ) dbt_internal_test
[0m08:18:41.631889 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:43.110621 [debug] [Thread-2  ]: SQL status: OK in 1.4800000190734863 seconds
[0m08:18:43.117239 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (execute): 08:18:41.604101 => 08:18:43.116721
[0m08:18:43.118535 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: ROLLBACK
[0m08:18:43.120047 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:43.121198 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m08:18:43.237198 [info ] [Thread-2  ]: 9 of 22 PASS not_null_silver_ratings_rating .................................... [[32mPASS[0m in 1.66s]
[0m08:18:43.239490 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:18:43.241143 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:18:43.242630 [info ] [Thread-2  ]: 10 of 22 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m08:18:43.245241 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a, now test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f)
[0m08:18:43.246492 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:18:43.271829 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m08:18:43.292487 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (compile): 08:18:43.247233 => 08:18:43.291823
[0m08:18:43.293754 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:18:43.300272 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m08:18:43.318102 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:43.319475 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m08:18:43.320488 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



      
    ) dbt_internal_test
[0m08:18:43.321646 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:44.510402 [debug] [Thread-2  ]: SQL status: OK in 1.190000057220459 seconds
[0m08:18:44.516917 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (execute): 08:18:43.294689 => 08:18:44.516429
[0m08:18:44.518146 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: ROLLBACK
[0m08:18:44.519508 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:44.520772 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m08:18:44.630392 [info ] [Thread-2  ]: 10 of 22 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 1.39s]
[0m08:18:44.632652 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:18:44.634096 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:18:44.635345 [info ] [Thread-2  ]: 11 of 22 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m08:18:44.637488 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f, now test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3)
[0m08:18:44.638696 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:18:44.653256 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m08:18:44.674510 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (compile): 08:18:44.639663 => 08:18:44.673721
[0m08:18:44.675905 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:18:44.683195 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m08:18:44.701505 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:44.702779 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m08:18:44.703888 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



      
    ) dbt_internal_test
[0m08:18:44.705117 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:45.612868 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m08:18:45.619509 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (execute): 08:18:44.676855 => 08:18:45.618957
[0m08:18:45.620639 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: ROLLBACK
[0m08:18:45.621768 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:45.622739 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m08:18:45.740023 [info ] [Thread-2  ]: 11 of 22 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 1.10s]
[0m08:18:45.742258 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:18:45.743757 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:18:45.744888 [info ] [Thread-2  ]: 12 of 22 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m08:18:45.746757 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3, now test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3)
[0m08:18:45.747727 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:18:45.760824 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m08:18:45.778387 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (compile): 08:18:45.748498 => 08:18:45.777753
[0m08:18:45.779639 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:18:45.787573 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m08:18:45.804766 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:45.805923 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m08:18:45.807025 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



      
    ) dbt_internal_test
[0m08:18:45.808371 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:46.879673 [debug] [Thread-2  ]: SQL status: OK in 1.0700000524520874 seconds
[0m08:18:46.886596 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (execute): 08:18:45.780459 => 08:18:46.886081
[0m08:18:46.887739 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: ROLLBACK
[0m08:18:46.888937 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:46.890070 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m08:18:46.995917 [info ] [Thread-2  ]: 12 of 22 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 1.25s]
[0m08:18:46.998172 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:18:46.999469 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:18:47.000577 [info ] [Thread-2  ]: 13 of 22 START test not_null_silver_tags_tag ................................... [RUN]
[0m08:18:47.002436 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3, now test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8)
[0m08:18:47.003562 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:18:47.021200 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m08:18:47.039775 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (compile): 08:18:47.004379 => 08:18:47.039204
[0m08:18:47.040912 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:18:47.047318 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m08:18:47.064792 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:47.065986 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m08:18:47.069391 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



      
    ) dbt_internal_test
[0m08:18:47.071226 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:48.047705 [debug] [Thread-2  ]: SQL status: OK in 0.9800000190734863 seconds
[0m08:18:48.054594 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (execute): 08:18:47.041753 => 08:18:48.054105
[0m08:18:48.055787 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: ROLLBACK
[0m08:18:48.056827 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:48.057763 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m08:18:48.171560 [info ] [Thread-2  ]: 13 of 22 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 1.17s]
[0m08:18:48.173677 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:18:48.175047 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:18:48.176193 [info ] [Thread-2  ]: 14 of 22 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m08:18:48.178192 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8, now test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86)
[0m08:18:48.179222 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:18:48.189200 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m08:18:48.205762 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (compile): 08:18:48.180003 => 08:18:48.205141
[0m08:18:48.206981 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:18:48.214143 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m08:18:48.230797 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:48.231986 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m08:18:48.233084 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



      
    ) dbt_internal_test
[0m08:18:48.234098 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:49.410006 [debug] [Thread-2  ]: SQL status: OK in 1.1799999475479126 seconds
[0m08:18:49.417451 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (execute): 08:18:48.208021 => 08:18:49.416737
[0m08:18:49.419068 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: ROLLBACK
[0m08:18:49.420304 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:49.421705 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m08:18:49.528977 [info ] [Thread-2  ]: 14 of 22 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 1.35s]
[0m08:18:49.531082 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:18:49.532430 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:18:49.533702 [info ] [Thread-2  ]: 15 of 22 START test not_null_silver_tags_user_id ............................... [RUN]
[0m08:18:49.535489 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86, now test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3)
[0m08:18:49.536718 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:18:49.546423 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m08:18:49.565300 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (compile): 08:18:49.537665 => 08:18:49.564387
[0m08:18:49.566466 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:18:49.575005 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m08:18:49.594684 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:49.595853 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m08:18:49.597008 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



      
    ) dbt_internal_test
[0m08:18:49.598408 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:50.479685 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m08:18:50.488140 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (execute): 08:18:49.567575 => 08:18:50.487616
[0m08:18:50.489487 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: ROLLBACK
[0m08:18:50.490823 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:50.492051 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m08:18:50.599757 [info ] [Thread-2  ]: 15 of 22 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 1.06s]
[0m08:18:50.602398 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:18:50.603853 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:18:50.604960 [info ] [Thread-2  ]: 16 of 22 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:18:50.606881 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m08:18:50.608272 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:18:50.645763 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:18:50.663819 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 08:18:50.609133 => 08:18:50.663305
[0m08:18:50.664889 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:18:50.670480 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:18:50.688825 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:50.689947 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:18:50.690900 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:18:50.692052 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:52.033899 [debug] [Thread-2  ]: SQL status: OK in 1.340000033378601 seconds
[0m08:18:52.040421 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 08:18:50.665659 => 08:18:52.039938
[0m08:18:52.041577 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m08:18:52.042634 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:52.043841 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m08:18:52.161994 [info ] [Thread-2  ]: 16 of 22 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.56s]
[0m08:18:52.164038 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:18:52.165373 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:18:52.166432 [info ] [Thread-2  ]: 17 of 22 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:18:52.168441 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m08:18:52.169479 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:18:52.181520 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:18:52.201447 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 08:18:52.170268 => 08:18:52.200530
[0m08:18:52.202676 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:18:52.210492 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:18:52.229045 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:52.230212 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:18:52.231477 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:18:52.232730 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:53.398093 [debug] [Thread-2  ]: SQL status: OK in 1.1699999570846558 seconds
[0m08:18:53.406233 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 08:18:52.203689 => 08:18:53.405422
[0m08:18:53.407530 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m08:18:53.408612 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:53.409754 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m08:18:53.522734 [info ] [Thread-2  ]: 17 of 22 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.35s]
[0m08:18:53.524578 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:18:53.526062 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:18:53.527257 [info ] [Thread-2  ]: 18 of 22 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:18:53.529031 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32)
[0m08:18:53.530240 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:18:53.544281 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m08:18:53.563612 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (compile): 08:18:53.531208 => 08:18:53.562956
[0m08:18:53.565195 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:18:53.574495 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m08:18:53.594616 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:53.595924 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m08:18:53.597221 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:18:53.598699 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:54.955044 [debug] [Thread-2  ]: SQL status: OK in 1.3600000143051147 seconds
[0m08:18:54.961432 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (execute): 08:18:53.566334 => 08:18:54.960898
[0m08:18:54.962528 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: ROLLBACK
[0m08:18:54.963528 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:54.964519 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m08:18:55.074030 [info ] [Thread-2  ]: 18 of 22 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.54s]
[0m08:18:55.076512 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:18:55.077950 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:18:55.079012 [info ] [Thread-2  ]: 19 of 22 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:18:55.081232 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32, now test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd)
[0m08:18:55.082361 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:18:55.111555 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m08:18:55.128987 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (compile): 08:18:55.083189 => 08:18:55.128225
[0m08:18:55.130357 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:18:55.136944 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m08:18:55.155378 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:55.157242 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m08:18:55.158880 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:18:55.160559 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:56.705265 [debug] [Thread-2  ]: SQL status: OK in 1.5399999618530273 seconds
[0m08:18:56.714394 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (execute): 08:18:55.131257 => 08:18:56.713359
[0m08:18:56.716105 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: ROLLBACK
[0m08:18:56.717492 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:56.718651 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m08:18:56.834173 [info ] [Thread-2  ]: 19 of 22 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.75s]
[0m08:18:56.836339 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:18:56.838002 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:18:56.839249 [info ] [Thread-2  ]: 20 of 22 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:18:56.841244 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd, now test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719)
[0m08:18:56.842495 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:18:56.859529 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m08:18:56.879422 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (compile): 08:18:56.843362 => 08:18:56.878485
[0m08:18:56.880862 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:18:56.887830 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m08:18:56.906574 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:56.907859 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m08:18:56.908932 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:18:56.910180 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:57.964377 [debug] [Thread-2  ]: SQL status: OK in 1.0499999523162842 seconds
[0m08:18:57.971150 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (execute): 08:18:56.882007 => 08:18:57.970564
[0m08:18:57.972299 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: ROLLBACK
[0m08:18:57.973331 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:57.974499 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m08:18:58.091038 [info ] [Thread-2  ]: 20 of 22 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 1.25s]
[0m08:18:58.093266 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:18:58.094517 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:18:58.095647 [info ] [Thread-2  ]: 21 of 22 START test unique_silver_links_movie_id ............................... [RUN]
[0m08:18:58.097446 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719, now test.my_dbt_project.unique_silver_links_movie_id.c422609054)
[0m08:18:58.098525 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:18:58.112392 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m08:18:58.130985 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (compile): 08:18:58.099333 => 08:18:58.130177
[0m08:18:58.132481 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:18:58.140349 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m08:18:58.157996 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:58.159247 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m08:18:58.160316 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:18:58.161474 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:18:59.016470 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m08:18:59.023739 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (execute): 08:18:58.133455 => 08:18:59.023182
[0m08:18:59.024927 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: ROLLBACK
[0m08:18:59.026153 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:18:59.027210 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m08:18:59.136837 [info ] [Thread-2  ]: 21 of 22 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 1.04s]
[0m08:18:59.139449 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:18:59.141033 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:18:59.142364 [info ] [Thread-2  ]: 22 of 22 START test unique_silver_movies_movie_id .............................. [RUN]
[0m08:18:59.144345 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_silver_links_movie_id.c422609054, now test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085)
[0m08:18:59.145424 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:18:59.158413 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m08:18:59.177752 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (compile): 08:18:59.146336 => 08:18:59.177013
[0m08:18:59.179049 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:18:59.206939 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m08:18:59.225687 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:18:59.226781 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m08:18:59.227808 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:18:59.228800 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:00.196882 [debug] [Thread-2  ]: SQL status: OK in 0.9700000286102295 seconds
[0m08:19:00.203104 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (execute): 08:18:59.179978 => 08:19:00.202649
[0m08:19:00.204080 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: ROLLBACK
[0m08:19:00.205116 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:00.206186 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m08:19:00.317125 [info ] [Thread-2  ]: 22 of 22 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 1.17s]
[0m08:19:00.319319 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:19:00.323239 [debug] [MainThread]: On master: ROLLBACK
[0m08:19:00.324187 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:19:00.703002 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:19:00.704258 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:00.705241 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:19:00.706261 [debug] [MainThread]: On master: ROLLBACK
[0m08:19:00.707192 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:19:00.708042 [debug] [MainThread]: On master: Close
[0m08:19:00.875949 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:19:00.877514 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085' was properly closed.
[0m08:19:00.882856 [info ] [MainThread]: 
[0m08:19:00.885786 [info ] [MainThread]: Finished running 22 tests in 0 hours 0 minutes and 31.93 seconds (31.93s).
[0m08:19:00.896802 [debug] [MainThread]: Command end result
[0m08:19:00.970606 [info ] [MainThread]: 
[0m08:19:00.973087 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:19:00.974917 [info ] [MainThread]: 
[0m08:19:00.976693 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=0 SKIP=0 TOTAL=22
[0m08:19:00.979445 [debug] [MainThread]: Command `dbt test` succeeded at 08:19:00.979040 after 34.45 seconds
[0m08:19:00.981910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fdb702520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fae9c1910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fae977970>]}
[0m08:19:00.983438 [debug] [MainThread]: Flushing usage events
[0m08:19:08.667362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d33a43550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d31fef1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d31fef8e0>]}


============================== 08:19:08.680193 | 919447a4-79eb-468a-ad2b-906c9b5cd3fa ==============================
[0m08:19:08.680193 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:19:08.681816 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/opt/airflow/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:19:10.377640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '919447a4-79eb-468a-ad2b-906c9b5cd3fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d31fef160>]}
[0m08:19:10.436926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '919447a4-79eb-468a-ad2b-906c9b5cd3fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d06d1a850>]}
[0m08:19:10.438540 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:19:10.511216 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:19:12.543008 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:19:12.543999 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:19:12.559010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '919447a4-79eb-468a-ad2b-906c9b5cd3fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d06cd13a0>]}
[0m08:19:12.600043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '919447a4-79eb-468a-ad2b-906c9b5cd3fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d06bfb3d0>]}
[0m08:19:12.601254 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:19:12.602350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '919447a4-79eb-468a-ad2b-906c9b5cd3fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d06bfb430>]}
[0m08:19:12.608028 [info ] [MainThread]: 
[0m08:19:12.609711 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:19:12.612944 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m08:19:12.621067 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:19:12.621996 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:19:12.622770 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:19:13.332894 [debug] [ThreadPool]: SQL status: OK in 0.7099999785423279 seconds
[0m08:19:13.336489 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:19:13.445407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '919447a4-79eb-468a-ad2b-906c9b5cd3fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d06bfb040>]}
[0m08:19:13.446520 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:13.447295 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:19:13.448493 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:19:13.449716 [info ] [MainThread]: 
[0m08:19:13.474680 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m08:19:13.475802 [info ] [Thread-2  ]: 1 of 35 START test accepted_rating_range_genre_popularity_avg_rating__5__0 ..... [RUN]
[0m08:19:13.477350 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413)
[0m08:19:13.478148 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m08:19:13.497223 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m08:19:13.509023 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (compile): 08:19:13.478897 => 08:19:13.508566
[0m08:19:13.509866 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m08:19:13.546038 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m08:19:13.557491 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:13.558518 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m08:19:13.559381 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m08:19:13.560305 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:14.495225 [debug] [Thread-2  ]: SQL status: OK in 0.9300000071525574 seconds
[0m08:19:14.507899 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (execute): 08:19:13.510530 => 08:19:14.507470
[0m08:19:14.508938 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: ROLLBACK
[0m08:19:14.509821 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:14.510573 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: Close
[0m08:19:14.618131 [info ] [Thread-2  ]: 1 of 35 PASS accepted_rating_range_genre_popularity_avg_rating__5__0 ........... [[32mPASS[0m in 1.14s]
[0m08:19:14.619741 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m08:19:14.620771 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:19:14.621616 [info ] [Thread-2  ]: 2 of 35 START test accepted_rating_range_movie_enriched_avg_rating__5__0 ....... [RUN]
[0m08:19:14.623350 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413, now test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8)
[0m08:19:14.624307 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:19:14.631889 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m08:19:14.644637 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (compile): 08:19:14.625181 => 08:19:14.644103
[0m08:19:14.645618 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:19:14.651540 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m08:19:14.663761 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:14.664641 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m08:19:14.665570 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m08:19:14.666362 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:15.423240 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m08:19:15.427362 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (execute): 08:19:14.646291 => 08:19:15.426992
[0m08:19:15.428192 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: ROLLBACK
[0m08:19:15.428979 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:15.429743 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: Close
[0m08:19:15.534266 [info ] [Thread-2  ]: 2 of 35 PASS accepted_rating_range_movie_enriched_avg_rating__5__0 ............. [[32mPASS[0m in 0.91s]
[0m08:19:15.536197 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:19:15.537187 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m08:19:15.538112 [info ] [Thread-2  ]: 3 of 35 START test accepted_rating_range_movie_performance_weighted_avg_rating__5__0  [RUN]
[0m08:19:15.539692 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8, now test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083)
[0m08:19:15.540515 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m08:19:15.551804 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m08:19:15.565473 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (compile): 08:19:15.541145 => 08:19:15.564833
[0m08:19:15.566560 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m08:19:15.572276 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m08:19:15.584908 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:15.585833 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m08:19:15.586732 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating < 0
   or weighted_avg_rating > 5


      
    ) dbt_internal_test
[0m08:19:15.587683 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:16.489360 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m08:19:16.496112 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (execute): 08:19:15.567414 => 08:19:16.495556
[0m08:19:16.497438 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: ROLLBACK
[0m08:19:16.498573 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:16.499545 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: Close
[0m08:19:16.610889 [info ] [Thread-2  ]: 3 of 35 PASS accepted_rating_range_movie_performance_weighted_avg_rating__5__0 . [[32mPASS[0m in 1.07s]
[0m08:19:16.612328 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m08:19:16.613397 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m08:19:16.614430 [info ] [Thread-2  ]: 4 of 35 START test accepted_rating_range_user_activity_avg_user_rating__5__0 ... [RUN]
[0m08:19:16.615902 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083, now test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a)
[0m08:19:16.616814 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m08:19:16.632213 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m08:19:16.646334 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (compile): 08:19:16.617478 => 08:19:16.645798
[0m08:19:16.647468 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m08:19:16.653126 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m08:19:16.667012 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:16.667838 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m08:19:16.668576 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating < 0
   or avg_user_rating > 5


      
    ) dbt_internal_test
[0m08:19:16.669419 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:17.612211 [debug] [Thread-2  ]: SQL status: OK in 0.9399999976158142 seconds
[0m08:19:17.616604 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (execute): 08:19:16.648347 => 08:19:17.616256
[0m08:19:17.617524 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: ROLLBACK
[0m08:19:17.618262 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:17.618969 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: Close
[0m08:19:17.724866 [info ] [Thread-2  ]: 4 of 35 PASS accepted_rating_range_user_activity_avg_user_rating__5__0 ......... [[32mPASS[0m in 1.11s]
[0m08:19:17.726392 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m08:19:17.727285 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m08:19:17.728039 [info ] [Thread-2  ]: 5 of 35 START test not_null_genre_popularity_avg_rating ........................ [RUN]
[0m08:19:17.729554 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a, now test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9)
[0m08:19:17.730327 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m08:19:17.741240 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m08:19:17.751531 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (compile): 08:19:17.730858 => 08:19:17.750911
[0m08:19:17.752529 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m08:19:17.757886 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m08:19:17.768284 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:17.769191 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m08:19:17.769911 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating is null



      
    ) dbt_internal_test
[0m08:19:17.770637 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:18.516638 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m08:19:18.520952 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (execute): 08:19:17.753230 => 08:19:18.520609
[0m08:19:18.521804 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: ROLLBACK
[0m08:19:18.522532 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:18.523314 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: Close
[0m08:19:18.630010 [info ] [Thread-2  ]: 5 of 35 PASS not_null_genre_popularity_avg_rating .............................. [[32mPASS[0m in 0.90s]
[0m08:19:18.631527 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m08:19:18.632500 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m08:19:18.633471 [info ] [Thread-2  ]: 6 of 35 START test not_null_genre_popularity_first_rating_date ................. [RUN]
[0m08:19:18.634887 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9, now test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32)
[0m08:19:18.635599 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m08:19:18.642100 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m08:19:18.652667 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (compile): 08:19:18.636176 => 08:19:18.652296
[0m08:19:18.653464 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m08:19:18.657530 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m08:19:18.667589 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:18.668299 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m08:19:18.669103 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where first_rating_date is null



      
    ) dbt_internal_test
[0m08:19:18.669831 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:19.391938 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m08:19:19.398652 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (execute): 08:19:18.653926 => 08:19:19.398107
[0m08:19:19.399710 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: ROLLBACK
[0m08:19:19.400624 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:19.401494 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: Close
[0m08:19:19.520013 [info ] [Thread-2  ]: 6 of 35 PASS not_null_genre_popularity_first_rating_date ....................... [[32mPASS[0m in 0.88s]
[0m08:19:19.522189 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m08:19:19.523564 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m08:19:19.524622 [info ] [Thread-2  ]: 7 of 35 START test not_null_genre_popularity_genres ............................ [RUN]
[0m08:19:19.526888 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32, now test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa)
[0m08:19:19.527982 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m08:19:19.553437 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m08:19:19.574144 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (compile): 08:19:19.528793 => 08:19:19.573171
[0m08:19:19.575642 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m08:19:19.584452 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m08:19:19.603667 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:19.604834 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m08:19:19.605915 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`genre_popularity`
where genres is null



      
    ) dbt_internal_test
[0m08:19:19.606901 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:20.382879 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m08:19:20.389492 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (execute): 08:19:19.576595 => 08:19:20.388989
[0m08:19:20.390749 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: ROLLBACK
[0m08:19:20.392369 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:20.393450 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: Close
[0m08:19:20.503980 [info ] [Thread-2  ]: 7 of 35 PASS not_null_genre_popularity_genres .................................. [[32mPASS[0m in 0.98s]
[0m08:19:20.506709 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m08:19:20.508152 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m08:19:20.509353 [info ] [Thread-2  ]: 8 of 35 START test not_null_genre_popularity_last_rating_date .................. [RUN]
[0m08:19:20.513863 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa, now test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432)
[0m08:19:20.515268 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m08:19:20.529115 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m08:19:20.548458 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (compile): 08:19:20.516208 => 08:19:20.547811
[0m08:19:20.549592 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m08:19:20.557517 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m08:19:20.575931 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:20.577373 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m08:19:20.578590 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where last_rating_date is null



      
    ) dbt_internal_test
[0m08:19:20.579722 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:21.337553 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m08:19:21.343988 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (execute): 08:19:20.550583 => 08:19:21.343282
[0m08:19:21.345222 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: ROLLBACK
[0m08:19:21.346469 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:21.347511 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: Close
[0m08:19:21.462501 [info ] [Thread-2  ]: 8 of 35 PASS not_null_genre_popularity_last_rating_date ........................ [[32mPASS[0m in 0.95s]
[0m08:19:21.465067 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m08:19:21.466714 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m08:19:21.468036 [info ] [Thread-2  ]: 9 of 35 START test not_null_genre_popularity_popularity_rank ................... [RUN]
[0m08:19:21.470241 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432, now test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc)
[0m08:19:21.471420 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m08:19:21.486982 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m08:19:21.507724 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (compile): 08:19:21.472156 => 08:19:21.506644
[0m08:19:21.509494 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m08:19:21.524977 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m08:19:21.557062 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:21.558502 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m08:19:21.559843 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select popularity_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where popularity_rank is null



      
    ) dbt_internal_test
[0m08:19:21.561254 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:22.458143 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m08:19:22.465479 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (execute): 08:19:21.510798 => 08:19:22.464793
[0m08:19:22.467716 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: ROLLBACK
[0m08:19:22.468970 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:22.470312 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: Close
[0m08:19:22.580539 [info ] [Thread-2  ]: 9 of 35 PASS not_null_genre_popularity_popularity_rank ......................... [[32mPASS[0m in 1.11s]
[0m08:19:22.583262 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m08:19:22.584762 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m08:19:22.586040 [info ] [Thread-2  ]: 10 of 35 START test not_null_genre_popularity_quality_rank ..................... [RUN]
[0m08:19:22.587729 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc, now test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a)
[0m08:19:22.588762 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m08:19:22.604033 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m08:19:22.621089 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (compile): 08:19:22.589569 => 08:19:22.620594
[0m08:19:22.622271 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m08:19:22.628378 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m08:19:22.648057 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:22.649166 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m08:19:22.650174 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select quality_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where quality_rank is null



      
    ) dbt_internal_test
[0m08:19:22.651181 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:23.502604 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m08:19:23.509311 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (execute): 08:19:22.623140 => 08:19:23.508865
[0m08:19:23.510444 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: ROLLBACK
[0m08:19:23.511490 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:23.512505 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: Close
[0m08:19:23.624344 [info ] [Thread-2  ]: 10 of 35 PASS not_null_genre_popularity_quality_rank ........................... [[32mPASS[0m in 1.04s]
[0m08:19:23.626761 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m08:19:23.628032 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m08:19:23.630143 [info ] [Thread-2  ]: 11 of 35 START test not_null_genre_popularity_rating_count ..................... [RUN]
[0m08:19:23.632139 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a, now test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856)
[0m08:19:23.633376 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m08:19:23.652458 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m08:19:23.683462 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (compile): 08:19:23.634222 => 08:19:23.682418
[0m08:19:23.684994 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m08:19:23.692039 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m08:19:23.709919 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:23.711244 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m08:19:23.712194 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`genre_popularity`
where rating_count is null



      
    ) dbt_internal_test
[0m08:19:23.713407 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:24.562351 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m08:19:24.567901 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (execute): 08:19:23.685783 => 08:19:24.567481
[0m08:19:24.569153 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: ROLLBACK
[0m08:19:24.570170 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:24.571238 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: Close
[0m08:19:24.683004 [info ] [Thread-2  ]: 11 of 35 PASS not_null_genre_popularity_rating_count ........................... [[32mPASS[0m in 1.05s]
[0m08:19:24.685206 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m08:19:24.686641 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:19:24.687829 [info ] [Thread-2  ]: 12 of 35 START test not_null_movie_enriched_avg_rating ......................... [RUN]
[0m08:19:24.689561 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856, now test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd)
[0m08:19:24.690756 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:19:24.699717 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m08:19:24.719466 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (compile): 08:19:24.691849 => 08:19:24.718715
[0m08:19:24.720922 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:19:24.728827 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m08:19:24.747965 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:24.749028 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m08:19:24.750160 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating is null



      
    ) dbt_internal_test
[0m08:19:24.751191 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:25.607336 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m08:19:25.615908 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (execute): 08:19:24.721817 => 08:19:25.615292
[0m08:19:25.617890 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: ROLLBACK
[0m08:19:25.619135 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:25.620048 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: Close
[0m08:19:25.728272 [info ] [Thread-2  ]: 12 of 35 PASS not_null_movie_enriched_avg_rating ............................... [[32mPASS[0m in 1.04s]
[0m08:19:25.731095 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:19:25.732639 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:19:25.734129 [info ] [Thread-2  ]: 13 of 35 START test not_null_movie_enriched_genres ............................. [RUN]
[0m08:19:25.736423 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd, now test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27)
[0m08:19:25.737833 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:19:25.750579 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m08:19:25.769917 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (compile): 08:19:25.738753 => 08:19:25.768985
[0m08:19:25.771538 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:19:25.802586 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m08:19:25.823072 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:25.824315 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m08:19:25.825330 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`movie_enriched`
where genres is null



      
    ) dbt_internal_test
[0m08:19:25.826367 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:26.677450 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m08:19:26.685531 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (execute): 08:19:25.772772 => 08:19:26.684929
[0m08:19:26.686670 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: ROLLBACK
[0m08:19:26.687866 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:26.688890 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: Close
[0m08:19:26.802647 [info ] [Thread-2  ]: 13 of 35 PASS not_null_movie_enriched_genres ................................... [[32mPASS[0m in 1.07s]
[0m08:19:26.804729 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:19:26.805763 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:19:26.806645 [info ] [Thread-2  ]: 14 of 35 START test not_null_movie_enriched_movie_id ........................... [RUN]
[0m08:19:26.808377 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27, now test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86)
[0m08:19:26.838026 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:19:26.885962 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m08:19:26.913593 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (compile): 08:19:26.865806 => 08:19:26.912630
[0m08:19:26.915091 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:19:26.923272 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m08:19:26.947244 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:26.949125 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m08:19:26.950753 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is null



      
    ) dbt_internal_test
[0m08:19:26.952292 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:27.706088 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m08:19:27.717542 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (execute): 08:19:26.916046 => 08:19:27.716773
[0m08:19:27.719091 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: ROLLBACK
[0m08:19:27.720247 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:27.721338 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: Close
[0m08:19:27.824035 [info ] [Thread-2  ]: 14 of 35 PASS not_null_movie_enriched_movie_id ................................. [[32mPASS[0m in 1.02s]
[0m08:19:27.826240 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:19:27.827523 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:19:27.828732 [info ] [Thread-2  ]: 15 of 35 START test not_null_movie_enriched_title .............................. [RUN]
[0m08:19:27.831713 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86, now test.my_dbt_project.not_null_movie_enriched_title.d20f00c292)
[0m08:19:27.832850 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:19:27.848965 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m08:19:27.866952 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (compile): 08:19:27.833862 => 08:19:27.866314
[0m08:19:27.868099 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:19:27.874938 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m08:19:27.892474 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:27.893884 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m08:19:27.895226 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`movie_enriched`
where title is null



      
    ) dbt_internal_test
[0m08:19:27.896610 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:28.677208 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m08:19:28.683893 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (execute): 08:19:27.869105 => 08:19:28.683396
[0m08:19:28.685307 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: ROLLBACK
[0m08:19:28.686455 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:28.687494 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: Close
[0m08:19:28.803152 [info ] [Thread-2  ]: 15 of 35 PASS not_null_movie_enriched_title .................................... [[32mPASS[0m in 0.97s]
[0m08:19:28.805448 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:19:28.807054 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:19:28.808524 [info ] [Thread-2  ]: 16 of 35 START test not_null_movie_enriched_total_ratings ...................... [RUN]
[0m08:19:28.812193 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_title.d20f00c292, now test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04)
[0m08:19:28.813679 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:19:28.824754 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m08:19:28.845447 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (compile): 08:19:28.815012 => 08:19:28.844792
[0m08:19:28.846617 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:19:28.865338 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m08:19:28.887463 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:28.888926 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m08:19:28.890047 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select total_ratings
from `workspace`.`movielens_volume`.`movie_enriched`
where total_ratings is null



      
    ) dbt_internal_test
[0m08:19:28.891205 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:29.629275 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m08:19:29.636721 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (execute): 08:19:28.847840 => 08:19:29.636058
[0m08:19:29.638195 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: ROLLBACK
[0m08:19:29.639661 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:29.641188 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: Close
[0m08:19:29.763252 [info ] [Thread-2  ]: 16 of 35 PASS not_null_movie_enriched_total_ratings ............................ [[32mPASS[0m in 0.95s]
[0m08:19:29.765416 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:19:29.767054 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m08:19:29.768271 [info ] [Thread-2  ]: 17 of 35 START test not_null_movie_performance_genre_rank ...................... [RUN]
[0m08:19:29.770278 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04, now test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e)
[0m08:19:29.771572 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m08:19:29.786720 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m08:19:29.808957 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (compile): 08:19:29.772454 => 08:19:29.808350
[0m08:19:29.810426 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m08:19:29.817949 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m08:19:29.838623 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:29.840042 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m08:19:29.841020 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genre_rank
from `workspace`.`movielens_volume`.`movie_performance`
where genre_rank is null



      
    ) dbt_internal_test
[0m08:19:29.842326 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:30.668545 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m08:19:30.674643 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (execute): 08:19:29.811498 => 08:19:30.674183
[0m08:19:30.676113 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: ROLLBACK
[0m08:19:30.677404 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:30.678533 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: Close
[0m08:19:30.782349 [info ] [Thread-2  ]: 17 of 35 PASS not_null_movie_performance_genre_rank ............................ [[32mPASS[0m in 1.01s]
[0m08:19:30.784488 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m08:19:30.785748 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m08:19:30.787286 [info ] [Thread-2  ]: 18 of 35 START test not_null_movie_performance_movie_id ........................ [RUN]
[0m08:19:30.789754 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e, now test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52)
[0m08:19:30.791514 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m08:19:30.807048 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m08:19:30.824950 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (compile): 08:19:30.792324 => 08:19:30.824198
[0m08:19:30.826277 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m08:19:30.832772 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m08:19:30.852821 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:30.854273 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m08:19:30.855614 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is null



      
    ) dbt_internal_test
[0m08:19:30.856716 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:32.063303 [debug] [Thread-2  ]: SQL status: OK in 1.2100000381469727 seconds
[0m08:19:32.071181 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (execute): 08:19:30.827116 => 08:19:32.070592
[0m08:19:32.072838 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: ROLLBACK
[0m08:19:32.074144 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:32.075167 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: Close
[0m08:19:32.185897 [info ] [Thread-2  ]: 18 of 35 PASS not_null_movie_performance_movie_id .............................. [[32mPASS[0m in 1.40s]
[0m08:19:32.188178 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m08:19:32.189833 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m08:19:32.191275 [info ] [Thread-2  ]: 19 of 35 START test not_null_movie_performance_polarization_score .............. [RUN]
[0m08:19:32.193617 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52, now test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6)
[0m08:19:32.195161 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m08:19:32.210721 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m08:19:32.231618 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (compile): 08:19:32.196084 => 08:19:32.230849
[0m08:19:32.232915 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m08:19:32.243203 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m08:19:32.262636 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:32.264266 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m08:19:32.265437 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select polarization_score
from `workspace`.`movielens_volume`.`movie_performance`
where polarization_score is null



      
    ) dbt_internal_test
[0m08:19:32.266590 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:33.155947 [debug] [Thread-2  ]: SQL status: OK in 0.8899999856948853 seconds
[0m08:19:33.165407 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (execute): 08:19:32.233902 => 08:19:33.164744
[0m08:19:33.167632 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: ROLLBACK
[0m08:19:33.169214 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:33.170379 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: Close
[0m08:19:33.279016 [info ] [Thread-2  ]: 19 of 35 PASS not_null_movie_performance_polarization_score .................... [[32mPASS[0m in 1.09s]
[0m08:19:33.281333 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m08:19:33.282730 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m08:19:33.284296 [info ] [Thread-2  ]: 20 of 35 START test not_null_movie_performance_rating_count .................... [RUN]
[0m08:19:33.286249 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6, now test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac)
[0m08:19:33.287569 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m08:19:33.304798 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m08:19:33.325724 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (compile): 08:19:33.288567 => 08:19:33.324852
[0m08:19:33.327205 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m08:19:33.335613 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m08:19:33.355318 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:33.356403 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m08:19:33.357478 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`movie_performance`
where rating_count is null



      
    ) dbt_internal_test
[0m08:19:33.359000 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:34.210555 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m08:19:34.217270 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (execute): 08:19:33.328122 => 08:19:34.216761
[0m08:19:34.218613 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: ROLLBACK
[0m08:19:34.220046 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:34.221141 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: Close
[0m08:19:34.336995 [info ] [Thread-2  ]: 20 of 35 PASS not_null_movie_performance_rating_count .......................... [[32mPASS[0m in 1.05s]
[0m08:19:34.339109 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m08:19:34.340609 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m08:19:34.341994 [info ] [Thread-2  ]: 21 of 35 START test not_null_movie_performance_top_rating_percentage ........... [RUN]
[0m08:19:34.343907 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac, now test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb)
[0m08:19:34.345065 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m08:19:34.354239 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m08:19:34.372810 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (compile): 08:19:34.345928 => 08:19:34.372088
[0m08:19:34.374022 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m08:19:34.379815 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m08:19:34.398681 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:34.399660 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m08:19:34.400690 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_rating_percentage
from `workspace`.`movielens_volume`.`movie_performance`
where top_rating_percentage is null



      
    ) dbt_internal_test
[0m08:19:34.402079 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:35.417089 [debug] [Thread-2  ]: SQL status: OK in 1.0199999809265137 seconds
[0m08:19:35.423488 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (execute): 08:19:34.374970 => 08:19:35.422861
[0m08:19:35.424720 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: ROLLBACK
[0m08:19:35.426039 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:35.427394 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: Close
[0m08:19:35.536874 [info ] [Thread-2  ]: 21 of 35 PASS not_null_movie_performance_top_rating_percentage ................. [[32mPASS[0m in 1.19s]
[0m08:19:35.538945 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m08:19:35.540404 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m08:19:35.541738 [info ] [Thread-2  ]: 22 of 35 START test not_null_movie_performance_weighted_avg_rating ............. [RUN]
[0m08:19:35.543793 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb, now test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5)
[0m08:19:35.545101 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m08:19:35.557042 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m08:19:35.576407 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (compile): 08:19:35.546023 => 08:19:35.575866
[0m08:19:35.577587 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m08:19:35.594579 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m08:19:35.613448 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:35.614546 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m08:19:35.615681 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select weighted_avg_rating
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating is null



      
    ) dbt_internal_test
[0m08:19:35.616830 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:36.569996 [debug] [Thread-2  ]: SQL status: OK in 0.949999988079071 seconds
[0m08:19:36.577039 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (execute): 08:19:35.578585 => 08:19:36.576442
[0m08:19:36.578218 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: ROLLBACK
[0m08:19:36.579308 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:36.580421 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: Close
[0m08:19:36.690916 [info ] [Thread-2  ]: 22 of 35 PASS not_null_movie_performance_weighted_avg_rating ................... [[32mPASS[0m in 1.15s]
[0m08:19:36.693470 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m08:19:36.695020 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m08:19:36.696258 [info ] [Thread-2  ]: 23 of 35 START test not_null_user_activity_avg_user_rating ..................... [RUN]
[0m08:19:36.698164 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5, now test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b)
[0m08:19:36.699392 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m08:19:36.709341 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m08:19:36.730976 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (compile): 08:19:36.700197 => 08:19:36.730222
[0m08:19:36.732453 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m08:19:36.739229 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m08:19:36.760317 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:36.761522 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m08:19:36.762519 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_user_rating
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating is null



      
    ) dbt_internal_test
[0m08:19:36.763914 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:39.066897 [debug] [Thread-2  ]: SQL status: OK in 2.299999952316284 seconds
[0m08:19:39.073747 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (execute): 08:19:36.733907 => 08:19:39.073274
[0m08:19:39.075074 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: ROLLBACK
[0m08:19:39.076386 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:39.077933 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: Close
[0m08:19:39.189800 [info ] [Thread-2  ]: 23 of 35 PASS not_null_user_activity_avg_user_rating ........................... [[32mPASS[0m in 2.49s]
[0m08:19:39.192154 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m08:19:39.193420 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m08:19:39.194747 [info ] [Thread-2  ]: 24 of 35 START test not_null_user_activity_first_activity_date ................. [RUN]
[0m08:19:39.196812 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b, now test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac)
[0m08:19:39.198300 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m08:19:39.208876 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m08:19:39.228061 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (compile): 08:19:39.199231 => 08:19:39.227511
[0m08:19:39.229368 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m08:19:39.237037 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m08:19:39.256954 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:39.258129 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m08:19:39.259295 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where first_activity_date is null



      
    ) dbt_internal_test
[0m08:19:39.260469 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:40.196743 [debug] [Thread-2  ]: SQL status: OK in 0.9399999976158142 seconds
[0m08:19:40.203052 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (execute): 08:19:39.230389 => 08:19:40.202500
[0m08:19:40.205363 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: ROLLBACK
[0m08:19:40.206468 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:40.207679 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: Close
[0m08:19:40.320408 [info ] [Thread-2  ]: 24 of 35 PASS not_null_user_activity_first_activity_date ....................... [[32mPASS[0m in 1.12s]
[0m08:19:40.322248 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m08:19:40.323989 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m08:19:40.325772 [info ] [Thread-2  ]: 25 of 35 START test not_null_user_activity_influence_score ..................... [RUN]
[0m08:19:40.327771 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac, now test.my_dbt_project.not_null_user_activity_influence_score.758852d325)
[0m08:19:40.328895 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m08:19:40.339388 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m08:19:40.357768 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (compile): 08:19:40.329749 => 08:19:40.356761
[0m08:19:40.359594 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m08:19:40.380902 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m08:19:40.399602 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:40.400675 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m08:19:40.401672 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select influence_score
from `workspace`.`movielens_volume`.`user_activity`
where influence_score is null



      
    ) dbt_internal_test
[0m08:19:40.402920 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:41.259104 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m08:19:41.265690 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (execute): 08:19:40.360351 => 08:19:41.264899
[0m08:19:41.267338 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: ROLLBACK
[0m08:19:41.268444 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:41.269552 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: Close
[0m08:19:41.383844 [info ] [Thread-2  ]: 25 of 35 PASS not_null_user_activity_influence_score ........................... [[32mPASS[0m in 1.06s]
[0m08:19:41.385929 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m08:19:41.387319 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m08:19:41.388543 [info ] [Thread-2  ]: 26 of 35 START test not_null_user_activity_last_activity_date .................. [RUN]
[0m08:19:41.390287 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_influence_score.758852d325, now test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad)
[0m08:19:41.391704 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m08:19:41.404303 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m08:19:41.423009 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (compile): 08:19:41.392670 => 08:19:41.422459
[0m08:19:41.424299 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m08:19:41.432437 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m08:19:41.452028 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:41.453750 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m08:19:41.454799 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where last_activity_date is null



      
    ) dbt_internal_test
[0m08:19:41.455846 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:42.364802 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m08:19:42.371575 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (execute): 08:19:41.425505 => 08:19:42.370971
[0m08:19:42.373071 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: ROLLBACK
[0m08:19:42.374387 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:42.375726 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: Close
[0m08:19:42.484683 [info ] [Thread-2  ]: 26 of 35 PASS not_null_user_activity_last_activity_date ........................ [[32mPASS[0m in 1.09s]
[0m08:19:42.487518 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m08:19:42.489025 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m08:19:42.490145 [info ] [Thread-2  ]: 27 of 35 START test not_null_user_activity_top_genre ........................... [RUN]
[0m08:19:42.493050 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad, now test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313)
[0m08:19:42.494379 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m08:19:42.508591 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m08:19:42.528770 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (compile): 08:19:42.495310 => 08:19:42.528193
[0m08:19:42.529966 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m08:19:42.537691 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m08:19:42.557401 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:42.558583 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m08:19:42.559836 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_genre
from `workspace`.`movielens_volume`.`user_activity`
where top_genre is null



      
    ) dbt_internal_test
[0m08:19:42.561061 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:43.624795 [debug] [Thread-2  ]: SQL status: OK in 1.059999942779541 seconds
[0m08:19:43.631445 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (execute): 08:19:42.531015 => 08:19:43.630996
[0m08:19:43.632947 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: ROLLBACK
[0m08:19:43.634065 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:43.635207 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: Close
[0m08:19:43.740779 [info ] [Thread-2  ]: 27 of 35 PASS not_null_user_activity_top_genre ................................. [[32mPASS[0m in 1.25s]
[0m08:19:43.742808 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m08:19:43.744036 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m08:19:43.745232 [info ] [Thread-2  ]: 28 of 35 START test not_null_user_activity_user_id ............................. [RUN]
[0m08:19:43.747518 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313, now test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36)
[0m08:19:43.748573 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m08:19:43.762684 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m08:19:43.781218 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (compile): 08:19:43.749419 => 08:19:43.780661
[0m08:19:43.782560 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m08:19:43.792827 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m08:19:43.813411 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:43.814488 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m08:19:43.815377 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`user_activity`
where user_id is null



      
    ) dbt_internal_test
[0m08:19:43.816451 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:44.579144 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m08:19:44.585669 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (execute): 08:19:43.783465 => 08:19:44.585126
[0m08:19:44.587371 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: ROLLBACK
[0m08:19:44.588985 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:44.590089 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: Close
[0m08:19:44.709707 [info ] [Thread-2  ]: 28 of 35 PASS not_null_user_activity_user_id ................................... [[32mPASS[0m in 0.96s]
[0m08:19:44.711595 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m08:19:44.713526 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m08:19:44.715130 [info ] [Thread-2  ]: 29 of 35 START test not_null_user_activity_user_rating_count ................... [RUN]
[0m08:19:44.717113 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36, now test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e)
[0m08:19:44.718501 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m08:19:44.735589 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m08:19:44.756619 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (compile): 08:19:44.719467 => 08:19:44.755973
[0m08:19:44.757772 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m08:19:44.767743 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m08:19:44.799067 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:44.800529 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m08:19:44.801617 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_rating_count
from `workspace`.`movielens_volume`.`user_activity`
where user_rating_count is null



      
    ) dbt_internal_test
[0m08:19:44.802774 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:45.600356 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m08:19:45.606630 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (execute): 08:19:44.758585 => 08:19:45.606159
[0m08:19:45.608027 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: ROLLBACK
[0m08:19:45.609295 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:45.610394 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: Close
[0m08:19:45.715730 [info ] [Thread-2  ]: 29 of 35 PASS not_null_user_activity_user_rating_count ......................... [[32mPASS[0m in 1.00s]
[0m08:19:45.717689 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m08:19:45.719069 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:19:45.720397 [info ] [Thread-2  ]: 30 of 35 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:19:45.722468 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m08:19:45.723703 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:19:45.742043 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:19:45.760269 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 08:19:45.724473 => 08:19:45.759676
[0m08:19:45.761879 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:19:45.768392 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:19:45.787632 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:45.788959 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:19:45.790241 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:19:45.810786 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:46.567446 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m08:19:46.573879 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 08:19:45.762894 => 08:19:46.573422
[0m08:19:46.575101 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m08:19:46.576461 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:46.578622 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m08:19:46.687168 [info ] [Thread-2  ]: 30 of 35 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.97s]
[0m08:19:46.688972 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:19:46.690280 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:19:46.691624 [info ] [Thread-2  ]: 31 of 35 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:19:46.693462 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m08:19:46.694614 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:19:46.707563 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:19:46.727749 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 08:19:46.695481 => 08:19:46.726919
[0m08:19:46.729204 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:19:46.736127 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:19:46.756722 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:46.758157 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:19:46.759157 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:19:46.760226 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:47.373233 [debug] [Thread-2  ]: SQL status: OK in 0.6100000143051147 seconds
[0m08:19:47.380331 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 08:19:46.730266 => 08:19:47.379781
[0m08:19:47.381627 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m08:19:47.382769 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:47.383863 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m08:19:47.495344 [info ] [Thread-2  ]: 31 of 35 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.80s]
[0m08:19:47.497510 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:19:47.498793 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m08:19:47.500049 [info ] [Thread-2  ]: 32 of 35 START test unique_genre_popularity_genres ............................. [RUN]
[0m08:19:47.501931 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075)
[0m08:19:47.503405 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m08:19:47.518085 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m08:19:47.538224 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (compile): 08:19:47.504258 => 08:19:47.537624
[0m08:19:47.539436 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m08:19:47.546613 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m08:19:47.566877 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:47.568155 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m08:19:47.569416 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    genres as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`genre_popularity`
where genres is not null
group by genres
having count(*) > 1



      
    ) dbt_internal_test
[0m08:19:47.570505 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:48.412619 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m08:19:48.422370 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (execute): 08:19:47.540249 => 08:19:48.421068
[0m08:19:48.423914 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: ROLLBACK
[0m08:19:48.425329 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:48.426867 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: Close
[0m08:19:48.540847 [info ] [Thread-2  ]: 32 of 35 PASS unique_genre_popularity_genres ................................... [[32mPASS[0m in 1.04s]
[0m08:19:48.542786 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m08:19:48.544176 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:19:48.545498 [info ] [Thread-2  ]: 33 of 35 START test unique_movie_enriched_movie_id ............................. [RUN]
[0m08:19:48.547938 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075, now test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a)
[0m08:19:48.549021 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:19:48.559061 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m08:19:48.578446 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (compile): 08:19:48.549984 => 08:19:48.577649
[0m08:19:48.580048 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:19:48.586769 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m08:19:48.605398 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:48.606595 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m08:19:48.607792 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:19:48.609103 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:49.408484 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m08:19:49.415036 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (execute): 08:19:48.581095 => 08:19:49.414592
[0m08:19:49.417129 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: ROLLBACK
[0m08:19:49.418623 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:49.419804 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m08:19:49.526515 [info ] [Thread-2  ]: 33 of 35 PASS unique_movie_enriched_movie_id ................................... [[32mPASS[0m in 0.98s]
[0m08:19:49.528890 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:19:49.530230 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m08:19:49.531385 [info ] [Thread-2  ]: 34 of 35 START test unique_movie_performance_movie_id .......................... [RUN]
[0m08:19:49.533461 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a, now test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22)
[0m08:19:49.534522 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m08:19:49.545574 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m08:19:49.565781 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (compile): 08:19:49.535389 => 08:19:49.565233
[0m08:19:49.567166 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m08:19:49.574087 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m08:19:49.592992 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:49.594062 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m08:19:49.595164 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:19:49.596381 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:50.392849 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m08:19:50.411189 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (execute): 08:19:49.568399 => 08:19:50.408142
[0m08:19:50.412538 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: ROLLBACK
[0m08:19:50.413929 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:50.415963 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: Close
[0m08:19:50.530935 [info ] [Thread-2  ]: 34 of 35 PASS unique_movie_performance_movie_id ................................ [[32mPASS[0m in 1.00s]
[0m08:19:50.533188 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m08:19:50.534695 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m08:19:50.536059 [info ] [Thread-2  ]: 35 of 35 START test unique_user_activity_user_id ............................... [RUN]
[0m08:19:50.538114 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22, now test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda)
[0m08:19:50.539317 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m08:19:50.549102 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m08:19:50.569225 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (compile): 08:19:50.540381 => 08:19:50.568665
[0m08:19:50.570391 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m08:19:50.577191 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m08:19:50.596066 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:50.597090 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m08:19:50.598145 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`user_activity`
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:19:50.599517 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:19:51.427334 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m08:19:51.433846 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (execute): 08:19:50.571349 => 08:19:51.433406
[0m08:19:51.435097 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: ROLLBACK
[0m08:19:51.437180 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:19:51.439003 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: Close
[0m08:19:51.552319 [info ] [Thread-2  ]: 35 of 35 PASS unique_user_activity_user_id ..................................... [[32mPASS[0m in 1.01s]
[0m08:19:51.557688 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m08:19:51.563219 [debug] [MainThread]: On master: ROLLBACK
[0m08:19:51.564958 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:19:52.021776 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:19:52.023251 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:19:52.024184 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:19:52.025404 [debug] [MainThread]: On master: ROLLBACK
[0m08:19:52.026632 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:19:52.027839 [debug] [MainThread]: On master: Close
[0m08:19:52.131656 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:19:52.132865 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda' was properly closed.
[0m08:19:52.136506 [info ] [MainThread]: 
[0m08:19:52.139186 [info ] [MainThread]: Finished running 35 tests in 0 hours 0 minutes and 39.53 seconds (39.53s).
[0m08:19:52.151669 [debug] [MainThread]: Command end result
[0m08:19:52.203927 [info ] [MainThread]: 
[0m08:19:52.205559 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:19:52.206767 [info ] [MainThread]: 
[0m08:19:52.208540 [info ] [MainThread]: Done. PASS=35 WARN=0 ERROR=0 SKIP=0 TOTAL=35
[0m08:19:52.211696 [debug] [MainThread]: Command `dbt test` succeeded at 08:19:52.211333 after 42.05 seconds
[0m08:19:52.213008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d33a43550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d06d1a850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d069e65e0>]}
[0m08:19:52.214206 [debug] [MainThread]: Flushing usage events
[0m10:29:48.859827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B4068BB60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3E768550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B41A60E10>]}


============================== 10:29:48.864981 | 61af5deb-2789-4d3b-a77e-e3c9b077cbee ==============================
[0m10:29:48.864981 [info ] [MainThread]: Running with dbt=1.10.9
[0m10:29:48.865657 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'version_check': 'True', 'invocation_command': 'dbt debug', 'log_format': 'default', 'warn_error': 'None', 'introspect': 'True', 'debug': 'False', 'log_cache_events': 'False', 'fail_fast': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'printer_width': '80', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\38349\\projects\\end_to_end_de_project\\dbt\\logs', 'indirect_selection': 'eager', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': 'C:\\Users\\38349\\projects\\end_to_end_de_project\\dbt'}
[0m10:29:48.889286 [info ] [MainThread]: dbt version: 1.10.9
[0m10:29:48.889829 [info ] [MainThread]: python version: 3.13.7
[0m10:29:48.890275 [info ] [MainThread]: python path: C:\Users\38349\AppData\Local\Programs\Python\Python313\python.exe
[0m10:29:48.890735 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m10:29:49.849700 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:29:49.850153 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:29:49.850425 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:29:50.525578 [info ] [MainThread]: Using profiles dir at C:\Users\38349\projects\end_to_end_de_project\dbt
[0m10:29:50.526196 [info ] [MainThread]: Using profiles.yml file at C:\Users\38349\projects\end_to_end_de_project\dbt\profiles.yml
[0m10:29:50.526694 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\38349\projects\end_to_end_de_project\dbt\dbt_project.yml
[0m10:29:50.527123 [info ] [MainThread]: adapter type: databricks
[0m10:29:50.527536 [info ] [MainThread]: adapter version: 1.10.11
[0m10:29:50.625777 [info ] [MainThread]: Configuration:
[0m10:29:50.626994 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m10:29:50.627440 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:29:50.627812 [info ] [MainThread]: Required dependencies:
[0m10:29:50.628204 [debug] [MainThread]: Executing "git --help"
[0m10:29:50.664265 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:29:50.664736 [debug] [MainThread]: STDERR: "b''"
[0m10:29:50.665107 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:29:50.665556 [info ] [MainThread]: Connection:
[0m10:29:50.665986 [info ] [MainThread]:   host: dbc-5f9e5db3-0d8c.cloud.databricks.com
[0m10:29:50.666350 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/ca437d60fdb2332b
[0m10:29:50.666706 [info ] [MainThread]:   catalog: workspace
[0m10:29:50.667020 [info ] [MainThread]:   schema: movielens_volume
[0m10:29:50.667553 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m10:29:51.285701 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m10:29:51.286069 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m10:29:51.286325 [debug] [MainThread]: Using databricks connection "debug"
[0m10:29:51.286579 [debug] [MainThread]: On debug: select 1 as id
[0m10:29:51.286808 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:29:51.669198 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f09ab2-f92d-1e72-94fd-2bb87e0cbe4c) - Created
[0m10:29:51.962073 [debug] [MainThread]: SQL status: OK in 0.680 seconds
[0m10:29:51.963387 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f09ab2-f92d-1e72-94fd-2bb87e0cbe4c, command-id=01f09ab2-f93e-1c3e-b1b1-c6fa7668e919) - Closing
[0m10:29:51.964407 [debug] [MainThread]: On debug: Close
[0m10:29:51.964877 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f09ab2-f92d-1e72-94fd-2bb87e0cbe4c) - Closing
[0m10:29:52.070371 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m10:29:52.071034 [info ] [MainThread]: [32mAll checks passed![0m
[0m10:29:52.072282 [debug] [MainThread]: Command `dbt debug` succeeded at 10:29:52.072153 after 3.38 seconds
[0m10:29:52.072662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5D0AF100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B5D0EB890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B441D0160>]}
[0m10:29:52.073050 [debug] [MainThread]: Flushing usage events
[0m10:29:52.598914 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:30:33.948494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B21F8F3B60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B21D9A8550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B220CBCE10>]}


============================== 10:30:33.953272 | 1aa7928c-a985-49f4-ab71-15e6b0494d6b ==============================
[0m10:30:33.953272 [info ] [MainThread]: Running with dbt=1.10.9
[0m10:30:33.953867 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'version_check': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'warn_error': 'None', 'introspect': 'True', 'debug': 'False', 'log_cache_events': 'False', 'fail_fast': 'False', 'no_print': 'None', 'use_experimental_parser': 'False', 'quiet': 'False', 'empty': 'False', 'printer_width': '80', 'indirect_selection': 'eager', 'log_path': 'C:\\Users\\38349\\projects\\end_to_end_de_project\\dbt\\logs', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': 'C:\\Users\\38349\\projects\\end_to_end_de_project\\dbt'}
[0m10:30:34.935184 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:30:34.935686 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:30:34.935987 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:30:35.905588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B220041810>]}
[0m10:30:35.969195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B223425480>]}
[0m10:30:35.969966 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m10:30:36.672651 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m10:30:36.770062 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m10:30:36.770716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23C7A9250>]}
[0m10:30:40.858537 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships`. Arguments to generic tests
should be nested under the `arguments` property.`
[0m10:30:40.859283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23C26E7B0>]}
[0m10:30:41.142122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23CB46430>]}
[0m10:30:41.255383 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\38349\projects\end_to_end_de_project\dbt\target\manifest.json
[0m10:30:41.277677 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\38349\projects\end_to_end_de_project\dbt\target\semantic_manifest.json
[0m10:30:41.307737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23C87E4E0>]}
[0m10:30:41.308445 [info ] [MainThread]: Found 9 models, 55 data tests, 4 sources, 803 macros
[0m10:30:41.309069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23CA74050>]}
[0m10:30:41.312970 [info ] [MainThread]: 
[0m10:30:41.313722 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:30:41.314276 [info ] [MainThread]: 
[0m10:30:41.315249 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m10:30:41.315668 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:30:41.325730 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m10:30:41.326381 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m10:30:41.339229 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m10:30:41.339756 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m10:30:41.340086 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:30:41.932766 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f09ab3-1723-1f19-8acb-76bd0165720c) - Created
[0m10:30:42.602244 [debug] [ThreadPool]: SQL status: OK in 1.260 seconds
[0m10:30:42.619313 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f09ab3-1723-1f19-8acb-76bd0165720c, command-id=01f09ab3-1734-17d4-a40e-ad57113f2a64) - Closing
[0m10:30:42.620132 [debug] [ThreadPool]: On list_workspace: Close
[0m10:30:42.620581 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f09ab3-1723-1f19-8acb-76bd0165720c) - Closing
[0m10:30:42.759186 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_movielens_volume) - Creating connection
[0m10:30:42.759772 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m10:30:42.768776 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m10:30:42.769219 [debug] [ThreadPool]: On list_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "list_workspace_movielens_volume"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'movielens_volume'

  
[0m10:30:42.769746 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:30:43.142675 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f09ab3-17dc-1304-87df-3fd096bd5bb7) - Created
[0m10:30:44.002135 [debug] [ThreadPool]: SQL status: OK in 1.230 seconds
[0m10:30:44.005577 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f09ab3-17dc-1304-87df-3fd096bd5bb7, command-id=01f09ab3-17ec-1cda-8f81-b10de1932019) - Closing
[0m10:30:44.006415 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m10:30:44.006728 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f09ab3-17dc-1304-87df-3fd096bd5bb7) - Closing
[0m10:30:44.120611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23C9C43C0>]}
[0m10:30:44.128821 [debug] [Thread-1 (]: Began running node model.my_dbt_project.silver_links
[0m10:30:44.129668 [info ] [Thread-1 (]: 1 of 9 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m10:30:44.130550 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.silver_links) - Creating connection
[0m10:30:44.131050 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.silver_links'
[0m10:30:44.131959 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.silver_links
[0m10:30:44.147389 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m10:30:44.148792 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.silver_links
[0m10:30:44.175090 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m10:30:44.175978 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m10:30:44.176926 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23DBBA0D0>]}
[0m10:30:44.221826 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m10:30:44.222885 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.silver_links"
[0m10:30:44.223350 [debug] [Thread-1 (]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
  using delta
      
      
      
      
      
      
      
      as
      with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m10:30:44.223672 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:30:44.585126 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-18b8-1264-976a-c2d307532f8a) - Created
[0m10:30:47.717813 [debug] [Thread-1 (]: SQL status: OK in 3.490 seconds
[0m10:30:47.718938 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-18b8-1264-976a-c2d307532f8a, command-id=01f09ab3-18c9-128d-9d28-c9ea4321ae80) - Closing
[0m10:30:47.733382 [debug] [Thread-1 (]: Applying tags to relation None
[0m10:30:47.750899 [debug] [Thread-1 (]: On model.my_dbt_project.silver_links: Close
[0m10:30:47.751380 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-18b8-1264-976a-c2d307532f8a) - Closing
[0m10:30:47.862784 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B21FFE83D0>]}
[0m10:30:47.863847 [info ] [Thread-1 (]: 1 of 9 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 3.73s]
[0m10:30:47.864680 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.silver_links
[0m10:30:47.865121 [debug] [Thread-1 (]: Began running node model.my_dbt_project.silver_movies
[0m10:30:47.865816 [info ] [Thread-1 (]: 2 of 9 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m10:30:47.866553 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.silver_movies) - Creating connection
[0m10:30:47.866909 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.silver_movies'
[0m10:30:47.867303 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.silver_movies
[0m10:30:47.873148 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m10:30:47.874391 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.silver_movies
[0m10:30:47.876916 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m10:30:47.879336 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m10:30:47.880447 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m10:30:47.880926 [debug] [Thread-1 (]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m10:30:47.881255 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:30:48.234655 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-1ae4-1a82-9d3d-56c2b07c3628) - Created
[0m10:30:50.476320 [debug] [Thread-1 (]: SQL status: OK in 2.590 seconds
[0m10:30:50.477379 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-1ae4-1a82-9d3d-56c2b07c3628, command-id=01f09ab3-1af5-1a56-804d-8affc3b6ac55) - Closing
[0m10:30:50.478100 [debug] [Thread-1 (]: Applying tags to relation None
[0m10:30:50.479135 [debug] [Thread-1 (]: On model.my_dbt_project.silver_movies: Close
[0m10:30:50.479477 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-1ae4-1a82-9d3d-56c2b07c3628) - Closing
[0m10:30:50.598304 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23DBBFB60>]}
[0m10:30:50.599142 [info ] [Thread-1 (]: 2 of 9 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 2.73s]
[0m10:30:50.600099 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.silver_movies
[0m10:30:50.600535 [debug] [Thread-1 (]: Began running node model.my_dbt_project.silver_ratings
[0m10:30:50.601075 [info ] [Thread-1 (]: 3 of 9 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m10:30:50.601804 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.silver_ratings) - Creating connection
[0m10:30:50.602233 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.silver_ratings'
[0m10:30:50.602565 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.silver_ratings
[0m10:30:50.605730 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m10:30:50.606836 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.silver_ratings
[0m10:30:50.610570 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m10:30:50.611940 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m10:30:50.612858 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m10:30:50.613366 [debug] [Thread-1 (]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m10:30:50.613739 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:30:50.983564 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-1c87-147a-ace3-4c70a6fe7017) - Created
[0m10:30:54.280305 [debug] [Thread-1 (]: SQL status: OK in 3.670 seconds
[0m10:30:54.281346 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-1c87-147a-ace3-4c70a6fe7017, command-id=01f09ab3-1c9a-14de-9997-aca6c41c1724) - Closing
[0m10:30:54.282082 [debug] [Thread-1 (]: Applying tags to relation None
[0m10:30:54.283120 [debug] [Thread-1 (]: On model.my_dbt_project.silver_ratings: Close
[0m10:30:54.283463 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-1c87-147a-ace3-4c70a6fe7017) - Closing
[0m10:30:54.398424 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23C9D3890>]}
[0m10:30:54.399172 [info ] [Thread-1 (]: 3 of 9 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 3.80s]
[0m10:30:54.400072 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.silver_ratings
[0m10:30:54.400547 [debug] [Thread-1 (]: Began running node model.my_dbt_project.silver_tags
[0m10:30:54.401088 [info ] [Thread-1 (]: 4 of 9 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m10:30:54.401781 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.silver_tags) - Creating connection
[0m10:30:54.402145 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.silver_tags'
[0m10:30:54.402471 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.silver_tags
[0m10:30:54.407388 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m10:30:54.408470 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.silver_tags
[0m10:30:54.410373 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m10:30:54.411577 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m10:30:54.412383 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m10:30:54.412827 [debug] [Thread-1 (]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m10:30:54.413138 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:30:54.790441 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-1eca-1614-8db1-a7cc54584a1f) - Created
[0m10:30:57.315092 [debug] [Thread-1 (]: SQL status: OK in 2.900 seconds
[0m10:30:57.316176 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-1eca-1614-8db1-a7cc54584a1f, command-id=01f09ab3-1ede-1ad1-9f5c-38f3f731460e) - Closing
[0m10:30:57.316917 [debug] [Thread-1 (]: Applying tags to relation None
[0m10:30:57.317884 [debug] [Thread-1 (]: On model.my_dbt_project.silver_tags: Close
[0m10:30:57.318196 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-1eca-1614-8db1-a7cc54584a1f) - Closing
[0m10:30:57.431750 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23C9D3710>]}
[0m10:30:57.432471 [info ] [Thread-1 (]: 4 of 9 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 3.03s]
[0m10:30:57.433304 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.silver_tags
[0m10:30:57.433738 [debug] [Thread-1 (]: Began running node model.my_dbt_project.genre_popularity
[0m10:30:57.434301 [info ] [Thread-1 (]: 5 of 9 START sql incremental model movielens_volume.genre_popularity ........... [RUN]
[0m10:30:57.435044 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.genre_popularity) - Creating connection
[0m10:30:57.435463 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.genre_popularity'
[0m10:30:57.435796 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.genre_popularity
[0m10:30:57.443534 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.genre_popularity"
[0m10:30:57.444693 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.genre_popularity
[0m10:30:57.477038 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m10:30:57.496980 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:30:57.497457 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'workspace' 
    AND schema_name = 'movielens_volume'
    AND table_name = 'genre_popularity'
  
[0m10:30:57.497781 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:30:57.841295 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-20a1-160a-b664-c416120db0a1) - Created
[0m10:30:58.255414 [debug] [Thread-1 (]: SQL status: OK in 0.760 seconds
[0m10:30:58.257194 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-20b0-116e-a2ed-884fd9291b56) - Closing
[0m10:30:58.262989 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:30:58.263444 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'workspace'
    AND schema_name = 'movielens_volume'
    AND table_name = 'genre_popularity';
  
[0m10:30:58.612397 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m10:30:58.614420 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-20f0-1953-abbe-98c4055d4527) - Closing
[0m10:30:58.620551 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:30:58.621016 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT column_name
  FROM `workspace`.`information_schema`.`columns`
  WHERE table_catalog = 'workspace' 
    AND table_schema = 'movielens_volume'
    AND table_name = 'genre_popularity'
    AND is_nullable = 'NO';
  
[0m10:30:59.166877 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m10:30:59.168705 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-2126-1c9f-852e-10c9c27aeaf2) - Closing
[0m10:30:59.175452 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:30:59.175932 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `workspace`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'workspace' 
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'genre_popularity' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'genre_popularity' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:31:00.184604 [debug] [Thread-1 (]: SQL status: OK in 1.010 seconds
[0m10:31:00.186351 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-2180-17cb-a274-b21aafb7e8ce) - Closing
[0m10:31:00.193551 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:31:00.194021 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `workspace`.information_schema.key_column_usage kcu
  JOIN `workspace`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `workspace`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'workspace'
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'genre_popularity'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'genre_popularity'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:31:01.333243 [debug] [Thread-1 (]: SQL status: OK in 1.140 seconds
[0m10:31:01.335350 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-221d-144b-8415-d435e8a97cd3) - Closing
[0m10:31:01.341262 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:31:01.341686 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'workspace'
    AND table_schema = 'movielens_volume'
    AND table_name = 'genre_popularity';
  
[0m10:31:01.749845 [debug] [Thread-1 (]: SQL status: OK in 0.410 seconds
[0m10:31:01.751963 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-22c7-1dd7-8b4d-abbb6499777c) - Closing
[0m10:31:01.755848 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:31:01.756240 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SHOW TBLPROPERTIES `workspace`.`movielens_volume`.`genre_popularity`
  
[0m10:31:02.057181 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m10:31:02.059224 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-2305-1235-a0b8-2b00c556d684) - Closing
[0m10:31:02.063449 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:31:02.063885 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m10:31:02.400095 [debug] [Thread-1 (]: SQL status: OK in 0.340 seconds
[0m10:31:02.402206 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-2334-148d-a5a5-2044e9cc3f67) - Closing
[0m10:31:02.414278 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={'genres': 'Pipe-separated movie genres', 'rating_count': 'Total number of ratings for this genre', 'avg_rating': 'Average rating for this genre (0–5)', 'popularity_rank': 'Rank of genre by number of ratings', 'quality_rank': 'Rank of genre by average rating', 'first_rating_date': 'Earliest rating date for this genre', 'last_rating_date': 'Most recent rating date for this genre'} quoted={'genres': False, 'rating_count': False, 'avg_rating': False, 'popularity_rank': False, 'quality_rank': False, 'first_rating_date': False, 'last_rating_date': False} persist=False and comments={'genres': '', 'rating_count': '', 'avg_rating': '', 'popularity_rank': '', 'quality_rank': '', 'first_rating_date': '', 'last_rating_date': '', '': ''} quoted={} persist=False
[0m10:31:02.417788 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:31:02.418247 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

  
    create or replace temporary view `genre_popularity__dbt_tmp` as
      -- =======================================================
-- Step 1: Base Ratings with Genres
-- Join silver ratings with silver movies to attach genres
-- Prepare rating timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        r.rating,
        r.rating_date as rating_timestamp,  
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Aggregate Genre Metrics
-- For each genre, calculate total ratings, avg rating,
-- and distribution insights
-- =======================================================
genre_stats as (
    select
        genres,  -- genres already formatted with spacing
        count(*) as rating_count,
        round(avg(rating), 2) as avg_rating,  
        to_date(min(rating_timestamp)) as first_rating_date,  
        to_date(max(rating_timestamp)) as last_rating_date   
    from ratings_base
    group by genres
),

-- =======================================================
-- Step 3: Ranking Genres
-- Rank genres by popularity (# of ratings)
-- and quality (avg rating)
-- =======================================================
ranked_genres as (
    select
        genres,
        rating_count,
        avg_rating,
        first_rating_date,
        last_rating_date,
        rank() over (order by rating_count desc) as popularity_rank,
        rank() over (order by avg_rating desc) as quality_rank
    from genre_stats
)

-- =======================================================
-- Step 4: Final Selection
-- One row per genre with all metrics
-- =======================================================
select
    genres,
    rating_count,
    avg_rating,
    popularity_rank,
    quality_rank,
    first_rating_date,
    last_rating_date
from ranked_genres


  -- Only insert new genres on incremental runs
  where genres not in (select genres from `workspace`.`movielens_volume`.`genre_popularity`)
  
[0m10:31:02.741675 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m10:31:02.742660 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-236a-19a9-ac2f-375a28f38ca5) - Closing
[0m10:31:02.769112 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:31:02.769898 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`movielens_volume`.`genre_popularity` AS JSON

  
[0m10:31:03.013752 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m10:31:03.016469 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-23a0-106b-9ded-a08d134f2aa2) - Closing
[0m10:31:03.021871 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:31:03.022592 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

    
  DESCRIBE TABLE EXTENDED `genre_popularity__dbt_tmp` AS JSON

  
[0m10:31:03.256372 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m10:31:03.258898 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-23cb-10bf-9987-43f6d9de27c7) - Closing
[0m10:31:03.270316 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.genre_popularity"
[0m10:31:03.271462 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:31:03.271929 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `workspace`.`movielens_volume`.`genre_popularity` as DBT_INTERNAL_DEST
    using
        `genre_popularity__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.genres <=> DBT_INTERNAL_DEST.genres
          
    when matched
        then update set
            *
    when not matched
        then insert
            *

[0m10:31:09.392597 [debug] [Thread-1 (]: SQL status: OK in 6.120 seconds
[0m10:31:09.393664 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-20a1-160a-b664-c416120db0a1, command-id=01f09ab3-23ec-1a7e-a462-8d3d2f9e9788) - Closing
[0m10:31:09.526647 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: Close
[0m10:31:09.527057 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-20a1-160a-b664-c416120db0a1) - Closing
[0m10:31:09.634694 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23C9D1190>]}
[0m10:31:09.635428 [info ] [Thread-1 (]: 5 of 9 OK created sql incremental model movielens_volume.genre_popularity ...... [[32mOK[0m in 12.20s]
[0m10:31:09.636389 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.genre_popularity
[0m10:31:09.636821 [debug] [Thread-1 (]: Began running node model.my_dbt_project.movie_enriched
[0m10:31:09.637325 [info ] [Thread-1 (]: 6 of 9 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m10:31:09.638019 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.movie_enriched) - Creating connection
[0m10:31:09.638412 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.movie_enriched'
[0m10:31:09.638735 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.movie_enriched
[0m10:31:09.642604 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m10:31:09.643681 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.movie_enriched
[0m10:31:09.645824 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m10:31:09.647174 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m10:31:09.648188 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m10:31:09.648848 [debug] [Thread-1 (]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
  using delta
      
      
      
      
      
      
      
      as
      -- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m10:31:09.649422 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:31:09.987790 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-27de-1f4e-b1cd-fca1157a4df3) - Created
[0m10:31:12.627633 [debug] [Thread-1 (]: SQL status: OK in 2.980 seconds
[0m10:31:12.628633 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-27de-1f4e-b1cd-fca1157a4df3, command-id=01f09ab3-27ed-1432-8f8a-690e779f6447) - Closing
[0m10:31:12.629339 [debug] [Thread-1 (]: Applying tags to relation None
[0m10:31:12.630392 [debug] [Thread-1 (]: On model.my_dbt_project.movie_enriched: Close
[0m10:31:12.630752 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-27de-1f4e-b1cd-fca1157a4df3) - Closing
[0m10:31:12.743310 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23C9D08F0>]}
[0m10:31:12.744232 [info ] [Thread-1 (]: 6 of 9 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 3.11s]
[0m10:31:12.745413 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.movie_enriched
[0m10:31:12.745933 [debug] [Thread-1 (]: Began running node model.my_dbt_project.user_activity
[0m10:31:12.746486 [info ] [Thread-1 (]: 7 of 9 START sql incremental model movielens_volume.user_activity .............. [RUN]
[0m10:31:12.747192 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.user_activity) - Creating connection
[0m10:31:12.747827 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.user_activity'
[0m10:31:12.748244 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.user_activity
[0m10:31:12.754217 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.user_activity"
[0m10:31:12.755361 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.user_activity
[0m10:31:12.757409 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m10:31:12.759975 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:12.760323 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'workspace' 
    AND schema_name = 'movielens_volume'
    AND table_name = 'user_activity'
  
[0m10:31:12.760679 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:31:13.086099 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de) - Created
[0m10:31:13.427667 [debug] [Thread-1 (]: SQL status: OK in 0.670 seconds
[0m10:31:13.429863 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-29c5-1a02-b261-6281a1482d5b) - Closing
[0m10:31:13.432468 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:13.432872 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'workspace'
    AND schema_name = 'movielens_volume'
    AND table_name = 'user_activity';
  
[0m10:31:13.733812 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m10:31:13.735635 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-29fa-1aed-94c9-1ca2d656ad4c) - Closing
[0m10:31:13.737811 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:13.738317 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT column_name
  FROM `workspace`.`information_schema`.`columns`
  WHERE table_catalog = 'workspace' 
    AND table_schema = 'movielens_volume'
    AND table_name = 'user_activity'
    AND is_nullable = 'NO';
  
[0m10:31:14.125393 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m10:31:14.127388 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-2a29-195e-babf-1c535a1c937a) - Closing
[0m10:31:14.129943 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:14.130344 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `workspace`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'workspace' 
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'user_activity' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'user_activity' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:31:14.765995 [debug] [Thread-1 (]: SQL status: OK in 0.640 seconds
[0m10:31:14.768621 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-2a64-1af4-a9f4-9ae70058e3da) - Closing
[0m10:31:14.771911 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:14.772658 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `workspace`.information_schema.key_column_usage kcu
  JOIN `workspace`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `workspace`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'workspace'
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'user_activity'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'user_activity'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:31:15.704938 [debug] [Thread-1 (]: SQL status: OK in 0.930 seconds
[0m10:31:15.706772 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-2ac7-143c-9c61-f32dedddb1d6) - Closing
[0m10:31:15.709514 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:15.710069 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'workspace'
    AND table_schema = 'movielens_volume'
    AND table_name = 'user_activity';
  
[0m10:31:16.119954 [debug] [Thread-1 (]: SQL status: OK in 0.410 seconds
[0m10:31:16.122206 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-2b56-119e-9cd9-9e24e217a649) - Closing
[0m10:31:16.125001 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:16.125463 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SHOW TBLPROPERTIES `workspace`.`movielens_volume`.`user_activity`
  
[0m10:31:16.376083 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m10:31:16.378172 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-2b95-1ca8-babc-613727dca3b0) - Closing
[0m10:31:16.380622 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:16.381037 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m10:31:16.625495 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m10:31:16.627538 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-2bbc-1a50-8eef-a0c4166c6309) - Closing
[0m10:31:16.628677 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={'user_id': 'Unique identifier for each user', 'user_rating_count': 'Total number of ratings submitted by the user', 'avg_user_rating': 'Average rating given by the user (0–5)', 'first_activity_date': 'Date of first rating by user (YYYY-MM-DD)', 'last_activity_date': 'Date of most recent rating by user (YYYY-MM-DD)', 'influence_score': 'User influence score based on total rating count', 'top_genre': "User's most rated genre"} quoted={'user_id': False, 'user_rating_count': False, 'avg_user_rating': False, 'first_activity_date': False, 'last_activity_date': False, 'influence_score': False, 'top_genre': False} persist=False and comments={'user_id': '', 'user_rating_count': '', 'avg_user_rating': '', 'first_activity_date': '', 'last_activity_date': '', 'influence_score': '', 'user_category': '', 'top_genre': '', '': ''} quoted={} persist=False
[0m10:31:16.629502 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:16.630041 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

  
    create or replace temporary view `user_activity__dbt_tmp` as
      -- =======================================================
-- Step 1: Base Ratings
-- Join ratings with movies to attach genres and prepare timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_timestamp,
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: User Activity (General)
-- Calculate total ratings, average rating, first and last activity dates per user
-- =======================================================
user_activity as (
    select
        user_id,
        count(*) as user_rating_count,
        round(avg(rating), 2) as avg_user_rating,
        min(rating_timestamp) as first_activity_date,
        max(rating_timestamp) as last_activity_date
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 3: User Influence Score
-- Compute influence based on total ratings (more ratings = higher influence)
-- =======================================================
user_influence as (
    select
        user_id,
        user_rating_count as influence_score
    from user_activity
),

-- =======================================================
-- Step 4: Active Users by Genre
-- Count ratings per genre and rank genres per user
-- =======================================================
user_genre_activity as (
    select
        user_id,
        regexp_replace(genres, '\\|', ' | ') as genres,
        count(*) as genre_rating_count,
        rank() over (partition by user_id order by count(*) desc) as genre_rank
    from ratings_base
    group by user_id, genres
),

-- =======================================================
-- Step 5: New vs Loyal Users
-- Categorize users as new if first activity < 30 days ago, else loyal
-- =======================================================
user_type as (
    select
        user_id,
        case
            when min(rating_timestamp) >= date_sub(current_date(), 30)
                then 'new_user'
            else 'loyal_user'
        end as user_category
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 6: Top Genre per User
-- Identify the most active genre for each user
-- =======================================================
top_genre as (
    select
        user_id,
        max(genres) as top_genre
    from user_genre_activity
    group by user_id
)

-- =======================================================
-- Final Selection
-- Combine general activity, influence, type, and top genre per user
-- =======================================================
select
    ua.user_id,
    ua.user_rating_count,
    ua.avg_user_rating,
    ua.first_activity_date,
    ua.last_activity_date,
    ui.influence_score,
    ut.user_category,
    tg.top_genre
from user_activity ua
join user_influence ui on ua.user_id = ui.user_id
join user_type ut on ua.user_id = ut.user_id
left join top_genre tg on ua.user_id = tg.user_id


  where ua.user_id not in (select user_id from `workspace`.`movielens_volume`.`user_activity`)
  
[0m10:31:16.993410 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m10:31:16.994382 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-2be2-1921-9457-2c3da27de293) - Closing
[0m10:31:16.996771 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:16.997463 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`movielens_volume`.`user_activity` AS JSON

  
[0m10:31:17.232969 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m10:31:17.235480 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-2c1a-1791-90cc-452e0df3c39e) - Closing
[0m10:31:17.238352 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:17.239102 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

    
  DESCRIBE TABLE EXTENDED `user_activity__dbt_tmp` AS JSON

  
[0m10:31:17.432423 [debug] [Thread-1 (]: SQL status: OK in 0.190 seconds
[0m10:31:17.434999 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-2c3f-1b7f-b46d-31319b73f981) - Closing
[0m10:31:17.436145 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.user_activity"
[0m10:31:17.437280 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:31:17.437786 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `workspace`.`movielens_volume`.`user_activity` as DBT_INTERNAL_DEST
    using
        `user_activity__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.user_id <=> DBT_INTERNAL_DEST.user_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *

[0m10:31:21.334100 [debug] [Thread-1 (]: SQL status: OK in 3.900 seconds
[0m10:31:21.335140 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de, command-id=01f09ab3-2c5d-1773-beac-dd46fb9c98b0) - Closing
[0m10:31:21.336490 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: Close
[0m10:31:21.336882 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-29b7-1829-ad07-3e4cd507d3de) - Closing
[0m10:31:21.445112 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23CA20C50>]}
[0m10:31:21.445848 [info ] [Thread-1 (]: 7 of 9 OK created sql incremental model movielens_volume.user_activity ......... [[32mOK[0m in 8.70s]
[0m10:31:21.446689 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.user_activity
[0m10:31:21.447157 [debug] [Thread-1 (]: Began running node model.my_dbt_project.user_preferences
[0m10:31:21.447672 [info ] [Thread-1 (]: 8 of 9 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m10:31:21.448397 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.user_preferences) - Creating connection
[0m10:31:21.448860 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.user_preferences'
[0m10:31:21.449334 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.user_preferences
[0m10:31:21.454657 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m10:31:21.455940 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.user_preferences
[0m10:31:21.458999 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m10:31:21.461995 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:21.462449 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'workspace' 
    AND schema_name = 'movielens_volume'
    AND table_name = 'user_preferences'
  
[0m10:31:21.463007 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:31:21.802317 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0) - Created
[0m10:31:22.129800 [debug] [Thread-1 (]: SQL status: OK in 0.670 seconds
[0m10:31:22.131429 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-2ef7-1f01-b652-39ebe7a31def) - Closing
[0m10:31:22.133331 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:22.133667 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'workspace'
    AND schema_name = 'movielens_volume'
    AND table_name = 'user_preferences';
  
[0m10:31:22.464625 [debug] [Thread-1 (]: SQL status: OK in 0.330 seconds
[0m10:31:22.467244 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-2f2b-1143-b320-885fcc974a0d) - Closing
[0m10:31:22.470196 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:22.470699 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT column_name
  FROM `workspace`.`information_schema`.`columns`
  WHERE table_catalog = 'workspace' 
    AND table_schema = 'movielens_volume'
    AND table_name = 'user_preferences'
    AND is_nullable = 'NO';
  
[0m10:31:22.892915 [debug] [Thread-1 (]: SQL status: OK in 0.420 seconds
[0m10:31:22.894727 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-2f61-1ebb-8500-171dad1ea2af) - Closing
[0m10:31:22.896850 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:22.897243 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `workspace`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'workspace' 
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'user_preferences' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'user_preferences' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:31:23.569459 [debug] [Thread-1 (]: SQL status: OK in 0.670 seconds
[0m10:31:23.571119 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-2f9e-13f8-8156-f6f0f168d677) - Closing
[0m10:31:23.578245 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:23.578985 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `workspace`.information_schema.key_column_usage kcu
  JOIN `workspace`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `workspace`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'workspace'
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'user_preferences'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'user_preferences'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:31:24.499977 [debug] [Thread-1 (]: SQL status: OK in 0.920 seconds
[0m10:31:24.502688 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-3006-1f5d-b8dc-e4e22de26ab2) - Closing
[0m10:31:24.504928 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:24.505298 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'workspace'
    AND table_schema = 'movielens_volume'
    AND table_name = 'user_preferences';
  
[0m10:31:24.810963 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m10:31:24.813506 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-3093-1ee1-8284-633b64ee007e) - Closing
[0m10:31:24.816758 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:24.817421 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SHOW TBLPROPERTIES `workspace`.`movielens_volume`.`user_preferences`
  
[0m10:31:25.101623 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m10:31:25.103454 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-30ca-1c4f-ac9e-124efd8d4689) - Closing
[0m10:31:25.105460 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:25.105830 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m10:31:25.339618 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m10:31:25.341869 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-30ef-16b5-aaad-84f05980baa3) - Closing
[0m10:31:25.343077 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'user_id': '', 'user_rating_count': '', 'user_avg_rating': '', 'first_activity_date': '', 'last_activity_date': '', 'influence_score': '', 'top_genre': '', '': ''} quoted={} persist=False
[0m10:31:25.343936 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:25.344519 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create or replace temporary view `user_preferences__dbt_tmp` as
      -- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score,  -- influence based on total ratings
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )
  
[0m10:31:25.688337 [debug] [Thread-1 (]: SQL status: OK in 0.340 seconds
[0m10:31:25.689375 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-3114-1978-9396-65d7a49b5c02) - Closing
[0m10:31:25.692358 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:25.693210 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`movielens_volume`.`user_preferences` AS JSON

  
[0m10:31:25.956758 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m10:31:25.959424 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-3149-142a-8669-9832b2ab04a7) - Closing
[0m10:31:25.962086 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:25.962903 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

    
  DESCRIBE TABLE EXTENDED `user_preferences__dbt_tmp` AS JSON

  
[0m10:31:26.160748 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m10:31:26.163562 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-3173-1bf6-95e4-2b4b9f295635) - Closing
[0m10:31:26.164720 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m10:31:26.165706 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:31:26.166100 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
    using
        `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.user_id <=> DBT_INTERNAL_DEST.user_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *

[0m10:31:32.474995 [debug] [Thread-1 (]: SQL status: OK in 6.310 seconds
[0m10:31:32.476001 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0, command-id=01f09ab3-3191-1368-a67e-188c48bbec87) - Closing
[0m10:31:32.602873 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: Close
[0m10:31:32.603333 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-2ee9-1840-ae30-0557f3cf5dd0) - Closing
[0m10:31:32.708257 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23DB9AF30>]}
[0m10:31:32.709120 [info ] [Thread-1 (]: 8 of 9 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 11.26s]
[0m10:31:32.709972 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.user_preferences
[0m10:31:32.710592 [debug] [Thread-1 (]: Began running node model.my_dbt_project.movie_performance
[0m10:31:32.711275 [info ] [Thread-1 (]: 9 of 9 START sql incremental model movielens_volume.movie_performance .......... [RUN]
[0m10:31:32.712134 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.movie_performance) - Creating connection
[0m10:31:32.712556 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.movie_performance'
[0m10:31:32.712883 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.movie_performance
[0m10:31:32.718317 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.movie_performance"
[0m10:31:32.719411 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.movie_performance
[0m10:31:32.721963 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m10:31:32.724888 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:32.725293 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'workspace' 
    AND schema_name = 'movielens_volume'
    AND table_name = 'movie_performance'
  
[0m10:31:32.725606 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:31:33.051009 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873) - Created
[0m10:31:33.368461 [debug] [Thread-1 (]: SQL status: OK in 0.640 seconds
[0m10:31:33.370842 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-35ab-1e6b-84ed-0e8049c1bd5b) - Closing
[0m10:31:33.372865 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:33.373204 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'workspace'
    AND schema_name = 'movielens_volume'
    AND table_name = 'movie_performance';
  
[0m10:31:33.662236 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m10:31:33.664130 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-35dd-1727-bdf9-dc01c6893494) - Closing
[0m10:31:33.666476 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:33.666903 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT column_name
  FROM `workspace`.`information_schema`.`columns`
  WHERE table_catalog = 'workspace' 
    AND table_schema = 'movielens_volume'
    AND table_name = 'movie_performance'
    AND is_nullable = 'NO';
  
[0m10:31:34.047445 [debug] [Thread-1 (]: SQL status: OK in 0.380 seconds
[0m10:31:34.049244 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-360a-10f7-8f2e-4f3eb6cc3482) - Closing
[0m10:31:34.051625 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:34.052129 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `workspace`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'workspace' 
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'movie_performance' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'movie_performance' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:31:34.714701 [debug] [Thread-1 (]: SQL status: OK in 0.660 seconds
[0m10:31:34.717284 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-3644-16c3-873c-7d34e2da7d9d) - Closing
[0m10:31:34.720980 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:34.721402 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `workspace`.information_schema.key_column_usage kcu
  JOIN `workspace`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `workspace`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'workspace'
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'movie_performance'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'movie_performance'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:31:35.611545 [debug] [Thread-1 (]: SQL status: OK in 0.890 seconds
[0m10:31:35.613831 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-36aa-16b1-8c98-c4852fa45ff9) - Closing
[0m10:31:35.615984 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:35.616393 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'workspace'
    AND table_schema = 'movielens_volume'
    AND table_name = 'movie_performance';
  
[0m10:31:35.962626 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m10:31:35.964533 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-3733-1602-99b1-da2d953225fd) - Closing
[0m10:31:35.966961 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:35.967571 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SHOW TBLPROPERTIES `workspace`.`movielens_volume`.`movie_performance`
  
[0m10:31:36.211378 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m10:31:36.213375 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-3769-1043-9d18-698dcafa2984) - Closing
[0m10:31:36.215444 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:36.215808 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m10:31:36.477850 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m10:31:36.480367 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-378e-1fc1-b617-fe89eab07adc) - Closing
[0m10:31:36.481455 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={'movie_id': 'Primary identifier for the movie', 'weighted_avg_rating': 'Weighted average rating for the movie (0–5)', 'rating_count': 'Number of ratings used in aggregation', 'genre_rank': 'Rank within genre ordered by weighted_avg_rating', 'polarization_score': 'Standard deviation of ratings (rounded 2 decimals)', 'top_rating_percentage': 'Percentage of ratings >= 4.5 (rounded 2 decimals)'} quoted={'movie_id': False, 'weighted_avg_rating': False, 'rating_count': False, 'genre_rank': False, 'polarization_score': False, 'top_rating_percentage': False} persist=False and comments={'movie_id': '', 'title': '', 'genres': '', 'weighted_avg_rating': '', 'rating_count': '', 'genre_rank': '', 'polarization_score': '', 'top_rating_percentage': '', 'tag_count': '', '': ''} quoted={} persist=False
[0m10:31:36.482169 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:36.482708 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

  
    create or replace temporary view `movie_performance__dbt_tmp` as
      -- =======================================================
-- Step 1: Base Ratings Join
-- Attach movie metadata (title, genres) to ratings
-- =======================================================
with ratings_base as (
    select
        r.movie_id,
        m.title,
        regexp_replace(m.genres, '\\|', ' | ') as genres,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_date
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Weighted Rating
-- Compute avg rating, rating count, weighted avg
-- =======================================================
weighted_rating as (
    select
        movie_id,
        title,
        genres,
        round(avg(rating), 2) as avg_rating,
        count(*) as rating_count,
        round(sum(rating) / count(*), 2) as weighted_avg_rating
    from ratings_base
    group by movie_id, title, genres
),

-- =======================================================
-- Step 3: Polarization Score
-- Standard deviation of ratings per movie
-- =======================================================
polarization as (
    select
        movie_id,
        round(coalesce(stddev(rating), 0.0), 2) as polarization_score
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 4: Top Rating Percentage
-- Percent of ratings >= 4.5
-- =======================================================
top_rating_pct as (
    select
        movie_id,
        round(100.0 * sum(case when rating >= 4.5 then 1 else 0 end) / count(*), 2) as top_rating_percentage
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 5: Genre Ranking
-- Rank movies within each genre by weighted_avg_rating
-- =======================================================
genre_rank as (
    select
        movie_id,
        genres,
        weighted_avg_rating,
        rank() over (partition by genres order by weighted_avg_rating desc) as genre_rank
    from weighted_rating
),

-- =======================================================
-- Step 6: Tag Count
-- Count distinct tags per movie
-- =======================================================
tag_count as (
    select
        movie_id,
        count(distinct tag) as tag_count
    from `workspace`.`movielens_volume`.`silver_tags`
    group by movie_id
)

-- =======================================================
-- Step 7: Final Selection
-- Combine all metrics into the gold table
-- =======================================================
select
    w.movie_id,
    w.title,
    w.genres,
    w.weighted_avg_rating,
    w.rating_count,
    g.genre_rank,
    p.polarization_score,
    tr.top_rating_percentage,
    coalesce(tc.tag_count, 0) as tag_count
from weighted_rating w
join genre_rank g
    on w.movie_id = g.movie_id
join polarization p
    on w.movie_id = p.movie_id
join top_rating_pct tr
    on w.movie_id = tr.movie_id
left join tag_count tc
    on w.movie_id = tc.movie_id


-- Use NOT EXISTS for safer incremental inserts
where not exists (
    select 1 
    from `workspace`.`movielens_volume`.`movie_performance` t
    where t.movie_id = w.movie_id
)
  
[0m10:31:36.892159 [debug] [Thread-1 (]: SQL status: OK in 0.410 seconds
[0m10:31:36.893165 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-37b7-1ed7-aaad-7392beb5ee8c) - Closing
[0m10:31:36.895565 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:36.896298 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`movielens_volume`.`movie_performance` AS JSON

  
[0m10:31:37.140247 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m10:31:37.142840 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-37f7-1322-95ca-4dd8d590fad8) - Closing
[0m10:31:37.145608 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:37.146324 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

    
  DESCRIBE TABLE EXTENDED `movie_performance__dbt_tmp` AS JSON

  
[0m10:31:37.336163 [debug] [Thread-1 (]: SQL status: OK in 0.190 seconds
[0m10:31:37.338852 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-381c-1dfd-986f-5bdd039b1ce3) - Closing
[0m10:31:37.339973 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.movie_performance"
[0m10:31:37.341002 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:31:37.341404 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `workspace`.`movielens_volume`.`movie_performance` as DBT_INTERNAL_DEST
    using
        `movie_performance__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.movie_id <=> DBT_INTERNAL_DEST.movie_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *

[0m10:31:42.124299 [debug] [Thread-1 (]: SQL status: OK in 4.780 seconds
[0m10:31:42.125281 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873, command-id=01f09ab3-383a-1ba2-8307-fc0011887221) - Closing
[0m10:31:42.126814 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: Close
[0m10:31:42.127238 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-359e-130c-92b6-3b1cdc327873) - Closing
[0m10:31:42.247425 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1aa7928c-a985-49f4-ab71-15e6b0494d6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23C81AB70>]}
[0m10:31:42.248099 [info ] [Thread-1 (]: 9 of 9 OK created sql incremental model movielens_volume.movie_performance ..... [[32mOK[0m in 9.54s]
[0m10:31:42.248924 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.movie_performance
[0m10:31:42.250423 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m10:31:42.250775 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:31:42.251241 [info ] [MainThread]: 
[0m10:31:42.251682 [info ] [MainThread]: Finished running 4 incremental models, 5 table models in 0 hours 1 minutes and 0.94 seconds (60.94s).
[0m10:31:42.253514 [debug] [MainThread]: Command end result
[0m10:31:42.302490 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\38349\projects\end_to_end_de_project\dbt\target\manifest.json
[0m10:31:42.305363 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\38349\projects\end_to_end_de_project\dbt\target\semantic_manifest.json
[0m10:31:42.311413 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\38349\projects\end_to_end_de_project\dbt\target\run_results.json
[0m10:31:42.311750 [info ] [MainThread]: 
[0m10:31:42.312263 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:31:42.312777 [info ] [MainThread]: 
[0m10:31:42.313292 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=9
[0m10:31:42.313866 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 10 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m10:31:42.314828 [debug] [MainThread]: Command `dbt run` succeeded at 10:31:42.314719 after 68.53 seconds
[0m10:31:42.315167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23CAE30B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B220AD6930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B23C3C5C10>]}
[0m10:31:42.315491 [debug] [MainThread]: Flushing usage events
[0m10:31:43.104675 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:35:25.593937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC4E4FB60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC2F08550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC6220E10>]}


============================== 10:35:25.598934 | 304dac56-587d-4e3d-9eee-43caecb5339c ==============================
[0m10:35:25.598934 [info ] [MainThread]: Running with dbt=1.10.9
[0m10:35:25.599519 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'warn_error': 'None', 'invocation_command': 'dbt run', 'log_format': 'default', 'use_colors': 'True', 'introspect': 'True', 'debug': 'False', 'printer_width': '80', 'fail_fast': 'False', 'no_print': 'None', 'use_experimental_parser': 'False', 'quiet': 'False', 'log_cache_events': 'False', 'empty': 'False', 'indirect_selection': 'eager', 'log_path': 'C:\\Users\\38349\\projects\\end_to_end_de_project\\dbt\\logs', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': 'C:\\Users\\38349\\projects\\end_to_end_de_project\\dbt'}
[0m10:35:26.554160 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:35:26.554640 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:35:26.554940 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:35:27.408274 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC55A1810>]}
[0m10:35:27.468370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC8955480>]}
[0m10:35:27.469065 [info ] [MainThread]: Registered adapter: databricks=1.10.11
[0m10:35:28.073437 [debug] [MainThread]: checksum: 6543b8b248ceda473ef0d611849d5d909085b6b714afa9b515e9635faea7af23, vars: {}, profile: , target: , version: 1.10.9
[0m10:35:28.297789 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:35:28.298155 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:35:28.343971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1792250>]}
[0m10:35:28.440222 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\38349\projects\end_to_end_de_project\dbt\target\manifest.json
[0m10:35:28.442932 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\38349\projects\end_to_end_de_project\dbt\target\semantic_manifest.json
[0m10:35:28.461522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE17EE120>]}
[0m10:35:28.462020 [info ] [MainThread]: Found 9 models, 55 data tests, 4 sources, 803 macros
[0m10:35:28.462481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1CF49F0>]}
[0m10:35:28.464921 [info ] [MainThread]: 
[0m10:35:28.465385 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:35:28.465915 [info ] [MainThread]: 
[0m10:35:28.466667 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m10:35:28.466950 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:35:28.473582 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m10:35:28.474016 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m10:35:28.483725 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m10:35:28.484098 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "list_workspace"} */

    show databases
  
[0m10:35:28.484348 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:35:28.908574 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f09ab3-c22f-10f9-a33c-357cd1f2d26d) - Created
[0m10:35:29.207256 [debug] [ThreadPool]: SQL status: OK in 0.720 seconds
[0m10:35:29.213558 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f09ab3-c22f-10f9-a33c-357cd1f2d26d, command-id=01f09ab3-c240-11e1-8903-294d25b4acfa) - Closing
[0m10:35:29.214146 [debug] [ThreadPool]: On list_workspace: Close
[0m10:35:29.214459 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f09ab3-c22f-10f9-a33c-357cd1f2d26d) - Closing
[0m10:35:29.338025 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_movielens_volume) - Creating connection
[0m10:35:29.338480 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m10:35:29.345357 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m10:35:29.345726 [debug] [ThreadPool]: On list_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "list_workspace_movielens_volume"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'movielens_volume'

  
[0m10:35:29.346020 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:35:29.682975 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f09ab3-c2a7-1327-821f-8db876946296) - Created
[0m10:35:30.121870 [debug] [ThreadPool]: SQL status: OK in 0.780 seconds
[0m10:35:30.124624 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f09ab3-c2a7-1327-821f-8db876946296, command-id=01f09ab3-c2b6-144b-b118-d90ff72d2045) - Closing
[0m10:35:30.125978 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m10:35:30.126307 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f09ab3-c2a7-1327-821f-8db876946296) - Closing
[0m10:35:30.271538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1ECE000>]}
[0m10:35:30.276471 [debug] [Thread-1 (]: Began running node model.my_dbt_project.silver_links
[0m10:35:30.277085 [info ] [Thread-1 (]: 1 of 9 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m10:35:30.277931 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.silver_links) - Creating connection
[0m10:35:30.278390 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.silver_links'
[0m10:35:30.278787 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.silver_links
[0m10:35:30.286856 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m10:35:30.287850 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.silver_links
[0m10:35:30.303646 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m10:35:30.304219 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m10:35:30.304846 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1EB8F50>]}
[0m10:35:30.345201 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m10:35:30.346275 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.silver_links"
[0m10:35:30.346672 [debug] [Thread-1 (]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
  using delta
      
      
      
      
      
      
      
      as
      with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m10:35:30.347033 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:35:30.698071 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-c342-1658-92d5-ad2784e68563) - Created
[0m10:35:32.971679 [debug] [Thread-1 (]: SQL status: OK in 2.620 seconds
[0m10:35:32.972679 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-c342-1658-92d5-ad2784e68563, command-id=01f09ab3-c351-13e9-8696-48b22c798433) - Closing
[0m10:35:32.986530 [debug] [Thread-1 (]: Applying tags to relation None
[0m10:35:33.001033 [debug] [Thread-1 (]: On model.my_dbt_project.silver_links: Close
[0m10:35:33.001367 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-c342-1658-92d5-ad2784e68563) - Closing
[0m10:35:33.110657 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1EDCF70>]}
[0m10:35:33.111409 [info ] [Thread-1 (]: 1 of 9 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 2.83s]
[0m10:35:33.112162 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.silver_links
[0m10:35:33.112592 [debug] [Thread-1 (]: Began running node model.my_dbt_project.silver_movies
[0m10:35:33.113144 [info ] [Thread-1 (]: 2 of 9 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m10:35:33.113771 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.silver_movies) - Creating connection
[0m10:35:33.114190 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.silver_movies'
[0m10:35:33.114638 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.silver_movies
[0m10:35:33.117949 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m10:35:33.119017 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.silver_movies
[0m10:35:33.122843 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m10:35:33.124160 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m10:35:33.124939 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m10:35:33.125367 [debug] [Thread-1 (]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m10:35:33.125678 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:35:33.453805 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-c4e6-172a-b514-937cf42e4b28) - Created
[0m10:35:35.266431 [debug] [Thread-1 (]: SQL status: OK in 2.140 seconds
[0m10:35:35.267492 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-c4e6-172a-b514-937cf42e4b28, command-id=01f09ab3-c4f5-1642-8b43-9f6589621493) - Closing
[0m10:35:35.268208 [debug] [Thread-1 (]: Applying tags to relation None
[0m10:35:35.269247 [debug] [Thread-1 (]: On model.my_dbt_project.silver_movies: Close
[0m10:35:35.269603 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-c4e6-172a-b514-937cf42e4b28) - Closing
[0m10:35:35.378029 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1F36C10>]}
[0m10:35:35.378733 [info ] [Thread-1 (]: 2 of 9 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 2.26s]
[0m10:35:35.379451 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.silver_movies
[0m10:35:35.379957 [debug] [Thread-1 (]: Began running node model.my_dbt_project.silver_ratings
[0m10:35:35.380575 [info ] [Thread-1 (]: 3 of 9 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m10:35:35.381207 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.silver_ratings) - Creating connection
[0m10:35:35.381581 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.silver_ratings'
[0m10:35:35.381906 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.silver_ratings
[0m10:35:35.386728 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m10:35:35.387884 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.silver_ratings
[0m10:35:35.389987 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m10:35:35.391401 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m10:35:35.392374 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m10:35:35.392856 [debug] [Thread-1 (]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m10:35:35.393185 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:35:35.725146 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-c642-1508-bbba-8e63f9d8cced) - Created
[0m10:35:39.112133 [debug] [Thread-1 (]: SQL status: OK in 3.720 seconds
[0m10:35:39.113141 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-c642-1508-bbba-8e63f9d8cced, command-id=01f09ab3-c650-10d6-a9f3-15c5b2ea226a) - Closing
[0m10:35:39.113857 [debug] [Thread-1 (]: Applying tags to relation None
[0m10:35:39.115058 [debug] [Thread-1 (]: On model.my_dbt_project.silver_ratings: Close
[0m10:35:39.115440 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-c642-1508-bbba-8e63f9d8cced) - Closing
[0m10:35:39.231190 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC4E2D850>]}
[0m10:35:39.231923 [info ] [Thread-1 (]: 3 of 9 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 3.85s]
[0m10:35:39.232648 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.silver_ratings
[0m10:35:39.233038 [debug] [Thread-1 (]: Began running node model.my_dbt_project.silver_tags
[0m10:35:39.233570 [info ] [Thread-1 (]: 4 of 9 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m10:35:39.234283 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.silver_tags) - Creating connection
[0m10:35:39.234633 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.silver_tags'
[0m10:35:39.234946 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.silver_tags
[0m10:35:39.238152 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m10:35:39.239244 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.silver_tags
[0m10:35:39.241759 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m10:35:39.243174 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m10:35:39.244099 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m10:35:39.244602 [debug] [Thread-1 (]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
  using delta
      
      
      
      
      
      
      
      as
      select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m10:35:39.245035 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:35:39.563257 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-c88c-1107-a086-081690478985) - Created
[0m10:35:42.188953 [debug] [Thread-1 (]: SQL status: OK in 2.940 seconds
[0m10:35:42.190507 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-c88c-1107-a086-081690478985, command-id=01f09ab3-c89a-132f-aa3e-b83dccd44e81) - Closing
[0m10:35:42.191475 [debug] [Thread-1 (]: Applying tags to relation None
[0m10:35:42.192920 [debug] [Thread-1 (]: On model.my_dbt_project.silver_tags: Close
[0m10:35:42.193348 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-c88c-1107-a086-081690478985) - Closing
[0m10:35:42.303216 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1BB2510>]}
[0m10:35:42.304131 [info ] [Thread-1 (]: 4 of 9 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 3.07s]
[0m10:35:42.305025 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.silver_tags
[0m10:35:42.305435 [debug] [Thread-1 (]: Began running node model.my_dbt_project.genre_popularity
[0m10:35:42.305966 [info ] [Thread-1 (]: 5 of 9 START sql incremental model movielens_volume.genre_popularity ........... [RUN]
[0m10:35:42.306685 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.genre_popularity) - Creating connection
[0m10:35:42.307035 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.genre_popularity'
[0m10:35:42.307362 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.genre_popularity
[0m10:35:42.318169 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.genre_popularity"
[0m10:35:42.319260 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.genre_popularity
[0m10:35:42.351724 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m10:35:42.372508 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:42.372994 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'workspace' 
    AND schema_name = 'movielens_volume'
    AND table_name = 'genre_popularity'
  
[0m10:35:42.373376 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:35:42.729066 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a) - Created
[0m10:35:43.046952 [debug] [Thread-1 (]: SQL status: OK in 0.670 seconds
[0m10:35:43.048746 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-ca7d-1815-ba68-f41bff34632f) - Closing
[0m10:35:43.054721 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:43.055120 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'workspace'
    AND schema_name = 'movielens_volume'
    AND table_name = 'genre_popularity';
  
[0m10:35:43.391093 [debug] [Thread-1 (]: SQL status: OK in 0.340 seconds
[0m10:35:43.392911 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-caaf-1955-b974-c882e15c6e12) - Closing
[0m10:35:43.399112 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:43.399521 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT column_name
  FROM `workspace`.`information_schema`.`columns`
  WHERE table_catalog = 'workspace' 
    AND table_schema = 'movielens_volume'
    AND table_name = 'genre_popularity'
    AND is_nullable = 'NO';
  
[0m10:35:43.789017 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m10:35:43.790747 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-cae4-1559-aa4b-da61ff36187b) - Closing
[0m10:35:43.797018 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:43.797461 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `workspace`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'workspace' 
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'genre_popularity' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'genre_popularity' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:35:44.476333 [debug] [Thread-1 (]: SQL status: OK in 0.680 seconds
[0m10:35:44.478330 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-cb20-148a-bba9-1746140ab0a4) - Closing
[0m10:35:44.485305 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:44.485733 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `workspace`.information_schema.key_column_usage kcu
  JOIN `workspace`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `workspace`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'workspace'
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'genre_popularity'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'genre_popularity'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:35:45.378885 [debug] [Thread-1 (]: SQL status: OK in 0.890 seconds
[0m10:35:45.380924 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-cb89-1a41-a172-06c90cc7cf66) - Closing
[0m10:35:45.386429 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:45.386829 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'workspace'
    AND table_schema = 'movielens_volume'
    AND table_name = 'genre_popularity';
  
[0m10:35:45.680720 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m10:35:45.682531 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-cc16-17ab-8016-c32cf3cdd6bd) - Closing
[0m10:35:45.686576 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:45.686956 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
SHOW TBLPROPERTIES `workspace`.`movielens_volume`.`genre_popularity`
  
[0m10:35:46.437868 [debug] [Thread-1 (]: SQL status: OK in 0.750 seconds
[0m10:35:46.439742 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-cc83-1008-8e3e-b849587659ff) - Closing
[0m10:35:46.443726 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:46.444113 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m10:35:46.763985 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m10:35:46.765998 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-ccb5-1db1-ba33-40e9d54343d1) - Closing
[0m10:35:46.767137 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={'genres': 'Pipe-separated movie genres', 'rating_count': 'Total number of ratings for this genre', 'avg_rating': 'Average rating for this genre (0–5)', 'popularity_rank': 'Rank of genre by number of ratings', 'quality_rank': 'Rank of genre by average rating', 'first_rating_date': 'Earliest rating date for this genre', 'last_rating_date': 'Most recent rating date for this genre'} quoted={'genres': False, 'rating_count': False, 'avg_rating': False, 'popularity_rank': False, 'quality_rank': False, 'first_rating_date': False, 'last_rating_date': False} persist=False and comments={'genres': '', 'rating_count': '', 'avg_rating': '', 'popularity_rank': '', 'quality_rank': '', 'first_rating_date': '', 'last_rating_date': '', '': ''} quoted={} persist=False
[0m10:35:46.770123 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:46.770605 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

  
    create or replace temporary view `genre_popularity__dbt_tmp` as
      -- =======================================================
-- Step 1: Base Ratings with Genres
-- Join silver ratings with silver movies to attach genres
-- Prepare rating timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        r.rating,
        r.rating_date as rating_timestamp,  
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Aggregate Genre Metrics
-- For each genre, calculate total ratings, avg rating,
-- and distribution insights
-- =======================================================
genre_stats as (
    select
        genres,  -- genres already formatted with spacing
        count(*) as rating_count,
        round(avg(rating), 2) as avg_rating,  
        to_date(min(rating_timestamp)) as first_rating_date,  
        to_date(max(rating_timestamp)) as last_rating_date   
    from ratings_base
    group by genres
),

-- =======================================================
-- Step 3: Ranking Genres
-- Rank genres by popularity (# of ratings)
-- and quality (avg rating)
-- =======================================================
ranked_genres as (
    select
        genres,
        rating_count,
        avg_rating,
        first_rating_date,
        last_rating_date,
        rank() over (order by rating_count desc) as popularity_rank,
        rank() over (order by avg_rating desc) as quality_rank
    from genre_stats
)

-- =======================================================
-- Step 4: Final Selection
-- One row per genre with all metrics
-- =======================================================
select
    genres,
    rating_count,
    avg_rating,
    popularity_rank,
    quality_rank,
    first_rating_date,
    last_rating_date
from ranked_genres


  -- Only insert new genres on incremental runs
  where genres not in (select genres from `workspace`.`movielens_volume`.`genre_popularity`)
  
[0m10:35:47.106141 [debug] [Thread-1 (]: SQL status: OK in 0.340 seconds
[0m10:35:47.107115 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-cceb-1676-a3b6-aa1103ff7e56) - Closing
[0m10:35:47.132478 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:47.133134 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`movielens_volume`.`genre_popularity` AS JSON

  
[0m10:35:47.381043 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m10:35:47.383563 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-cd21-131a-bab6-6eedbc5209cf) - Closing
[0m10:35:47.386054 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:47.386742 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

    
  DESCRIBE TABLE EXTENDED `genre_popularity__dbt_tmp` AS JSON

  
[0m10:35:47.589635 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m10:35:47.592192 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-cd48-1b64-8739-118125c3b264) - Closing
[0m10:35:47.603460 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.genre_popularity"
[0m10:35:47.604483 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:35:47.604951 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `workspace`.`movielens_volume`.`genre_popularity` as DBT_INTERNAL_DEST
    using
        `genre_popularity__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.genres <=> DBT_INTERNAL_DEST.genres
          
    when matched
        then update set
            *
    when not matched
        then insert
            *

[0m10:35:53.541649 [debug] [Thread-1 (]: SQL status: OK in 5.940 seconds
[0m10:35:53.542651 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a, command-id=01f09ab3-cd6e-16a3-a6c9-2f224b71da2d) - Closing
[0m10:35:53.676474 [debug] [Thread-1 (]: On model.my_dbt_project.genre_popularity: Close
[0m10:35:53.676890 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-ca6e-10d2-a584-048ab045ef1a) - Closing
[0m10:35:53.792212 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE188A8D0>]}
[0m10:35:53.792983 [info ] [Thread-1 (]: 5 of 9 OK created sql incremental model movielens_volume.genre_popularity ...... [[32mOK[0m in 11.49s]
[0m10:35:53.793707 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.genre_popularity
[0m10:35:53.794092 [debug] [Thread-1 (]: Began running node model.my_dbt_project.movie_enriched
[0m10:35:53.794599 [info ] [Thread-1 (]: 6 of 9 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m10:35:53.795207 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.movie_enriched) - Creating connection
[0m10:35:53.795546 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.movie_enriched'
[0m10:35:53.795857 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.movie_enriched
[0m10:35:53.799305 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m10:35:53.800467 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.movie_enriched
[0m10:35:53.802940 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m10:35:53.804335 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m10:35:53.805381 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m10:35:53.805916 [debug] [Thread-1 (]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
  using delta
      
      
      
      
      
      
      
      as
      -- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m10:35:53.806297 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:35:54.141496 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-d13c-1a6c-8d74-415ac06f37b8) - Created
[0m10:35:56.540751 [debug] [Thread-1 (]: SQL status: OK in 2.730 seconds
[0m10:35:56.541749 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d13c-1a6c-8d74-415ac06f37b8, command-id=01f09ab3-d14c-1294-b321-e6583845d9d5) - Closing
[0m10:35:56.542467 [debug] [Thread-1 (]: Applying tags to relation None
[0m10:35:56.543512 [debug] [Thread-1 (]: On model.my_dbt_project.movie_enriched: Close
[0m10:35:56.543857 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-d13c-1a6c-8d74-415ac06f37b8) - Closing
[0m10:35:56.658410 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1CA9970>]}
[0m10:35:56.659136 [info ] [Thread-1 (]: 6 of 9 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 2.86s]
[0m10:35:56.659924 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.movie_enriched
[0m10:35:56.660320 [debug] [Thread-1 (]: Began running node model.my_dbt_project.user_activity
[0m10:35:56.660844 [info ] [Thread-1 (]: 7 of 9 START sql incremental model movielens_volume.user_activity .............. [RUN]
[0m10:35:56.661468 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.user_activity) - Creating connection
[0m10:35:56.661844 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.user_activity'
[0m10:35:56.662177 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.user_activity
[0m10:35:56.668036 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.user_activity"
[0m10:35:56.669153 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.user_activity
[0m10:35:56.671299 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m10:35:56.677517 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:35:56.677961 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'workspace' 
    AND schema_name = 'movielens_volume'
    AND table_name = 'user_activity'
  
[0m10:35:56.678285 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:35:56.999313 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976) - Created
[0m10:35:57.291709 [debug] [Thread-1 (]: SQL status: OK in 0.610 seconds
[0m10:35:57.293645 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d2fe-13f3-b373-509c32a604cb) - Closing
[0m10:35:57.296059 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:35:57.296504 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'workspace'
    AND schema_name = 'movielens_volume'
    AND table_name = 'user_activity';
  
[0m10:35:57.599570 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m10:35:57.601381 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d32c-15bc-aaa3-09fe45cc0247) - Closing
[0m10:35:57.603725 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:35:57.604120 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT column_name
  FROM `workspace`.`information_schema`.`columns`
  WHERE table_catalog = 'workspace' 
    AND table_schema = 'movielens_volume'
    AND table_name = 'user_activity'
    AND is_nullable = 'NO';
  
[0m10:35:58.038049 [debug] [Thread-1 (]: SQL status: OK in 0.430 seconds
[0m10:35:58.039851 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d363-13e9-90d3-3a53c435b1c7) - Closing
[0m10:35:58.041860 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:35:58.042247 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `workspace`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'workspace' 
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'user_activity' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'user_activity' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:35:58.614945 [debug] [Thread-1 (]: SQL status: OK in 0.570 seconds
[0m10:35:58.616893 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d39d-1ba4-8997-2ab5e5bbd84f) - Closing
[0m10:35:58.619188 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:35:58.619613 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `workspace`.information_schema.key_column_usage kcu
  JOIN `workspace`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `workspace`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'workspace'
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'user_activity'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'user_activity'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:35:59.502444 [debug] [Thread-1 (]: SQL status: OK in 0.880 seconds
[0m10:35:59.504809 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d3f5-1bf9-a6ec-1d1d79380d46) - Closing
[0m10:35:59.506887 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:35:59.507275 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'workspace'
    AND table_schema = 'movielens_volume'
    AND table_name = 'user_activity';
  
[0m10:35:59.798365 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m10:35:59.800109 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d47d-1099-a3dc-74c43f29afad) - Closing
[0m10:35:59.802417 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:35:59.802854 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
SHOW TBLPROPERTIES `workspace`.`movielens_volume`.`user_activity`
  
[0m10:36:00.061728 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m10:36:00.063647 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d4aa-17d6-96cf-e53f546a6955) - Closing
[0m10:36:00.065468 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:36:00.065795 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m10:36:00.311358 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m10:36:00.313781 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d4d1-1a9d-a94c-1d46f1a6b7db) - Closing
[0m10:36:00.314842 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={'user_id': 'Unique identifier for each user', 'user_rating_count': 'Total number of ratings submitted by the user', 'avg_user_rating': 'Average rating given by the user (0–5)', 'first_activity_date': 'Date of first rating by user (YYYY-MM-DD)', 'last_activity_date': 'Date of most recent rating by user (YYYY-MM-DD)', 'influence_score': 'User influence score based on total rating count', 'top_genre': "User's most rated genre"} quoted={'user_id': False, 'user_rating_count': False, 'avg_user_rating': False, 'first_activity_date': False, 'last_activity_date': False, 'influence_score': False, 'top_genre': False} persist=False and comments={'user_id': '', 'user_rating_count': '', 'avg_user_rating': '', 'first_activity_date': '', 'last_activity_date': '', 'influence_score': '', 'user_category': '', 'top_genre': '', '': ''} quoted={} persist=False
[0m10:36:00.315594 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:36:00.316150 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

  
    create or replace temporary view `user_activity__dbt_tmp` as
      -- =======================================================
-- Step 1: Base Ratings
-- Join ratings with movies to attach genres and prepare timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_timestamp,
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: User Activity (General)
-- Calculate total ratings, average rating, first and last activity dates per user
-- =======================================================
user_activity as (
    select
        user_id,
        count(*) as user_rating_count,
        round(avg(rating), 2) as avg_user_rating,
        min(rating_timestamp) as first_activity_date,
        max(rating_timestamp) as last_activity_date
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 3: User Influence Score
-- Compute influence based on total ratings (more ratings = higher influence)
-- =======================================================
user_influence as (
    select
        user_id,
        user_rating_count as influence_score
    from user_activity
),

-- =======================================================
-- Step 4: Active Users by Genre
-- Count ratings per genre and rank genres per user
-- =======================================================
user_genre_activity as (
    select
        user_id,
        regexp_replace(genres, '\\|', ' | ') as genres,
        count(*) as genre_rating_count,
        rank() over (partition by user_id order by count(*) desc) as genre_rank
    from ratings_base
    group by user_id, genres
),

-- =======================================================
-- Step 5: New vs Loyal Users
-- Categorize users as new if first activity < 30 days ago, else loyal
-- =======================================================
user_type as (
    select
        user_id,
        case
            when min(rating_timestamp) >= date_sub(current_date(), 30)
                then 'new_user'
            else 'loyal_user'
        end as user_category
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 6: Top Genre per User
-- Identify the most active genre for each user
-- =======================================================
top_genre as (
    select
        user_id,
        max(genres) as top_genre
    from user_genre_activity
    group by user_id
)

-- =======================================================
-- Final Selection
-- Combine general activity, influence, type, and top genre per user
-- =======================================================
select
    ua.user_id,
    ua.user_rating_count,
    ua.avg_user_rating,
    ua.first_activity_date,
    ua.last_activity_date,
    ui.influence_score,
    ut.user_category,
    tg.top_genre
from user_activity ua
join user_influence ui on ua.user_id = ui.user_id
join user_type ut on ua.user_id = ut.user_id
left join top_genre tg on ua.user_id = tg.user_id


  where ua.user_id not in (select user_id from `workspace`.`movielens_volume`.`user_activity`)
  
[0m10:36:00.635858 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m10:36:00.636879 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d4f8-1927-8ad6-3a256ff06336) - Closing
[0m10:36:00.639233 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:36:00.639944 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`movielens_volume`.`user_activity` AS JSON

  
[0m10:36:00.855412 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m10:36:00.857957 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d529-1de1-aa3c-7297d77b8b84) - Closing
[0m10:36:00.860482 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:36:00.861159 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

    
  DESCRIBE TABLE EXTENDED `user_activity__dbt_tmp` AS JSON

  
[0m10:36:01.025522 [debug] [Thread-1 (]: SQL status: OK in 0.160 seconds
[0m10:36:01.028049 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d54b-17a6-8a0f-34f87c590ae0) - Closing
[0m10:36:01.029199 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.user_activity"
[0m10:36:01.030299 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:36:01.030795 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `workspace`.`movielens_volume`.`user_activity` as DBT_INTERNAL_DEST
    using
        `user_activity__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.user_id <=> DBT_INTERNAL_DEST.user_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *

[0m10:36:04.076275 [debug] [Thread-1 (]: SQL status: OK in 3.050 seconds
[0m10:36:04.077301 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976, command-id=01f09ab3-d565-1bb0-9fb1-6da923e4e889) - Closing
[0m10:36:04.078610 [debug] [Thread-1 (]: On model.my_dbt_project.user_activity: Close
[0m10:36:04.078951 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-d2f0-1d32-bbe2-0efed67de976) - Closing
[0m10:36:04.193106 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1B78B90>]}
[0m10:36:04.193859 [info ] [Thread-1 (]: 7 of 9 OK created sql incremental model movielens_volume.user_activity ......... [[32mOK[0m in 7.53s]
[0m10:36:04.194639 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.user_activity
[0m10:36:04.195019 [debug] [Thread-1 (]: Began running node model.my_dbt_project.user_preferences
[0m10:36:04.195557 [info ] [Thread-1 (]: 8 of 9 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m10:36:04.196293 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.user_preferences) - Creating connection
[0m10:36:04.196683 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.user_preferences'
[0m10:36:04.197008 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.user_preferences
[0m10:36:04.201166 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m10:36:04.202341 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.user_preferences
[0m10:36:04.204809 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m10:36:04.207183 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:04.207564 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'workspace' 
    AND schema_name = 'movielens_volume'
    AND table_name = 'user_preferences'
  
[0m10:36:04.207885 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:36:04.545788 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-d76e-1900-80a7-236bafebd573) - Created
[0m10:36:04.883789 [debug] [Thread-1 (]: SQL status: OK in 0.680 seconds
[0m10:36:04.885899 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d77d-1997-b8fe-25dbdfc83d50) - Closing
[0m10:36:04.888745 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:04.889357 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'workspace'
    AND schema_name = 'movielens_volume'
    AND table_name = 'user_preferences';
  
[0m10:36:05.174164 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m10:36:05.176083 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d7b2-199e-a5df-9581ecb95164) - Closing
[0m10:36:05.178155 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:05.178541 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT column_name
  FROM `workspace`.`information_schema`.`columns`
  WHERE table_catalog = 'workspace' 
    AND table_schema = 'movielens_volume'
    AND table_name = 'user_preferences'
    AND is_nullable = 'NO';
  
[0m10:36:05.573234 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m10:36:05.575251 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d7de-1863-8311-0b4fd5d6bb1a) - Closing
[0m10:36:05.578057 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:05.578540 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `workspace`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'workspace' 
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'user_preferences' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'user_preferences' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:36:06.197056 [debug] [Thread-1 (]: SQL status: OK in 0.620 seconds
[0m10:36:06.198878 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d81b-16d0-8d57-601b07f836f7) - Closing
[0m10:36:06.200885 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:06.201298 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `workspace`.information_schema.key_column_usage kcu
  JOIN `workspace`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `workspace`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'workspace'
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'user_preferences'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'user_preferences'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:36:07.048695 [debug] [Thread-1 (]: SQL status: OK in 0.850 seconds
[0m10:36:07.050557 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d87a-1dca-bc63-f93959e120c2) - Closing
[0m10:36:07.053560 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:07.053967 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'workspace'
    AND table_schema = 'movielens_volume'
    AND table_name = 'user_preferences';
  
[0m10:36:07.327980 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m10:36:07.330652 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d8fc-1b2f-9d30-0b8d6556efe5) - Closing
[0m10:36:07.333887 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:07.334668 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
SHOW TBLPROPERTIES `workspace`.`movielens_volume`.`user_preferences`
  
[0m10:36:07.663105 [debug] [Thread-1 (]: SQL status: OK in 0.330 seconds
[0m10:36:07.665199 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d927-1e1e-8d78-64d985cc3bdc) - Closing
[0m10:36:07.667489 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:07.667868 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m10:36:07.919431 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m10:36:07.921432 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d95a-17b1-8d90-b78d157a700a) - Closing
[0m10:36:07.922241 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'user_id': '', 'user_rating_count': '', 'user_avg_rating': '', 'first_activity_date': '', 'last_activity_date': '', 'influence_score': '', 'top_genre': '', '': ''} quoted={} persist=False
[0m10:36:07.922899 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:07.923427 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create or replace temporary view `user_preferences__dbt_tmp` as
      -- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score,  -- influence based on total ratings
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )
  
[0m10:36:08.216389 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m10:36:08.217414 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d981-1ae5-b750-349fdd7a63fe) - Closing
[0m10:36:08.220067 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:08.220752 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`movielens_volume`.`user_preferences` AS JSON

  
[0m10:36:08.455020 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m10:36:08.457780 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d9ae-1bbb-9f06-70d45edf9d33) - Closing
[0m10:36:08.460034 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:08.460775 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

    
  DESCRIBE TABLE EXTENDED `user_preferences__dbt_tmp` AS JSON

  
[0m10:36:08.626784 [debug] [Thread-1 (]: SQL status: OK in 0.170 seconds
[0m10:36:08.629321 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d9d3-1170-b080-e0b30bc71275) - Closing
[0m10:36:08.630585 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m10:36:08.631654 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:36:08.632166 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
    using
        `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.user_id <=> DBT_INTERNAL_DEST.user_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *

[0m10:36:14.960890 [debug] [Thread-1 (]: SQL status: OK in 6.330 seconds
[0m10:36:14.962001 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-d76e-1900-80a7-236bafebd573, command-id=01f09ab3-d9ed-14a9-9136-353232836d9d) - Closing
[0m10:36:15.096679 [debug] [Thread-1 (]: On model.my_dbt_project.user_preferences: Close
[0m10:36:15.097208 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-d76e-1900-80a7-236bafebd573) - Closing
[0m10:36:15.208259 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1E49490>]}
[0m10:36:15.209275 [info ] [Thread-1 (]: 8 of 9 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 11.01s]
[0m10:36:15.210083 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.user_preferences
[0m10:36:15.210512 [debug] [Thread-1 (]: Began running node model.my_dbt_project.movie_performance
[0m10:36:15.211074 [info ] [Thread-1 (]: 9 of 9 START sql incremental model movielens_volume.movie_performance .......... [RUN]
[0m10:36:15.211778 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.my_dbt_project.movie_performance) - Creating connection
[0m10:36:15.212201 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.my_dbt_project.movie_performance'
[0m10:36:15.212631 [debug] [Thread-1 (]: Began compiling node model.my_dbt_project.movie_performance
[0m10:36:15.225528 [debug] [Thread-1 (]: Writing injected SQL for node "model.my_dbt_project.movie_performance"
[0m10:36:15.226788 [debug] [Thread-1 (]: Began executing node model.my_dbt_project.movie_performance
[0m10:36:15.228905 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m10:36:15.345761 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:15.346368 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'workspace' 
    AND schema_name = 'movielens_volume'
    AND table_name = 'movie_performance'
  
[0m10:36:15.346704 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:36:15.989261 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b) - Created
[0m10:36:16.309342 [debug] [Thread-1 (]: SQL status: OK in 0.960 seconds
[0m10:36:16.311458 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-de50-1345-9276-9c777a375655) - Closing
[0m10:36:16.314286 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:16.314871 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'workspace'
    AND schema_name = 'movielens_volume'
    AND table_name = 'movie_performance';
  
[0m10:36:16.622294 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m10:36:16.624636 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-de81-1a9b-b7bd-5e24d43ef854) - Closing
[0m10:36:16.626539 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:16.626904 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT column_name
  FROM `workspace`.`information_schema`.`columns`
  WHERE table_catalog = 'workspace' 
    AND table_schema = 'movielens_volume'
    AND table_name = 'movie_performance'
    AND is_nullable = 'NO';
  
[0m10:36:17.025393 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m10:36:17.027760 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-deb1-1017-b731-a1b289274c18) - Closing
[0m10:36:17.031201 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:17.031724 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `workspace`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'workspace' 
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'movie_performance' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'movie_performance' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:36:17.657050 [debug] [Thread-1 (]: SQL status: OK in 0.620 seconds
[0m10:36:17.659274 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-deef-129c-9715-ba5ae9b8796b) - Closing
[0m10:36:17.661827 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:17.662348 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `workspace`.information_schema.key_column_usage kcu
  JOIN `workspace`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `workspace`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'workspace'
    AND kcu.table_schema = 'movielens_volume'
    AND kcu.table_name = 'movie_performance'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `workspace`.information_schema.table_constraints
      WHERE table_catalog = 'workspace'
        AND table_schema = 'movielens_volume'
        AND table_name = 'movie_performance'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m10:36:18.539425 [debug] [Thread-1 (]: SQL status: OK in 0.880 seconds
[0m10:36:18.541818 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-df4f-1348-ac53-14dc6f086faa) - Closing
[0m10:36:18.545078 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:18.545636 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'workspace'
    AND table_schema = 'movielens_volume'
    AND table_name = 'movie_performance';
  
[0m10:36:18.813733 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m10:36:18.815445 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-dfd5-1ed3-a9c7-d0826d92365a) - Closing
[0m10:36:18.818285 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:18.818871 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
SHOW TBLPROPERTIES `workspace`.`movielens_volume`.`movie_performance`
  
[0m10:36:19.093102 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m10:36:19.095325 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-dfff-18b9-b2fb-a76e4b7aaa47) - Closing
[0m10:36:19.098301 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:19.098766 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m10:36:19.377903 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m10:36:19.380348 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-e02a-176f-9b2e-ddc071fc0ca8) - Closing
[0m10:36:19.381433 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={'movie_id': 'Primary identifier for the movie', 'weighted_avg_rating': 'Weighted average rating for the movie (0–5)', 'rating_count': 'Number of ratings used in aggregation', 'genre_rank': 'Rank within genre ordered by weighted_avg_rating', 'polarization_score': 'Standard deviation of ratings (rounded 2 decimals)', 'top_rating_percentage': 'Percentage of ratings >= 4.5 (rounded 2 decimals)'} quoted={'movie_id': False, 'weighted_avg_rating': False, 'rating_count': False, 'genre_rank': False, 'polarization_score': False, 'top_rating_percentage': False} persist=False and comments={'movie_id': '', 'title': '', 'genres': '', 'weighted_avg_rating': '', 'rating_count': '', 'genre_rank': '', 'polarization_score': '', 'top_rating_percentage': '', 'tag_count': '', '': ''} quoted={} persist=False
[0m10:36:19.382176 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:19.382647 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

  
    create or replace temporary view `movie_performance__dbt_tmp` as
      -- =======================================================
-- Step 1: Base Ratings Join
-- Attach movie metadata (title, genres) to ratings
-- =======================================================
with ratings_base as (
    select
        r.movie_id,
        m.title,
        regexp_replace(m.genres, '\\|', ' | ') as genres,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_date
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Weighted Rating
-- Compute avg rating, rating count, weighted avg
-- =======================================================
weighted_rating as (
    select
        movie_id,
        title,
        genres,
        round(avg(rating), 2) as avg_rating,
        count(*) as rating_count,
        round(sum(rating) / count(*), 2) as weighted_avg_rating
    from ratings_base
    group by movie_id, title, genres
),

-- =======================================================
-- Step 3: Polarization Score
-- Standard deviation of ratings per movie
-- =======================================================
polarization as (
    select
        movie_id,
        round(coalesce(stddev(rating), 0.0), 2) as polarization_score
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 4: Top Rating Percentage
-- Percent of ratings >= 4.5
-- =======================================================
top_rating_pct as (
    select
        movie_id,
        round(100.0 * sum(case when rating >= 4.5 then 1 else 0 end) / count(*), 2) as top_rating_percentage
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 5: Genre Ranking
-- Rank movies within each genre by weighted_avg_rating
-- =======================================================
genre_rank as (
    select
        movie_id,
        genres,
        weighted_avg_rating,
        rank() over (partition by genres order by weighted_avg_rating desc) as genre_rank
    from weighted_rating
),

-- =======================================================
-- Step 6: Tag Count
-- Count distinct tags per movie
-- =======================================================
tag_count as (
    select
        movie_id,
        count(distinct tag) as tag_count
    from `workspace`.`movielens_volume`.`silver_tags`
    group by movie_id
)

-- =======================================================
-- Step 7: Final Selection
-- Combine all metrics into the gold table
-- =======================================================
select
    w.movie_id,
    w.title,
    w.genres,
    w.weighted_avg_rating,
    w.rating_count,
    g.genre_rank,
    p.polarization_score,
    tr.top_rating_percentage,
    coalesce(tc.tag_count, 0) as tag_count
from weighted_rating w
join genre_rank g
    on w.movie_id = g.movie_id
join polarization p
    on w.movie_id = p.movie_id
join top_rating_pct tr
    on w.movie_id = tr.movie_id
left join tag_count tc
    on w.movie_id = tc.movie_id


-- Use NOT EXISTS for safer incremental inserts
where not exists (
    select 1 
    from `workspace`.`movielens_volume`.`movie_performance` t
    where t.movie_id = w.movie_id
)
  
[0m10:36:19.704149 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m10:36:19.705584 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-e055-1b27-9013-a5f9a208ba73) - Closing
[0m10:36:19.708160 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:19.708798 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

    
  DESCRIBE TABLE EXTENDED `workspace`.`movielens_volume`.`movie_performance` AS JSON

  
[0m10:36:19.951997 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m10:36:19.955079 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-e087-12e8-8f79-c2dd06e6fdbd) - Closing
[0m10:36:19.959000 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:19.959982 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

    
  DESCRIBE TABLE EXTENDED `movie_performance__dbt_tmp` AS JSON

  
[0m10:36:20.128109 [debug] [Thread-1 (]: SQL status: OK in 0.170 seconds
[0m10:36:20.130552 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-e0ae-1436-b4ec-f4914f816e07) - Closing
[0m10:36:20.131588 [debug] [Thread-1 (]: Writing runtime sql for node "model.my_dbt_project.movie_performance"
[0m10:36:20.132653 [debug] [Thread-1 (]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:36:20.133116 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.10.9", "dbt_databricks_version": "1.10.11", "databricks_sql_connector_version": "4.0.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `workspace`.`movielens_volume`.`movie_performance` as DBT_INTERNAL_DEST
    using
        `movie_performance__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.movie_id <=> DBT_INTERNAL_DEST.movie_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *

[0m10:36:25.231601 [debug] [Thread-1 (]: SQL status: OK in 5.100 seconds
[0m10:36:25.232608 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b, command-id=01f09ab3-e0c7-1e65-8e85-2adbed6f0f27) - Closing
[0m10:36:25.233993 [debug] [Thread-1 (]: On model.my_dbt_project.movie_performance: Close
[0m10:36:25.234383 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f09ab3-de40-1dc5-8a26-461a6dcfa18b) - Closing
[0m10:36:25.344770 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '304dac56-587d-4e3d-9eee-43caecb5339c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1E76930>]}
[0m10:36:25.345551 [info ] [Thread-1 (]: 9 of 9 OK created sql incremental model movielens_volume.movie_performance ..... [[32mOK[0m in 10.13s]
[0m10:36:25.346301 [debug] [Thread-1 (]: Finished running node model.my_dbt_project.movie_performance
[0m10:36:25.347743 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m10:36:25.348122 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:36:25.348698 [info ] [MainThread]: 
[0m10:36:25.349164 [info ] [MainThread]: Finished running 4 incremental models, 5 table models in 0 hours 0 minutes and 56.88 seconds (56.88s).
[0m10:36:25.351015 [debug] [MainThread]: Command end result
[0m10:36:25.396771 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\38349\projects\end_to_end_de_project\dbt\target\manifest.json
[0m10:36:25.399768 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\38349\projects\end_to_end_de_project\dbt\target\semantic_manifest.json
[0m10:36:25.405891 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\38349\projects\end_to_end_de_project\dbt\target\run_results.json
[0m10:36:25.406211 [info ] [MainThread]: 
[0m10:36:25.406645 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:36:25.407008 [info ] [MainThread]: 
[0m10:36:25.407392 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=9
[0m10:36:25.408191 [debug] [MainThread]: Command `dbt run` succeeded at 10:36:25.408086 after 59.98 seconds
[0m10:36:25.408514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1F07D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1F079B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCE1F07770>]}
[0m10:36:25.408849 [debug] [MainThread]: Flushing usage events
[0m10:36:26.219228 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m08:40:25.387156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9053ac75b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9052070220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9052070940>]}


============================== 08:40:25.397443 | 4a034c61-7255-4a67-86df-ddeaa8b6c653 ==============================
[0m08:40:25.397443 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:40:25.398518 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:40:26.539724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4a034c61-7255-4a67-86df-ddeaa8b6c653', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90520701c0>]}
[0m08:40:26.569721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4a034c61-7255-4a67-86df-ddeaa8b6c653', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9026d9bb80>]}
[0m08:40:26.570879 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:40:26.617643 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:40:26.740060 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m08:40:26.741064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4a034c61-7255-4a67-86df-ddeaa8b6c653', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9026ddcf10>]}
[0m08:40:29.526068 [debug] [MainThread]: 1603: static parser failed on gold/genre_popularity.sql
[0m08:40:29.560215 [debug] [MainThread]: 1602: parser fallback to jinja rendering on gold/genre_popularity.sql
[0m08:40:29.569960 [debug] [MainThread]: 1699: static parser successfully parsed gold/movie_enriched.sql
[0m08:40:29.577208 [debug] [MainThread]: 1603: static parser failed on gold/movie_performance.sql
[0m08:40:29.587569 [debug] [MainThread]: 1602: parser fallback to jinja rendering on gold/movie_performance.sql
[0m08:40:29.592545 [debug] [MainThread]: 1603: static parser failed on gold/user_activity.sql
[0m08:40:29.603192 [debug] [MainThread]: 1602: parser fallback to jinja rendering on gold/user_activity.sql
[0m08:40:29.606386 [debug] [MainThread]: 1603: static parser failed on gold/user_preferences.sql
[0m08:40:29.615040 [debug] [MainThread]: 1602: parser fallback to jinja rendering on gold/user_preferences.sql
[0m08:40:29.618721 [debug] [MainThread]: 1699: static parser successfully parsed silver/silver_links.sql
[0m08:40:29.624659 [debug] [MainThread]: 1699: static parser successfully parsed silver/silver_movies.sql
[0m08:40:29.630053 [debug] [MainThread]: 1699: static parser successfully parsed silver/silver_ratings.sql
[0m08:40:29.636993 [debug] [MainThread]: 1699: static parser successfully parsed silver/silver_tags.sql
[0m08:40:30.142927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4a034c61-7255-4a67-86df-ddeaa8b6c653', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9026ae6310>]}
[0m08:40:30.179145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4a034c61-7255-4a67-86df-ddeaa8b6c653', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9026b137f0>]}
[0m08:40:30.180129 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:40:30.181055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a034c61-7255-4a67-86df-ddeaa8b6c653', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9026d3a0a0>]}
[0m08:40:30.183555 [info ] [MainThread]: 
[0m08:40:30.185246 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:40:30.187441 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:40:30.188151 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:40:30.188721 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m08:40:30.189278 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:40:30.880517 [debug] [ThreadPool]: SQL status: OK in 0.6899999976158142 seconds
[0m08:40:30.882220 [debug] [ThreadPool]: On list_workspace: Close
[0m08:40:30.991454 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m08:40:30.992930 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m08:40:31.006957 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m08:40:31.007603 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m08:40:31.008126 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m08:40:31.008748 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:40:31.628932 [debug] [ThreadPool]: SQL status: OK in 0.6200000047683716 seconds
[0m08:40:31.630796 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m08:40:31.631467 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m08:40:31.632162 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m08:40:31.632805 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m08:40:31.744942 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m08:40:31.753641 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:40:31.754444 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:40:31.755236 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:40:32.372517 [debug] [ThreadPool]: SQL status: OK in 0.6200000047683716 seconds
[0m08:40:32.375485 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:40:32.480698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a034c61-7255-4a67-86df-ddeaa8b6c653', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f902521dd00>]}
[0m08:40:32.481642 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:40:32.482192 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:40:32.483397 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:40:32.484193 [info ] [MainThread]: 
[0m08:40:32.501152 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_links
[0m08:40:32.502132 [info ] [Thread-4  ]: 1 of 4 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m08:40:32.503551 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.silver_links)
[0m08:40:32.504315 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_links
[0m08:40:32.509610 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m08:40:32.519208 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (compile): 08:40:32.504884 => 08:40:32.518774
[0m08:40:32.520180 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_links
[0m08:40:32.540801 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:40:32.541493 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m08:40:32.542025 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

      describe extended `workspace`.`movielens_volume`.`silver_links`
  
[0m08:40:32.542623 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:40:33.144226 [debug] [Thread-4  ]: SQL status: OK in 0.6000000238418579 seconds
[0m08:40:33.207138 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m08:40:33.215969 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m08:40:33.216690 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
    using delta
      
      
      
      
      
      
      as
      

with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m08:40:35.364529 [debug] [Thread-4  ]: SQL status: OK in 2.1500000953674316 seconds
[0m08:40:35.407857 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (execute): 08:40:32.520743 => 08:40:35.407600
[0m08:40:35.408704 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: ROLLBACK
[0m08:40:35.409417 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:40:35.410130 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: Close
[0m08:40:35.519012 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4a034c61-7255-4a67-86df-ddeaa8b6c653', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8feffae700>]}
[0m08:40:35.520171 [info ] [Thread-4  ]: 1 of 4 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 3.02s]
[0m08:40:35.521371 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_links
[0m08:40:35.522399 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_movies
[0m08:40:35.523472 [info ] [Thread-4  ]: 2 of 4 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m08:40:35.524865 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_links, now model.my_dbt_project.silver_movies)
[0m08:40:35.525538 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_movies
[0m08:40:35.531344 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m08:40:35.540826 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (compile): 08:40:35.526128 => 08:40:35.540400
[0m08:40:35.541556 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_movies
[0m08:40:35.548982 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:40:35.550026 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m08:40:35.550736 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

      describe extended `workspace`.`movielens_volume`.`silver_movies`
  
[0m08:40:35.551347 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:40:36.165014 [debug] [Thread-4  ]: SQL status: OK in 0.6100000143051147 seconds
[0m08:40:36.170961 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m08:40:36.178980 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m08:40:36.179717 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m08:40:37.983227 [debug] [Thread-4  ]: SQL status: OK in 1.7999999523162842 seconds
[0m08:40:37.987105 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (execute): 08:40:35.542031 => 08:40:37.986825
[0m08:40:37.987812 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: ROLLBACK
[0m08:40:37.988407 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:40:37.988962 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: Close
[0m08:40:38.098914 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4a034c61-7255-4a67-86df-ddeaa8b6c653', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8feff6fb20>]}
[0m08:40:38.101200 [info ] [Thread-4  ]: 2 of 4 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 2.57s]
[0m08:40:38.104771 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_movies
[0m08:40:38.110633 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_ratings
[0m08:40:38.112346 [info ] [Thread-4  ]: 3 of 4 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m08:40:38.115001 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_movies, now model.my_dbt_project.silver_ratings)
[0m08:40:38.116390 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_ratings
[0m08:40:38.126337 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m08:40:38.139991 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (compile): 08:40:38.117398 => 08:40:38.139292
[0m08:40:38.141037 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_ratings
[0m08:40:38.152611 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:40:38.153569 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m08:40:38.154412 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

      describe extended `workspace`.`movielens_volume`.`silver_ratings`
  
[0m08:40:38.155199 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:40:39.531974 [debug] [Thread-4  ]: SQL status: OK in 1.3799999952316284 seconds
[0m08:40:39.538109 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m08:40:40.324441 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m08:40:40.325390 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m08:40:44.764469 [debug] [Thread-4  ]: SQL status: OK in 4.440000057220459 seconds
[0m08:40:44.768431 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (execute): 08:40:38.141554 => 08:40:44.768163
[0m08:40:44.769308 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: ROLLBACK
[0m08:40:44.770140 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:40:44.770699 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: Close
[0m08:40:44.880297 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4a034c61-7255-4a67-86df-ddeaa8b6c653', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8feffa69d0>]}
[0m08:40:44.881610 [info ] [Thread-4  ]: 3 of 4 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 6.77s]
[0m08:40:44.882904 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_ratings
[0m08:40:44.883819 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_tags
[0m08:40:44.885153 [info ] [Thread-4  ]: 4 of 4 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m08:40:44.886685 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_ratings, now model.my_dbt_project.silver_tags)
[0m08:40:44.887474 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_tags
[0m08:40:44.893288 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m08:40:44.901727 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (compile): 08:40:44.887987 => 08:40:44.901258
[0m08:40:44.902401 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_tags
[0m08:40:44.908550 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:40:44.909048 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m08:40:44.909533 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

      describe extended `workspace`.`movielens_volume`.`silver_tags`
  
[0m08:40:44.910050 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:40:45.499959 [debug] [Thread-4  ]: SQL status: OK in 0.5899999737739563 seconds
[0m08:40:45.507644 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m08:40:45.515965 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m08:40:45.516657 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m08:40:48.202930 [debug] [Thread-4  ]: SQL status: OK in 2.690000057220459 seconds
[0m08:40:48.206456 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (execute): 08:40:44.902829 => 08:40:48.206165
[0m08:40:48.207080 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: ROLLBACK
[0m08:40:48.207662 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:40:48.208253 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: Close
[0m08:40:48.312204 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4a034c61-7255-4a67-86df-ddeaa8b6c653', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8feffa69d0>]}
[0m08:40:48.313472 [info ] [Thread-4  ]: 4 of 4 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 3.43s]
[0m08:40:48.314717 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_tags
[0m08:40:48.317633 [debug] [MainThread]: On master: ROLLBACK
[0m08:40:48.318322 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:40:48.652509 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:40:48.653352 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:40:48.653984 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:40:48.654627 [debug] [MainThread]: On master: ROLLBACK
[0m08:40:48.655438 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:40:48.656011 [debug] [MainThread]: On master: Close
[0m08:40:48.760162 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:40:48.761014 [debug] [MainThread]: Connection 'model.my_dbt_project.silver_tags' was properly closed.
[0m08:40:48.763402 [info ] [MainThread]: 
[0m08:40:48.764989 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 18.58 seconds (18.58s).
[0m08:40:48.766992 [debug] [MainThread]: Command end result
[0m08:40:48.900334 [info ] [MainThread]: 
[0m08:40:48.901427 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:40:48.902217 [info ] [MainThread]: 
[0m08:40:48.903003 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m08:40:48.904204 [debug] [MainThread]: Command `dbt run` succeeded at 08:40:48.903990 after 22.02 seconds
[0m08:40:48.905015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9053ac75b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9026d05280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f905315eeb0>]}
[0m08:40:48.905781 [debug] [MainThread]: Flushing usage events
[0m08:40:53.815460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d6f4c3640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d6da6f1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d6da6f940>]}


============================== 08:40:53.822871 | a20e4df6-90ca-4298-9442-f5fe542c536d ==============================
[0m08:40:53.822871 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:40:53.823817 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:40:54.801156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a20e4df6-90ca-4298-9442-f5fe542c536d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d6da6f190>]}
[0m08:40:54.835064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a20e4df6-90ca-4298-9442-f5fe542c536d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d427a8b80>]}
[0m08:40:54.836214 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:40:54.884708 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:40:56.090061 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:40:56.090723 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:40:56.101344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a20e4df6-90ca-4298-9442-f5fe542c536d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d42751400>]}
[0m08:40:56.133426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a20e4df6-90ca-4298-9442-f5fe542c536d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d4267e430>]}
[0m08:40:56.134463 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:40:56.135371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a20e4df6-90ca-4298-9442-f5fe542c536d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d4267e490>]}
[0m08:40:56.137865 [info ] [MainThread]: 
[0m08:40:56.139281 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:40:56.141665 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:40:56.142382 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:40:56.142970 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m08:40:56.143569 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:40:56.685416 [debug] [ThreadPool]: SQL status: OK in 0.5400000214576721 seconds
[0m08:40:56.687202 [debug] [ThreadPool]: On list_workspace: Close
[0m08:40:56.791697 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m08:40:56.792992 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m08:40:56.806914 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m08:40:56.807658 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m08:40:56.808255 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m08:40:56.808839 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:40:57.406740 [debug] [ThreadPool]: SQL status: OK in 0.6000000238418579 seconds
[0m08:40:57.408510 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m08:40:57.409156 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m08:40:57.409778 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m08:40:57.410322 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m08:40:57.514563 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m08:40:57.524184 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:40:57.524994 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:40:57.525593 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:40:58.118106 [debug] [ThreadPool]: SQL status: OK in 0.5899999737739563 seconds
[0m08:40:58.121800 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:40:58.237127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a20e4df6-90ca-4298-9442-f5fe542c536d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d4267e160>]}
[0m08:40:58.238208 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:40:58.238927 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:40:58.240304 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:40:58.241391 [info ] [MainThread]: 
[0m08:40:58.263607 [debug] [Thread-4  ]: Began running node model.my_dbt_project.genre_popularity
[0m08:40:58.264702 [info ] [Thread-4  ]: 1 of 5 START sql incremental model movielens_volume.genre_popularity ........... [RUN]
[0m08:40:58.266104 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.genre_popularity)
[0m08:40:58.266964 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.genre_popularity
[0m08:40:58.290847 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.genre_popularity"
[0m08:40:58.300768 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (compile): 08:40:58.267501 => 08:40:58.300230
[0m08:40:58.301775 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.genre_popularity
[0m08:40:58.350864 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:40:58.351684 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m08:40:58.352320 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m08:40:58.353057 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:40:58.958067 [debug] [Thread-4  ]: SQL status: OK in 0.6000000238418579 seconds
[0m08:40:58.992749 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m08:40:58.993822 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

  
    create temporary view `genre_popularity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings with Genres
-- Join silver ratings with silver movies to attach genres
-- Prepare rating timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        r.rating,
        r.rating_date as rating_timestamp,  
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Aggregate Genre Metrics
-- For each genre, calculate total ratings, avg rating,
-- and distribution insights
-- =======================================================
genre_stats as (
    select
        genres,  -- genres already formatted with spacing
        count(*) as rating_count,
        round(avg(rating), 2) as avg_rating,  
        to_date(min(rating_timestamp)) as first_rating_date,  
        to_date(max(rating_timestamp)) as last_rating_date   
    from ratings_base
    group by genres
),

-- =======================================================
-- Step 3: Ranking Genres
-- Rank genres by popularity (# of ratings)
-- and quality (avg rating)
-- =======================================================
ranked_genres as (
    select
        genres,
        rating_count,
        avg_rating,
        first_rating_date,
        last_rating_date,
        rank() over (order by rating_count desc) as popularity_rank,
        rank() over (order by avg_rating desc) as quality_rank
    from genre_stats
)

-- =======================================================
-- Step 4: Final Selection
-- One row per genre with all metrics
-- =======================================================
select
    genres,
    rating_count,
    avg_rating,
    popularity_rank,
    quality_rank,
    first_rating_date,
    last_rating_date
from ranked_genres


  -- Only insert new genres on incremental runs
  where genres not in (select genres from `workspace`.`movielens_volume`.`genre_popularity`)

  
[0m08:40:59.367326 [debug] [Thread-4  ]: SQL status: OK in 0.3700000047683716 seconds
[0m08:40:59.399321 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m08:40:59.400247 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m08:40:59.611620 [debug] [Thread-4  ]: SQL status: OK in 0.20999999344348907 seconds
[0m08:40:59.625304 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.genre_popularity"
[0m08:40:59.633825 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m08:40:59.634575 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`genre_popularity` as DBT_INTERNAL_DEST
      using `genre_popularity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.genres = DBT_INTERNAL_DEST.genres
          

      when matched then update set
         * 

      when not matched then insert *

[0m08:41:05.319889 [debug] [Thread-4  ]: SQL status: OK in 5.679999828338623 seconds
[0m08:41:05.480084 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (execute): 08:40:58.302346 => 08:41:05.479655
[0m08:41:05.481069 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: ROLLBACK
[0m08:41:05.481849 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:05.482563 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: Close
[0m08:41:05.593601 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a20e4df6-90ca-4298-9442-f5fe542c536d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d424993d0>]}
[0m08:41:05.595386 [info ] [Thread-4  ]: 1 of 5 OK created sql incremental model movielens_volume.genre_popularity ...... [[32mOK[0m in 7.33s]
[0m08:41:05.596676 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.genre_popularity
[0m08:41:05.597592 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_enriched
[0m08:41:05.598557 [info ] [Thread-4  ]: 2 of 5 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m08:41:05.599930 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.genre_popularity, now model.my_dbt_project.movie_enriched)
[0m08:41:05.600714 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_enriched
[0m08:41:05.606769 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m08:41:05.615659 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (compile): 08:41:05.601293 => 08:41:05.615134
[0m08:41:05.616439 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_enriched
[0m08:41:05.632907 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:05.633749 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m08:41:05.634371 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

      describe extended `workspace`.`movielens_volume`.`movie_enriched`
  
[0m08:41:05.635111 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:41:06.512479 [debug] [Thread-4  ]: SQL status: OK in 0.8799999952316284 seconds
[0m08:41:06.554944 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m08:41:06.563187 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m08:41:06.564187 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m08:41:08.862761 [debug] [Thread-4  ]: SQL status: OK in 2.299999952316284 seconds
[0m08:41:08.870907 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (execute): 08:41:05.616898 => 08:41:08.870643
[0m08:41:08.871652 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: ROLLBACK
[0m08:41:08.872382 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:08.872922 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: Close
[0m08:41:08.978309 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a20e4df6-90ca-4298-9442-f5fe542c536d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d4140e7f0>]}
[0m08:41:08.979455 [info ] [Thread-4  ]: 2 of 5 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 3.38s]
[0m08:41:08.980672 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_enriched
[0m08:41:08.981475 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_performance
[0m08:41:08.982228 [info ] [Thread-4  ]: 3 of 5 START sql incremental model movielens_volume.movie_performance .......... [RUN]
[0m08:41:08.983358 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_enriched, now model.my_dbt_project.movie_performance)
[0m08:41:08.984031 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_performance
[0m08:41:08.991147 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_performance"
[0m08:41:08.998859 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (compile): 08:41:08.984596 => 08:41:08.998535
[0m08:41:08.999501 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_performance
[0m08:41:09.009593 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:09.010360 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m08:41:09.010919 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m08:41:09.011607 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:41:09.608246 [debug] [Thread-4  ]: SQL status: OK in 0.6000000238418579 seconds
[0m08:41:09.613185 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m08:41:09.614151 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

  
    create temporary view `movie_performance__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings Join
-- Attach movie metadata (title, genres) to ratings
-- =======================================================
with ratings_base as (
    select
        r.movie_id,
        m.title,
        regexp_replace(m.genres, '\\|', ' | ') as genres,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_date
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Weighted Rating
-- Compute avg rating, rating count, weighted avg
-- =======================================================
weighted_rating as (
    select
        movie_id,
        title,
        genres,
        round(avg(rating), 2) as avg_rating,
        count(*) as rating_count,
        round(sum(rating) / count(*), 2) as weighted_avg_rating
    from ratings_base
    group by movie_id, title, genres
),

-- =======================================================
-- Step 3: Polarization Score
-- Standard deviation of ratings per movie
-- =======================================================
polarization as (
    select
        movie_id,
        round(coalesce(stddev(rating), 0.0), 2) as polarization_score
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 4: Top Rating Percentage
-- Percent of ratings >= 4.5
-- =======================================================
top_rating_pct as (
    select
        movie_id,
        round(100.0 * sum(case when rating >= 4.5 then 1 else 0 end) / count(*), 2) as top_rating_percentage
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 5: Genre Ranking
-- Rank movies within each genre by weighted_avg_rating
-- =======================================================
genre_rank as (
    select
        movie_id,
        genres,
        weighted_avg_rating,
        rank() over (partition by genres order by weighted_avg_rating desc) as genre_rank
    from weighted_rating
),

-- =======================================================
-- Step 6: Tag Count
-- Count distinct tags per movie
-- =======================================================
tag_count as (
    select
        movie_id,
        count(distinct tag) as tag_count
    from `workspace`.`movielens_volume`.`silver_tags`
    group by movie_id
)

-- =======================================================
-- Step 7: Final Selection
-- Combine all metrics into the gold table
-- =======================================================
select
    w.movie_id,
    w.title,
    w.genres,
    w.weighted_avg_rating,
    w.rating_count,
    g.genre_rank,
    p.polarization_score,
    tr.top_rating_percentage,
    coalesce(tc.tag_count, 0) as tag_count
from weighted_rating w
join genre_rank g
    on w.movie_id = g.movie_id
join polarization p
    on w.movie_id = p.movie_id
join top_rating_pct tr
    on w.movie_id = tr.movie_id
left join tag_count tc
    on w.movie_id = tc.movie_id


-- Use NOT EXISTS for safer incremental inserts
where not exists (
    select 1 
    from `workspace`.`movielens_volume`.`movie_performance` t
    where t.movie_id = w.movie_id
)

  
[0m08:41:09.926992 [debug] [Thread-4  ]: SQL status: OK in 0.3100000023841858 seconds
[0m08:41:09.932049 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m08:41:09.932752 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m08:41:10.144806 [debug] [Thread-4  ]: SQL status: OK in 0.20999999344348907 seconds
[0m08:41:10.149436 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_performance"
[0m08:41:10.158419 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m08:41:10.159234 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`movie_performance` as DBT_INTERNAL_DEST
      using `movie_performance__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.movie_id = DBT_INTERNAL_DEST.movie_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m08:41:16.076761 [debug] [Thread-4  ]: SQL status: OK in 5.920000076293945 seconds
[0m08:41:16.080202 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (execute): 08:41:08.999969 => 08:41:16.079922
[0m08:41:16.081028 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: ROLLBACK
[0m08:41:16.081642 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:16.082195 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: Close
[0m08:41:16.208059 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a20e4df6-90ca-4298-9442-f5fe542c536d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d4140e8e0>]}
[0m08:41:16.209210 [info ] [Thread-4  ]: 3 of 5 OK created sql incremental model movielens_volume.movie_performance ..... [[32mOK[0m in 7.22s]
[0m08:41:16.210253 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_performance
[0m08:41:16.211129 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_activity
[0m08:41:16.212231 [info ] [Thread-4  ]: 4 of 5 START sql incremental model movielens_volume.user_activity .............. [RUN]
[0m08:41:16.213530 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_performance, now model.my_dbt_project.user_activity)
[0m08:41:16.214161 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_activity
[0m08:41:16.221287 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_activity"
[0m08:41:16.230126 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (compile): 08:41:16.214596 => 08:41:16.229716
[0m08:41:16.230913 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_activity
[0m08:41:16.237374 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:16.238391 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m08:41:16.239158 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m08:41:16.239853 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:41:17.010095 [debug] [Thread-4  ]: SQL status: OK in 0.7699999809265137 seconds
[0m08:41:17.014981 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m08:41:17.015844 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

  
    create temporary view `user_activity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Join ratings with movies to attach genres and prepare timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_timestamp,
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: User Activity (General)
-- Calculate total ratings, average rating, first and last activity dates per user
-- =======================================================
user_activity as (
    select
        user_id,
        count(*) as user_rating_count,
        round(avg(rating), 2) as avg_user_rating,
        min(rating_timestamp) as first_activity_date,
        max(rating_timestamp) as last_activity_date
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 3: User Influence Score
-- Compute influence based on total ratings (more ratings = higher influence)
-- =======================================================
user_influence as (
    select
        user_id,
        user_rating_count as influence_score
    from user_activity
),

-- =======================================================
-- Step 4: Active Users by Genre
-- Count ratings per genre and rank genres per user
-- =======================================================
user_genre_activity as (
    select
        user_id,
        regexp_replace(genres, '\\|', ' | ') as genres,
        count(*) as genre_rating_count,
        rank() over (partition by user_id order by count(*) desc) as genre_rank
    from ratings_base
    group by user_id, genres
),

-- =======================================================
-- Step 5: New vs Loyal Users
-- Categorize users as new if first activity < 30 days ago, else loyal
-- =======================================================
user_type as (
    select
        user_id,
        case
            when min(rating_timestamp) >= date_sub(current_date(), 30)
                then 'new_user'
            else 'loyal_user'
        end as user_category
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 6: Top Genre per User
-- Identify the most active genre for each user
-- =======================================================
top_genre as (
    select
        user_id,
        max(genres) as top_genre
    from user_genre_activity
    group by user_id
)

-- =======================================================
-- Final Selection
-- Combine general activity, influence, type, and top genre per user
-- =======================================================
select
    ua.user_id,
    ua.user_rating_count,
    ua.avg_user_rating,
    ua.first_activity_date,
    ua.last_activity_date,
    ui.influence_score,
    ut.user_category,
    tg.top_genre
from user_activity ua
join user_influence ui on ua.user_id = ui.user_id
join user_type ut on ua.user_id = ut.user_id
left join top_genre tg on ua.user_id = tg.user_id


  where ua.user_id not in (select user_id from `workspace`.`movielens_volume`.`user_activity`)

  
[0m08:41:17.348105 [debug] [Thread-4  ]: SQL status: OK in 0.33000001311302185 seconds
[0m08:41:17.357033 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m08:41:17.357770 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m08:41:17.600156 [debug] [Thread-4  ]: SQL status: OK in 0.23999999463558197 seconds
[0m08:41:17.604977 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_activity"
[0m08:41:17.613997 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m08:41:17.614870 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_activity` as DBT_INTERNAL_DEST
      using `user_activity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m08:41:20.959714 [debug] [Thread-4  ]: SQL status: OK in 3.3399999141693115 seconds
[0m08:41:20.963362 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (execute): 08:41:16.231476 => 08:41:20.963048
[0m08:41:20.964202 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: ROLLBACK
[0m08:41:20.964989 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:20.965639 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: Close
[0m08:41:21.085333 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a20e4df6-90ca-4298-9442-f5fe542c536d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d41351fd0>]}
[0m08:41:21.086653 [info ] [Thread-4  ]: 4 of 5 OK created sql incremental model movielens_volume.user_activity ......... [[32mOK[0m in 4.87s]
[0m08:41:21.087949 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_activity
[0m08:41:21.088820 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_preferences
[0m08:41:21.089865 [info ] [Thread-4  ]: 5 of 5 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m08:41:21.091739 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.user_activity, now model.my_dbt_project.user_preferences)
[0m08:41:21.092499 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_preferences
[0m08:41:21.099462 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m08:41:21.112296 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (compile): 08:41:21.093037 => 08:41:21.111858
[0m08:41:21.113009 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_preferences
[0m08:41:21.119487 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:21.120113 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:41:21.120616 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m08:41:21.121214 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:41:21.756227 [debug] [Thread-4  ]: SQL status: OK in 0.6399999856948853 seconds
[0m08:41:21.761798 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:41:21.762934 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create temporary view `user_preferences__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score,  -- influence based on total ratings
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )

  
[0m08:41:22.041168 [debug] [Thread-4  ]: SQL status: OK in 0.2800000011920929 seconds
[0m08:41:22.046366 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:41:22.047143 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m08:41:22.272058 [debug] [Thread-4  ]: SQL status: OK in 0.2199999988079071 seconds
[0m08:41:22.276687 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m08:41:22.285159 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:41:22.285856 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
      using `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m08:41:27.909884 [debug] [Thread-4  ]: SQL status: OK in 5.619999885559082 seconds
[0m08:41:28.046685 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (execute): 08:41:21.113481 => 08:41:28.046419
[0m08:41:28.047546 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: ROLLBACK
[0m08:41:28.048337 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:28.048948 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: Close
[0m08:41:28.154078 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a20e4df6-90ca-4298-9442-f5fe542c536d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d41449fa0>]}
[0m08:41:28.155332 [info ] [Thread-4  ]: 5 of 5 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 7.06s]
[0m08:41:28.156691 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_preferences
[0m08:41:28.159580 [debug] [MainThread]: On master: ROLLBACK
[0m08:41:28.160392 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:41:28.494224 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:41:28.495125 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:28.495728 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:41:28.496371 [debug] [MainThread]: On master: ROLLBACK
[0m08:41:28.497068 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:41:28.497764 [debug] [MainThread]: On master: Close
[0m08:41:28.604760 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:41:28.605587 [debug] [MainThread]: Connection 'model.my_dbt_project.user_preferences' was properly closed.
[0m08:41:28.608002 [info ] [MainThread]: 
[0m08:41:28.609369 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 32.47 seconds (32.47s).
[0m08:41:28.611588 [debug] [MainThread]: Command end result
[0m08:41:28.640590 [info ] [MainThread]: 
[0m08:41:28.641459 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:41:28.642152 [info ] [MainThread]: 
[0m08:41:28.642949 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m08:41:28.644336 [debug] [MainThread]: Command `dbt run` succeeded at 08:41:28.644162 after 33.33 seconds
[0m08:41:28.645094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d6f4c3640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d427a8b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5d424b8040>]}
[0m08:41:28.645762 [debug] [MainThread]: Flushing usage events
[0m08:41:32.995460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82103835b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f820e8f0220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f820e8f0940>]}


============================== 08:41:33.002389 | 743cab9e-33fe-4c81-9d0a-772c1d76cf70 ==============================
[0m08:41:33.002389 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:41:33.003195 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:41:33.923835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '743cab9e-33fe-4c81-9d0a-772c1d76cf70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f820e8f01c0>]}
[0m08:41:33.954931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '743cab9e-33fe-4c81-9d0a-772c1d76cf70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81e3615c40>]}
[0m08:41:33.955936 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:41:33.998031 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:41:35.344133 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:41:35.344968 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:41:35.355880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '743cab9e-33fe-4c81-9d0a-772c1d76cf70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81e35cb400>]}
[0m08:41:35.391110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '743cab9e-33fe-4c81-9d0a-772c1d76cf70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81e34f6430>]}
[0m08:41:35.392725 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:41:35.394211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '743cab9e-33fe-4c81-9d0a-772c1d76cf70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81e34f6490>]}
[0m08:41:35.399453 [info ] [MainThread]: 
[0m08:41:35.401875 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:41:35.407717 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m08:41:35.419340 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:41:35.420680 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:41:35.421588 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:41:36.232818 [debug] [ThreadPool]: SQL status: OK in 0.8100000023841858 seconds
[0m08:41:36.235772 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:41:36.336714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '743cab9e-33fe-4c81-9d0a-772c1d76cf70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81e34f6310>]}
[0m08:41:36.337731 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:36.338368 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:41:36.339299 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:41:36.339986 [info ] [MainThread]: 
[0m08:41:36.355016 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:41:36.356109 [info ] [Thread-2  ]: 1 of 22 START test accepted_rating_range_silver_ratings_rating__5__0 ........... [RUN]
[0m08:41:36.357558 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff)
[0m08:41:36.358285 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:41:36.370632 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m08:41:36.381011 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (compile): 08:41:36.359012 => 08:41:36.380534
[0m08:41:36.381765 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:41:36.411832 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m08:41:36.421421 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:36.422113 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m08:41:36.422699 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`silver_ratings`
where rating < 0
   or rating > 5


      
    ) dbt_internal_test
[0m08:41:36.423417 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:37.200849 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m08:41:37.211226 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (execute): 08:41:36.382335 => 08:41:37.210867
[0m08:41:37.212021 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: ROLLBACK
[0m08:41:37.212706 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:37.213338 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: Close
[0m08:41:37.314444 [info ] [Thread-2  ]: 1 of 22 PASS accepted_rating_range_silver_ratings_rating__5__0 ................. [[32mPASS[0m in 0.96s]
[0m08:41:37.316619 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:41:37.317876 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:41:37.319589 [info ] [Thread-2  ]: 2 of 22 START test not_null_silver_links_imdb_id ............................... [RUN]
[0m08:41:37.321252 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff, now test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51)
[0m08:41:37.322259 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:41:37.336186 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m08:41:37.347177 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (compile): 08:41:37.322893 => 08:41:37.346699
[0m08:41:37.348097 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:41:37.352415 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m08:41:37.364058 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:37.364839 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m08:41:37.365561 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



      
    ) dbt_internal_test
[0m08:41:37.366468 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:38.208311 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m08:41:38.212351 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (execute): 08:41:37.348777 => 08:41:38.211985
[0m08:41:38.213253 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: ROLLBACK
[0m08:41:38.214009 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:38.214649 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m08:41:38.322229 [info ] [Thread-2  ]: 2 of 22 PASS not_null_silver_links_imdb_id ..................................... [[32mPASS[0m in 1.00s]
[0m08:41:38.324433 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:41:38.326000 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:41:38.327635 [info ] [Thread-2  ]: 3 of 22 START test not_null_silver_links_movie_id .............................. [RUN]
[0m08:41:38.330803 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51, now test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d)
[0m08:41:38.332499 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:41:38.350542 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m08:41:38.366076 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (compile): 08:41:38.333630 => 08:41:38.365364
[0m08:41:38.367278 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:41:38.373844 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m08:41:38.387763 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:38.388752 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m08:41:38.389432 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



      
    ) dbt_internal_test
[0m08:41:38.390251 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:39.134550 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m08:41:39.138734 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (execute): 08:41:38.368171 => 08:41:39.138363
[0m08:41:39.139567 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: ROLLBACK
[0m08:41:39.140238 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:39.140911 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m08:41:39.243194 [info ] [Thread-2  ]: 3 of 22 PASS not_null_silver_links_movie_id .................................... [[32mPASS[0m in 0.91s]
[0m08:41:39.245653 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:41:39.246677 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:41:39.247592 [info ] [Thread-2  ]: 4 of 22 START test not_null_silver_links_tmdb_id ............................... [RUN]
[0m08:41:39.249482 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d, now test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f)
[0m08:41:39.250639 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:41:39.268538 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m08:41:39.281067 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (compile): 08:41:39.251496 => 08:41:39.280449
[0m08:41:39.281953 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:41:39.289399 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m08:41:39.299416 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:39.300106 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m08:41:39.300683 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



      
    ) dbt_internal_test
[0m08:41:39.301299 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:40.016515 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m08:41:40.020813 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (execute): 08:41:39.282546 => 08:41:40.020465
[0m08:41:40.021696 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: ROLLBACK
[0m08:41:40.022575 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:40.023169 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m08:41:40.133014 [info ] [Thread-2  ]: 4 of 22 PASS not_null_silver_links_tmdb_id ..................................... [[32mPASS[0m in 0.88s]
[0m08:41:40.134254 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:41:40.135033 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:41:40.135641 [info ] [Thread-2  ]: 5 of 22 START test not_null_silver_movies_genres ............................... [RUN]
[0m08:41:40.136922 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f, now test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63)
[0m08:41:40.137691 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:41:40.146435 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m08:41:40.157717 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (compile): 08:41:40.138174 => 08:41:40.157172
[0m08:41:40.158550 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:41:40.162392 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m08:41:40.172418 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:40.173069 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m08:41:40.173640 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



      
    ) dbt_internal_test
[0m08:41:40.174370 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:40.976801 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m08:41:40.980942 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (execute): 08:41:40.159213 => 08:41:40.980595
[0m08:41:40.981651 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: ROLLBACK
[0m08:41:40.982304 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:40.982990 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m08:41:41.092070 [info ] [Thread-2  ]: 5 of 22 PASS not_null_silver_movies_genres ..................................... [[32mPASS[0m in 0.96s]
[0m08:41:41.093496 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:41:41.094351 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:41:41.095178 [info ] [Thread-2  ]: 6 of 22 START test not_null_silver_movies_movie_id ............................. [RUN]
[0m08:41:41.096740 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63, now test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e)
[0m08:41:41.097410 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:41:41.103379 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m08:41:41.111648 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (compile): 08:41:41.097927 => 08:41:41.111207
[0m08:41:41.112331 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:41:41.116443 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m08:41:41.124275 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:41.124948 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m08:41:41.125556 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



      
    ) dbt_internal_test
[0m08:41:41.126156 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:43.453956 [debug] [Thread-2  ]: SQL status: OK in 2.3299999237060547 seconds
[0m08:41:43.458429 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (execute): 08:41:41.112772 => 08:41:43.457944
[0m08:41:43.459395 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: ROLLBACK
[0m08:41:43.460175 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:43.460816 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m08:41:43.571073 [info ] [Thread-2  ]: 6 of 22 PASS not_null_silver_movies_movie_id ................................... [[32mPASS[0m in 2.47s]
[0m08:41:43.572558 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:41:43.573318 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:41:43.574123 [info ] [Thread-2  ]: 7 of 22 START test not_null_silver_movies_title ................................ [RUN]
[0m08:41:43.575391 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e, now test.my_dbt_project.not_null_silver_movies_title.3a779bd2de)
[0m08:41:43.575938 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:41:43.588433 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m08:41:43.597631 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (compile): 08:41:43.576468 => 08:41:43.596928
[0m08:41:43.598681 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:41:43.602748 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m08:41:43.612183 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:43.612841 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m08:41:43.613464 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



      
    ) dbt_internal_test
[0m08:41:43.614080 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:44.390127 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m08:41:44.394368 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (execute): 08:41:43.599290 => 08:41:44.394058
[0m08:41:44.395157 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: ROLLBACK
[0m08:41:44.396004 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:44.396775 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m08:41:44.513296 [info ] [Thread-2  ]: 7 of 22 PASS not_null_silver_movies_title ...................................... [[32mPASS[0m in 0.94s]
[0m08:41:44.514788 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:41:44.515632 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:41:44.516358 [info ] [Thread-2  ]: 8 of 22 START test not_null_silver_ratings_movie_id ............................ [RUN]
[0m08:41:44.517616 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_title.3a779bd2de, now test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36)
[0m08:41:44.518258 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:41:44.524327 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m08:41:44.532305 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (compile): 08:41:44.518747 => 08:41:44.531941
[0m08:41:44.532924 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:41:44.536770 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m08:41:44.544860 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:44.545443 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m08:41:44.545962 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



      
    ) dbt_internal_test
[0m08:41:44.546553 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:45.320346 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m08:41:45.324490 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (execute): 08:41:44.533390 => 08:41:45.324185
[0m08:41:45.325256 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: ROLLBACK
[0m08:41:45.326175 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:45.326933 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m08:41:45.427644 [info ] [Thread-2  ]: 8 of 22 PASS not_null_silver_ratings_movie_id .................................. [[32mPASS[0m in 0.91s]
[0m08:41:45.429071 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:41:45.429946 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:41:45.430698 [info ] [Thread-2  ]: 9 of 22 START test not_null_silver_ratings_rating .............................. [RUN]
[0m08:41:45.431923 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36, now test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a)
[0m08:41:45.432634 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:41:45.438625 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m08:41:45.450317 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (compile): 08:41:45.433184 => 08:41:45.450003
[0m08:41:45.451044 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:41:45.454629 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m08:41:45.462952 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:45.463485 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m08:41:45.464136 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



      
    ) dbt_internal_test
[0m08:41:45.464772 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:46.278060 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m08:41:46.282083 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (execute): 08:41:45.451446 => 08:41:46.281771
[0m08:41:46.282976 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: ROLLBACK
[0m08:41:46.283781 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:46.284527 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m08:41:46.390400 [info ] [Thread-2  ]: 9 of 22 PASS not_null_silver_ratings_rating .................................... [[32mPASS[0m in 0.96s]
[0m08:41:46.391614 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:41:46.392410 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:41:46.393156 [info ] [Thread-2  ]: 10 of 22 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m08:41:46.394344 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a, now test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f)
[0m08:41:46.395014 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:41:46.407821 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m08:41:46.417261 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (compile): 08:41:46.395518 => 08:41:46.416940
[0m08:41:46.417893 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:41:46.421715 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m08:41:46.433654 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:46.434401 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m08:41:46.435044 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



      
    ) dbt_internal_test
[0m08:41:46.435635 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:47.216813 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m08:41:47.220738 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (execute): 08:41:46.418372 => 08:41:47.220413
[0m08:41:47.221667 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: ROLLBACK
[0m08:41:47.222477 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:47.223086 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m08:41:47.330782 [info ] [Thread-2  ]: 10 of 22 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 0.94s]
[0m08:41:47.332033 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:41:47.332887 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:41:47.333779 [info ] [Thread-2  ]: 11 of 22 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m08:41:47.335479 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f, now test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3)
[0m08:41:47.336099 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:41:47.345863 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m08:41:47.354976 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (compile): 08:41:47.336605 => 08:41:47.354600
[0m08:41:47.355662 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:41:47.359356 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m08:41:47.368663 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:47.369330 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m08:41:47.369914 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



      
    ) dbt_internal_test
[0m08:41:47.370579 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:48.123440 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m08:41:48.127611 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (execute): 08:41:47.356098 => 08:41:48.127302
[0m08:41:48.128333 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: ROLLBACK
[0m08:41:48.129125 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:48.129734 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m08:41:48.238354 [info ] [Thread-2  ]: 11 of 22 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 0.90s]
[0m08:41:48.239589 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:41:48.240350 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:41:48.241096 [info ] [Thread-2  ]: 12 of 22 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m08:41:48.242343 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3, now test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3)
[0m08:41:48.243098 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:41:48.251407 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m08:41:48.261165 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (compile): 08:41:48.243689 => 08:41:48.260844
[0m08:41:48.261807 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:41:48.265771 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m08:41:48.275464 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:48.276094 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m08:41:48.276687 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



      
    ) dbt_internal_test
[0m08:41:48.277342 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:49.026227 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m08:41:49.030127 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (execute): 08:41:48.262256 => 08:41:49.029821
[0m08:41:49.030889 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: ROLLBACK
[0m08:41:49.031652 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:49.032317 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m08:41:49.133077 [info ] [Thread-2  ]: 12 of 22 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 0.89s]
[0m08:41:49.134461 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:41:49.135495 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:41:49.136285 [info ] [Thread-2  ]: 13 of 22 START test not_null_silver_tags_tag ................................... [RUN]
[0m08:41:49.137644 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3, now test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8)
[0m08:41:49.138384 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:41:49.147818 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m08:41:49.156664 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (compile): 08:41:49.138852 => 08:41:49.156367
[0m08:41:49.157294 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:41:49.161094 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m08:41:49.170303 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:49.170911 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m08:41:49.171683 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



      
    ) dbt_internal_test
[0m08:41:49.172456 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:49.867220 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m08:41:49.871260 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (execute): 08:41:49.157805 => 08:41:49.870900
[0m08:41:49.872064 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: ROLLBACK
[0m08:41:49.872707 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:49.873311 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m08:41:49.982131 [info ] [Thread-2  ]: 13 of 22 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 0.84s]
[0m08:41:49.983486 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:41:49.984250 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:41:49.984942 [info ] [Thread-2  ]: 14 of 22 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m08:41:49.986091 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8, now test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86)
[0m08:41:49.986699 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:41:49.992801 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m08:41:50.001830 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (compile): 08:41:49.987158 => 08:41:50.001430
[0m08:41:50.002513 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:41:50.006199 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m08:41:50.015304 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:50.015934 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m08:41:50.016474 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



      
    ) dbt_internal_test
[0m08:41:50.017052 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:50.692939 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m08:41:50.696986 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (execute): 08:41:50.003021 => 08:41:50.696653
[0m08:41:50.697809 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: ROLLBACK
[0m08:41:50.698566 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:50.699400 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m08:41:50.801672 [info ] [Thread-2  ]: 14 of 22 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 0.82s]
[0m08:41:50.803002 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:41:50.803729 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:41:50.804456 [info ] [Thread-2  ]: 15 of 22 START test not_null_silver_tags_user_id ............................... [RUN]
[0m08:41:50.805589 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86, now test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3)
[0m08:41:50.806233 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:41:50.812095 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m08:41:50.822498 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (compile): 08:41:50.806768 => 08:41:50.822028
[0m08:41:50.823308 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:41:50.827589 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m08:41:50.837557 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:50.838512 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m08:41:50.839498 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



      
    ) dbt_internal_test
[0m08:41:50.840296 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:51.523571 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m08:41:51.527604 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (execute): 08:41:50.823783 => 08:41:51.527262
[0m08:41:51.528449 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: ROLLBACK
[0m08:41:51.529159 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:51.529949 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m08:41:51.635855 [info ] [Thread-2  ]: 15 of 22 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 0.83s]
[0m08:41:51.637258 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:41:51.638109 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:41:51.638848 [info ] [Thread-2  ]: 16 of 22 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:41:51.640127 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m08:41:51.640894 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:41:51.660899 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:41:51.670513 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 08:41:51.641491 => 08:41:51.670073
[0m08:41:51.671521 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:41:51.675596 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:41:51.685722 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:51.686598 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:41:51.687382 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:41:51.688048 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:52.767529 [debug] [Thread-2  ]: SQL status: OK in 1.0800000429153442 seconds
[0m08:41:52.776377 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 08:41:51.672054 => 08:41:52.775821
[0m08:41:52.777861 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m08:41:52.779205 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:52.780115 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m08:41:52.883540 [info ] [Thread-2  ]: 16 of 22 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.24s]
[0m08:41:52.885048 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:41:52.885824 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:41:52.886534 [info ] [Thread-2  ]: 17 of 22 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:41:52.887815 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m08:41:52.888575 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:41:52.896687 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:41:52.905645 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 08:41:52.889105 => 08:41:52.905309
[0m08:41:52.906367 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:41:52.909899 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:41:52.919296 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:52.920031 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:41:52.920646 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:41:52.921321 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:53.801767 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m08:41:53.805697 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 08:41:52.906797 => 08:41:53.805389
[0m08:41:53.806460 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m08:41:53.807101 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:53.807784 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m08:41:53.916545 [info ] [Thread-2  ]: 17 of 22 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.03s]
[0m08:41:53.918084 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:41:53.918905 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:41:53.919633 [info ] [Thread-2  ]: 18 of 22 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:41:53.920739 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32)
[0m08:41:53.921406 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:41:53.929146 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m08:41:53.937874 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (compile): 08:41:53.921866 => 08:41:53.937586
[0m08:41:53.938550 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:41:53.942034 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m08:41:53.950918 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:53.951558 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m08:41:53.952251 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:41:53.952822 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:55.061199 [debug] [Thread-2  ]: SQL status: OK in 1.1100000143051147 seconds
[0m08:41:55.065377 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (execute): 08:41:53.939061 => 08:41:55.065039
[0m08:41:55.066220 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: ROLLBACK
[0m08:41:55.066950 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:55.067630 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m08:41:55.176375 [info ] [Thread-2  ]: 18 of 22 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.26s]
[0m08:41:55.177691 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:41:55.178536 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:41:55.179332 [info ] [Thread-2  ]: 19 of 22 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:41:55.180545 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32, now test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd)
[0m08:41:55.181203 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:41:55.197058 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m08:41:55.207108 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (compile): 08:41:55.181695 => 08:41:55.206683
[0m08:41:55.207809 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:41:55.211835 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m08:41:55.221009 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:55.221669 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m08:41:55.222218 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:41:55.222803 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:56.270911 [debug] [Thread-2  ]: SQL status: OK in 1.0499999523162842 seconds
[0m08:41:56.274913 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (execute): 08:41:55.208292 => 08:41:56.274609
[0m08:41:56.275648 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: ROLLBACK
[0m08:41:56.276289 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:56.276900 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m08:41:56.384909 [info ] [Thread-2  ]: 19 of 22 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.20s]
[0m08:41:56.386320 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:41:56.387149 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:41:56.387870 [info ] [Thread-2  ]: 20 of 22 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:41:56.389204 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd, now test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719)
[0m08:41:56.389960 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:41:56.400126 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m08:41:56.409752 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (compile): 08:41:56.390535 => 08:41:56.409358
[0m08:41:56.410466 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:41:56.414497 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m08:41:56.424147 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:56.424875 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m08:41:56.425565 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:41:56.426349 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:57.341245 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m08:41:57.345239 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (execute): 08:41:56.410993 => 08:41:57.344881
[0m08:41:57.346000 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: ROLLBACK
[0m08:41:57.346931 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:57.347535 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m08:41:57.459701 [info ] [Thread-2  ]: 20 of 22 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 1.07s]
[0m08:41:57.460877 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:41:57.461682 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:41:57.462554 [info ] [Thread-2  ]: 21 of 22 START test unique_silver_links_movie_id ............................... [RUN]
[0m08:41:57.463726 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719, now test.my_dbt_project.unique_silver_links_movie_id.c422609054)
[0m08:41:57.464349 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:41:57.472404 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m08:41:57.481400 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (compile): 08:41:57.464795 => 08:41:57.481062
[0m08:41:57.482136 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:41:57.485846 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m08:41:57.494860 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:57.495493 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m08:41:57.496065 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:41:57.496801 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:58.176020 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m08:41:58.180073 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (execute): 08:41:57.482612 => 08:41:58.179685
[0m08:41:58.180914 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: ROLLBACK
[0m08:41:58.181635 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:58.182327 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m08:41:58.286255 [info ] [Thread-2  ]: 21 of 22 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 0.82s]
[0m08:41:58.287710 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:41:58.288837 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:41:58.289933 [info ] [Thread-2  ]: 22 of 22 START test unique_silver_movies_movie_id .............................. [RUN]
[0m08:41:58.291351 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_silver_links_movie_id.c422609054, now test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085)
[0m08:41:58.292024 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:41:58.298608 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m08:41:58.309555 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (compile): 08:41:58.292521 => 08:41:58.308864
[0m08:41:58.310343 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:41:58.322654 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m08:41:58.332288 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:58.332998 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m08:41:58.333564 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:41:58.334229 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:41:59.023912 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m08:41:59.027679 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (execute): 08:41:58.310871 => 08:41:59.027383
[0m08:41:59.028400 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: ROLLBACK
[0m08:41:59.028998 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:41:59.029617 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m08:41:59.138685 [info ] [Thread-2  ]: 22 of 22 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 0.85s]
[0m08:41:59.139860 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:41:59.142081 [debug] [MainThread]: On master: ROLLBACK
[0m08:41:59.142629 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:41:59.480917 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:41:59.481660 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:41:59.482184 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:41:59.482706 [debug] [MainThread]: On master: ROLLBACK
[0m08:41:59.483210 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:41:59.483989 [debug] [MainThread]: On master: Close
[0m08:41:59.585957 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:41:59.586724 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085' was properly closed.
[0m08:41:59.588968 [info ] [MainThread]: 
[0m08:41:59.590150 [info ] [MainThread]: Finished running 22 tests in 0 hours 0 minutes and 24.19 seconds (24.19s).
[0m08:41:59.595096 [debug] [MainThread]: Command end result
[0m08:41:59.623142 [info ] [MainThread]: 
[0m08:41:59.624084 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:41:59.624898 [info ] [MainThread]: 
[0m08:41:59.625624 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=0 SKIP=0 TOTAL=22
[0m08:41:59.626651 [debug] [MainThread]: Command `dbt test` succeeded at 08:41:59.626520 after 25.15 seconds
[0m08:41:59.627262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82103835b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81e3615c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81e3754400>]}
[0m08:41:59.628015 [debug] [MainThread]: Flushing usage events
[0m08:42:03.699674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e4bef6640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e4a42e250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e4a42ea00>]}


============================== 08:42:03.706863 | 89c51ac6-35ab-46c5-bb37-1643900b5bfe ==============================
[0m08:42:03.706863 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:42:03.707844 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:42:04.674128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '89c51ac6-35ab-46c5-bb37-1643900b5bfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e4a42e220>]}
[0m08:42:04.704184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '89c51ac6-35ab-46c5-bb37-1643900b5bfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e1f1b9a00>]}
[0m08:42:04.705245 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:42:04.749801 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:42:05.922148 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:42:05.922772 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:42:05.934022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '89c51ac6-35ab-46c5-bb37-1643900b5bfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e1f151490>]}
[0m08:42:05.965002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '89c51ac6-35ab-46c5-bb37-1643900b5bfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e1f07f490>]}
[0m08:42:05.965861 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:42:05.966627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89c51ac6-35ab-46c5-bb37-1643900b5bfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e1f07f460>]}
[0m08:42:05.970928 [info ] [MainThread]: 
[0m08:42:05.972345 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:42:05.974721 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m08:42:05.980909 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:42:05.981552 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:42:05.982094 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:42:06.893962 [debug] [ThreadPool]: SQL status: OK in 0.9100000262260437 seconds
[0m08:42:06.896847 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:42:07.008322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89c51ac6-35ab-46c5-bb37-1643900b5bfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e1f07f370>]}
[0m08:42:07.009832 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:07.010665 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:42:07.012046 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:42:07.013063 [info ] [MainThread]: 
[0m08:42:07.037490 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m08:42:07.038631 [info ] [Thread-2  ]: 1 of 35 START test accepted_rating_range_genre_popularity_avg_rating__5__0 ..... [RUN]
[0m08:42:07.040206 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413)
[0m08:42:07.040990 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m08:42:07.059694 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m08:42:07.071020 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (compile): 08:42:07.041593 => 08:42:07.070524
[0m08:42:07.071870 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m08:42:07.101208 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m08:42:07.110627 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:07.111259 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m08:42:07.111894 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m08:42:07.112611 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:07.936832 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m08:42:07.945570 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (execute): 08:42:07.072369 => 08:42:07.945239
[0m08:42:07.946455 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: ROLLBACK
[0m08:42:07.947320 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:07.947886 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: Close
[0m08:42:08.092733 [info ] [Thread-2  ]: 1 of 35 PASS accepted_rating_range_genre_popularity_avg_rating__5__0 ........... [[32mPASS[0m in 1.05s]
[0m08:42:08.094358 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m08:42:08.095412 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:42:08.096365 [info ] [Thread-2  ]: 2 of 35 START test accepted_rating_range_movie_enriched_avg_rating__5__0 ....... [RUN]
[0m08:42:08.097705 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413, now test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8)
[0m08:42:08.098420 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:42:08.104683 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m08:42:08.116553 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (compile): 08:42:08.099029 => 08:42:08.116125
[0m08:42:08.117306 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:42:08.121268 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m08:42:08.130802 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:08.131465 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m08:42:08.132105 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m08:42:08.132726 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:09.084658 [debug] [Thread-2  ]: SQL status: OK in 0.949999988079071 seconds
[0m08:42:09.088739 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (execute): 08:42:08.117823 => 08:42:09.088398
[0m08:42:09.089495 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: ROLLBACK
[0m08:42:09.090125 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:09.090748 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: Close
[0m08:42:09.202098 [info ] [Thread-2  ]: 2 of 35 PASS accepted_rating_range_movie_enriched_avg_rating__5__0 ............. [[32mPASS[0m in 1.10s]
[0m08:42:09.203535 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:42:09.204352 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m08:42:09.205055 [info ] [Thread-2  ]: 3 of 35 START test accepted_rating_range_movie_performance_weighted_avg_rating__5__0  [RUN]
[0m08:42:09.206507 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8, now test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083)
[0m08:42:09.207322 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m08:42:09.215761 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m08:42:09.225293 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (compile): 08:42:09.207885 => 08:42:09.224694
[0m08:42:09.226541 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m08:42:09.230630 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m08:42:09.240616 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:09.241393 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m08:42:09.242214 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating < 0
   or weighted_avg_rating > 5


      
    ) dbt_internal_test
[0m08:42:09.242853 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:10.191194 [debug] [Thread-2  ]: SQL status: OK in 0.949999988079071 seconds
[0m08:42:10.195163 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (execute): 08:42:09.227120 => 08:42:10.194852
[0m08:42:10.196023 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: ROLLBACK
[0m08:42:10.196869 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:10.197525 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: Close
[0m08:42:10.302292 [info ] [Thread-2  ]: 3 of 35 PASS accepted_rating_range_movie_performance_weighted_avg_rating__5__0 . [[32mPASS[0m in 1.10s]
[0m08:42:10.303613 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m08:42:10.304419 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m08:42:10.305212 [info ] [Thread-2  ]: 4 of 35 START test accepted_rating_range_user_activity_avg_user_rating__5__0 ... [RUN]
[0m08:42:10.306555 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083, now test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a)
[0m08:42:10.307239 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m08:42:10.318515 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m08:42:10.327704 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (compile): 08:42:10.307720 => 08:42:10.327317
[0m08:42:10.328505 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m08:42:10.332241 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m08:42:10.340445 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:10.340971 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m08:42:10.341501 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating < 0
   or avg_user_rating > 5


      
    ) dbt_internal_test
[0m08:42:10.342126 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:11.403366 [debug] [Thread-2  ]: SQL status: OK in 1.059999942779541 seconds
[0m08:42:11.407329 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (execute): 08:42:10.329036 => 08:42:11.407025
[0m08:42:11.408077 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: ROLLBACK
[0m08:42:11.408910 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:11.409717 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: Close
[0m08:42:11.541760 [info ] [Thread-2  ]: 4 of 35 PASS accepted_rating_range_user_activity_avg_user_rating__5__0 ......... [[32mPASS[0m in 1.24s]
[0m08:42:11.543125 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m08:42:11.544096 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m08:42:11.544816 [info ] [Thread-2  ]: 5 of 35 START test not_null_genre_popularity_avg_rating ........................ [RUN]
[0m08:42:11.545935 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a, now test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9)
[0m08:42:11.546575 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m08:42:11.556107 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m08:42:11.565304 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (compile): 08:42:11.547047 => 08:42:11.565011
[0m08:42:11.565969 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m08:42:11.569649 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m08:42:11.578669 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:11.579216 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m08:42:11.579875 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating is null



      
    ) dbt_internal_test
[0m08:42:11.580568 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:12.321740 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m08:42:12.325559 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (execute): 08:42:11.566456 => 08:42:12.325266
[0m08:42:12.326332 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: ROLLBACK
[0m08:42:12.327052 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:12.327774 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: Close
[0m08:42:12.465865 [info ] [Thread-2  ]: 5 of 35 PASS not_null_genre_popularity_avg_rating .............................. [[32mPASS[0m in 0.92s]
[0m08:42:12.467198 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m08:42:12.468025 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m08:42:12.468782 [info ] [Thread-2  ]: 6 of 35 START test not_null_genre_popularity_first_rating_date ................. [RUN]
[0m08:42:12.470026 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9, now test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32)
[0m08:42:12.470715 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m08:42:12.477195 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m08:42:12.487042 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (compile): 08:42:12.471274 => 08:42:12.486592
[0m08:42:12.487744 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m08:42:12.492099 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m08:42:12.502141 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:12.502781 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m08:42:12.503341 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where first_rating_date is null



      
    ) dbt_internal_test
[0m08:42:12.504047 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:13.214416 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m08:42:13.218100 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (execute): 08:42:12.488258 => 08:42:13.217795
[0m08:42:13.218793 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: ROLLBACK
[0m08:42:13.219487 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:13.220121 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: Close
[0m08:42:14.064046 [info ] [Thread-2  ]: 6 of 35 PASS not_null_genre_popularity_first_rating_date ....................... [[32mPASS[0m in 1.59s]
[0m08:42:14.065525 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m08:42:14.066383 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m08:42:14.067023 [info ] [Thread-2  ]: 7 of 35 START test not_null_genre_popularity_genres ............................ [RUN]
[0m08:42:14.068122 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32, now test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa)
[0m08:42:14.068725 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m08:42:14.083217 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m08:42:14.092288 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (compile): 08:42:14.069347 => 08:42:14.091906
[0m08:42:14.870085 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m08:42:14.874223 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m08:42:14.882982 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:14.883526 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m08:42:14.884044 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`genre_popularity`
where genres is null



      
    ) dbt_internal_test
[0m08:42:14.884629 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:15.595542 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m08:42:15.599882 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (execute): 08:42:14.870610 => 08:42:15.599503
[0m08:42:15.600611 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: ROLLBACK
[0m08:42:15.601235 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:15.601872 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: Close
[0m08:42:15.709365 [info ] [Thread-2  ]: 7 of 35 PASS not_null_genre_popularity_genres .................................. [[32mPASS[0m in 1.64s]
[0m08:42:15.710703 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m08:42:15.711542 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m08:42:15.712263 [info ] [Thread-2  ]: 8 of 35 START test not_null_genre_popularity_last_rating_date .................. [RUN]
[0m08:42:15.713444 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa, now test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432)
[0m08:42:15.714227 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m08:42:15.722659 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m08:42:15.731895 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (compile): 08:42:15.714820 => 08:42:15.731556
[0m08:42:15.732549 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m08:42:15.736227 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m08:42:15.744573 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:15.745084 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m08:42:15.745652 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where last_rating_date is null



      
    ) dbt_internal_test
[0m08:42:15.746313 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:16.488067 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m08:42:16.492325 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (execute): 08:42:15.732987 => 08:42:16.492012
[0m08:42:16.493060 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: ROLLBACK
[0m08:42:16.493754 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:16.494419 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: Close
[0m08:42:16.612387 [info ] [Thread-2  ]: 8 of 35 PASS not_null_genre_popularity_last_rating_date ........................ [[32mPASS[0m in 0.90s]
[0m08:42:16.613751 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m08:42:16.614611 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m08:42:16.615464 [info ] [Thread-2  ]: 9 of 35 START test not_null_genre_popularity_popularity_rank ................... [RUN]
[0m08:42:16.616853 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432, now test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc)
[0m08:42:16.617569 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m08:42:16.625967 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m08:42:16.634444 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (compile): 08:42:16.618082 => 08:42:16.634108
[0m08:42:16.635131 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m08:42:16.639036 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m08:42:16.647160 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:16.647683 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m08:42:16.648331 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select popularity_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where popularity_rank is null



      
    ) dbt_internal_test
[0m08:42:16.648995 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:17.414227 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m08:42:17.418694 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (execute): 08:42:16.635634 => 08:42:17.418364
[0m08:42:17.419495 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: ROLLBACK
[0m08:42:17.420130 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:17.420847 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: Close
[0m08:42:17.531395 [info ] [Thread-2  ]: 9 of 35 PASS not_null_genre_popularity_popularity_rank ......................... [[32mPASS[0m in 0.91s]
[0m08:42:17.532755 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m08:42:17.533566 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m08:42:17.534315 [info ] [Thread-2  ]: 10 of 35 START test not_null_genre_popularity_quality_rank ..................... [RUN]
[0m08:42:17.535472 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc, now test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a)
[0m08:42:17.536176 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m08:42:17.545572 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m08:42:17.554206 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (compile): 08:42:17.536664 => 08:42:17.553883
[0m08:42:17.554919 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m08:42:17.558853 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m08:42:17.567615 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:17.568316 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m08:42:17.568823 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select quality_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where quality_rank is null



      
    ) dbt_internal_test
[0m08:42:17.569368 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:18.286854 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m08:42:18.290997 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (execute): 08:42:17.555424 => 08:42:18.290679
[0m08:42:18.291773 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: ROLLBACK
[0m08:42:18.292515 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:18.293159 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: Close
[0m08:42:18.398274 [info ] [Thread-2  ]: 10 of 35 PASS not_null_genre_popularity_quality_rank ........................... [[32mPASS[0m in 0.86s]
[0m08:42:18.399928 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m08:42:18.400811 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m08:42:18.401575 [info ] [Thread-2  ]: 11 of 35 START test not_null_genre_popularity_rating_count ..................... [RUN]
[0m08:42:18.402816 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a, now test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856)
[0m08:42:18.403481 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m08:42:18.410456 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m08:42:18.421765 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (compile): 08:42:18.404029 => 08:42:18.421325
[0m08:42:18.422485 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m08:42:18.426819 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m08:42:18.436201 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:18.436841 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m08:42:18.437421 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`genre_popularity`
where rating_count is null



      
    ) dbt_internal_test
[0m08:42:18.438190 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:19.187324 [debug] [Thread-2  ]: SQL status: OK in 0.75 seconds
[0m08:42:19.191529 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (execute): 08:42:18.422984 => 08:42:19.191227
[0m08:42:19.192224 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: ROLLBACK
[0m08:42:19.192847 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:19.193525 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: Close
[0m08:42:19.311109 [info ] [Thread-2  ]: 11 of 35 PASS not_null_genre_popularity_rating_count ........................... [[32mPASS[0m in 0.91s]
[0m08:42:19.312570 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m08:42:19.313371 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:42:19.314105 [info ] [Thread-2  ]: 12 of 35 START test not_null_movie_enriched_avg_rating ......................... [RUN]
[0m08:42:19.315303 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856, now test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd)
[0m08:42:19.315998 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:42:19.322134 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m08:42:19.331347 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (compile): 08:42:19.316557 => 08:42:19.330966
[0m08:42:19.332120 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:42:19.336068 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m08:42:19.345257 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:19.345900 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m08:42:19.346488 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating is null



      
    ) dbt_internal_test
[0m08:42:19.347143 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:20.030466 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m08:42:20.034336 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (execute): 08:42:19.332648 => 08:42:20.034020
[0m08:42:20.035088 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: ROLLBACK
[0m08:42:20.035787 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:20.036484 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: Close
[0m08:42:20.148853 [info ] [Thread-2  ]: 12 of 35 PASS not_null_movie_enriched_avg_rating ............................... [[32mPASS[0m in 0.83s]
[0m08:42:20.150279 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:42:20.151147 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:42:20.152093 [info ] [Thread-2  ]: 13 of 35 START test not_null_movie_enriched_genres ............................. [RUN]
[0m08:42:20.153554 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd, now test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27)
[0m08:42:20.154283 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:42:20.160413 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m08:42:20.170147 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (compile): 08:42:20.154738 => 08:42:20.169757
[0m08:42:20.170877 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:42:20.183494 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m08:42:20.192492 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:20.193053 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m08:42:20.193640 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`movie_enriched`
where genres is null



      
    ) dbt_internal_test
[0m08:42:20.194524 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:20.961229 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m08:42:20.965305 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (execute): 08:42:20.171478 => 08:42:20.964955
[0m08:42:20.966118 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: ROLLBACK
[0m08:42:20.966869 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:20.967516 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: Close
[0m08:42:21.080297 [info ] [Thread-2  ]: 13 of 35 PASS not_null_movie_enriched_genres ................................... [[32mPASS[0m in 0.93s]
[0m08:42:21.081608 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:42:21.082481 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:42:21.083259 [info ] [Thread-2  ]: 14 of 35 START test not_null_movie_enriched_movie_id ........................... [RUN]
[0m08:42:21.084680 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27, now test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86)
[0m08:42:21.085392 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:42:21.091557 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m08:42:21.101052 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (compile): 08:42:21.085882 => 08:42:21.100664
[0m08:42:21.101743 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:42:21.105583 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m08:42:21.114872 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:21.115526 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m08:42:21.116136 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is null



      
    ) dbt_internal_test
[0m08:42:21.116850 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:21.791187 [debug] [Thread-2  ]: SQL status: OK in 0.6700000166893005 seconds
[0m08:42:21.795321 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (execute): 08:42:21.102166 => 08:42:21.795016
[0m08:42:21.796076 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: ROLLBACK
[0m08:42:21.796831 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:21.797558 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: Close
[0m08:42:21.902308 [info ] [Thread-2  ]: 14 of 35 PASS not_null_movie_enriched_movie_id ................................. [[32mPASS[0m in 0.82s]
[0m08:42:21.903652 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:42:21.904572 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:42:21.905274 [info ] [Thread-2  ]: 15 of 35 START test not_null_movie_enriched_title .............................. [RUN]
[0m08:42:21.906462 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86, now test.my_dbt_project.not_null_movie_enriched_title.d20f00c292)
[0m08:42:21.907141 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:42:21.913345 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m08:42:21.922433 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (compile): 08:42:21.907770 => 08:42:21.922072
[0m08:42:21.923109 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:42:21.927002 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m08:42:21.936182 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:21.936849 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m08:42:21.937473 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`movie_enriched`
where title is null



      
    ) dbt_internal_test
[0m08:42:21.938082 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:22.672595 [debug] [Thread-2  ]: SQL status: OK in 0.7300000190734863 seconds
[0m08:42:22.676899 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (execute): 08:42:21.923565 => 08:42:22.676566
[0m08:42:22.677649 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: ROLLBACK
[0m08:42:22.678413 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:22.679165 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: Close
[0m08:42:22.786786 [info ] [Thread-2  ]: 15 of 35 PASS not_null_movie_enriched_title .................................... [[32mPASS[0m in 0.88s]
[0m08:42:22.788330 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:42:22.789615 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:42:22.790383 [info ] [Thread-2  ]: 16 of 35 START test not_null_movie_enriched_total_ratings ...................... [RUN]
[0m08:42:22.791829 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_title.d20f00c292, now test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04)
[0m08:42:22.793236 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:42:22.801536 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m08:42:22.816821 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (compile): 08:42:22.793953 => 08:42:22.816195
[0m08:42:22.817862 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:42:22.834278 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m08:42:22.847915 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:22.849035 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m08:42:22.849912 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select total_ratings
from `workspace`.`movielens_volume`.`movie_enriched`
where total_ratings is null



      
    ) dbt_internal_test
[0m08:42:22.850949 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:23.525663 [debug] [Thread-2  ]: SQL status: OK in 0.6700000166893005 seconds
[0m08:42:23.529442 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (execute): 08:42:22.818630 => 08:42:23.529142
[0m08:42:23.530169 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: ROLLBACK
[0m08:42:23.530802 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:23.531373 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: Close
[0m08:42:23.647268 [info ] [Thread-2  ]: 16 of 35 PASS not_null_movie_enriched_total_ratings ............................ [[32mPASS[0m in 0.86s]
[0m08:42:23.648719 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:42:23.649532 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m08:42:23.650292 [info ] [Thread-2  ]: 17 of 35 START test not_null_movie_performance_genre_rank ...................... [RUN]
[0m08:42:23.651703 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04, now test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e)
[0m08:42:23.652343 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m08:42:23.660658 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m08:42:23.669513 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (compile): 08:42:23.652837 => 08:42:23.669130
[0m08:42:23.670203 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m08:42:23.673902 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m08:42:23.682742 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:23.683329 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m08:42:23.683924 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genre_rank
from `workspace`.`movielens_volume`.`movie_performance`
where genre_rank is null



      
    ) dbt_internal_test
[0m08:42:23.684566 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:24.366950 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m08:42:24.370986 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (execute): 08:42:23.670633 => 08:42:24.370678
[0m08:42:24.371757 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: ROLLBACK
[0m08:42:24.372626 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:24.373415 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: Close
[0m08:42:24.478954 [info ] [Thread-2  ]: 17 of 35 PASS not_null_movie_performance_genre_rank ............................ [[32mPASS[0m in 0.83s]
[0m08:42:24.480344 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m08:42:24.481158 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m08:42:24.482150 [info ] [Thread-2  ]: 18 of 35 START test not_null_movie_performance_movie_id ........................ [RUN]
[0m08:42:24.483330 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e, now test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52)
[0m08:42:24.483917 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m08:42:24.492032 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m08:42:24.501676 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (compile): 08:42:24.484408 => 08:42:24.501295
[0m08:42:24.502407 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m08:42:24.506272 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m08:42:24.514871 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:24.515423 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m08:42:24.515930 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is null



      
    ) dbt_internal_test
[0m08:42:24.516558 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:25.192947 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m08:42:25.197514 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (execute): 08:42:24.502962 => 08:42:25.197173
[0m08:42:25.198987 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: ROLLBACK
[0m08:42:25.199673 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:25.200337 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: Close
[0m08:42:25.312203 [info ] [Thread-2  ]: 18 of 35 PASS not_null_movie_performance_movie_id .............................. [[32mPASS[0m in 0.83s]
[0m08:42:25.313543 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m08:42:25.314342 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m08:42:25.315057 [info ] [Thread-2  ]: 19 of 35 START test not_null_movie_performance_polarization_score .............. [RUN]
[0m08:42:25.316416 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52, now test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6)
[0m08:42:25.317081 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m08:42:25.325372 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m08:42:25.334630 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (compile): 08:42:25.317595 => 08:42:25.334338
[0m08:42:25.335230 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m08:42:25.340401 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m08:42:25.353012 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:25.353732 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m08:42:25.354314 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select polarization_score
from `workspace`.`movielens_volume`.`movie_performance`
where polarization_score is null



      
    ) dbt_internal_test
[0m08:42:25.354984 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:26.031910 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m08:42:26.035790 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (execute): 08:42:25.335660 => 08:42:26.035485
[0m08:42:26.036536 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: ROLLBACK
[0m08:42:26.037275 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:26.038090 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: Close
[0m08:42:26.144159 [info ] [Thread-2  ]: 19 of 35 PASS not_null_movie_performance_polarization_score .................... [[32mPASS[0m in 0.83s]
[0m08:42:26.145537 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m08:42:26.146369 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m08:42:26.147225 [info ] [Thread-2  ]: 20 of 35 START test not_null_movie_performance_rating_count .................... [RUN]
[0m08:42:26.148465 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6, now test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac)
[0m08:42:26.149157 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m08:42:26.158058 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m08:42:26.167156 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (compile): 08:42:26.149724 => 08:42:26.166852
[0m08:42:26.167841 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m08:42:26.171571 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m08:42:26.180296 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:26.180860 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m08:42:26.181458 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`movie_performance`
where rating_count is null



      
    ) dbt_internal_test
[0m08:42:26.182207 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:26.857755 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m08:42:26.862170 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (execute): 08:42:26.168393 => 08:42:26.861824
[0m08:42:26.863015 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: ROLLBACK
[0m08:42:26.863730 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:26.864606 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: Close
[0m08:42:26.967828 [info ] [Thread-2  ]: 20 of 35 PASS not_null_movie_performance_rating_count .......................... [[32mPASS[0m in 0.82s]
[0m08:42:26.969291 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m08:42:26.970087 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m08:42:26.970789 [info ] [Thread-2  ]: 21 of 35 START test not_null_movie_performance_top_rating_percentage ........... [RUN]
[0m08:42:26.971944 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac, now test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb)
[0m08:42:26.972566 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m08:42:26.978880 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m08:42:26.988931 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (compile): 08:42:26.973183 => 08:42:26.988509
[0m08:42:26.989669 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m08:42:26.993493 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m08:42:27.003500 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:27.004194 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m08:42:27.004776 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_rating_percentage
from `workspace`.`movielens_volume`.`movie_performance`
where top_rating_percentage is null



      
    ) dbt_internal_test
[0m08:42:27.005494 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:27.702020 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m08:42:27.705964 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (execute): 08:42:26.990147 => 08:42:27.705644
[0m08:42:27.706793 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: ROLLBACK
[0m08:42:27.707474 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:27.708302 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: Close
[0m08:42:27.824957 [info ] [Thread-2  ]: 21 of 35 PASS not_null_movie_performance_top_rating_percentage ................. [[32mPASS[0m in 0.85s]
[0m08:42:27.826474 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m08:42:27.827587 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m08:42:27.828363 [info ] [Thread-2  ]: 22 of 35 START test not_null_movie_performance_weighted_avg_rating ............. [RUN]
[0m08:42:27.829579 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb, now test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5)
[0m08:42:27.830170 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m08:42:27.836338 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m08:42:27.846891 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (compile): 08:42:27.830626 => 08:42:27.846278
[0m08:42:27.847930 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m08:42:27.858481 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m08:42:27.868820 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:27.869518 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m08:42:27.870297 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select weighted_avg_rating
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating is null



      
    ) dbt_internal_test
[0m08:42:27.871104 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:28.571527 [debug] [Thread-2  ]: SQL status: OK in 0.699999988079071 seconds
[0m08:42:28.576133 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (execute): 08:42:27.848502 => 08:42:28.575733
[0m08:42:28.576967 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: ROLLBACK
[0m08:42:28.577897 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:28.578644 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: Close
[0m08:42:28.689443 [info ] [Thread-2  ]: 22 of 35 PASS not_null_movie_performance_weighted_avg_rating ................... [[32mPASS[0m in 0.86s]
[0m08:42:28.690902 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m08:42:28.691773 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m08:42:28.692653 [info ] [Thread-2  ]: 23 of 35 START test not_null_user_activity_avg_user_rating ..................... [RUN]
[0m08:42:28.694043 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5, now test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b)
[0m08:42:28.694818 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m08:42:28.701380 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m08:42:28.711485 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (compile): 08:42:28.695384 => 08:42:28.710851
[0m08:42:28.712422 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m08:42:28.716348 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m08:42:28.728974 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:28.729681 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m08:42:28.730227 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_user_rating
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating is null



      
    ) dbt_internal_test
[0m08:42:28.730825 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:29.450660 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m08:42:29.454700 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (execute): 08:42:28.712913 => 08:42:29.454380
[0m08:42:29.455420 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: ROLLBACK
[0m08:42:29.456027 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:29.456640 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: Close
[0m08:42:29.562697 [info ] [Thread-2  ]: 23 of 35 PASS not_null_user_activity_avg_user_rating ........................... [[32mPASS[0m in 0.87s]
[0m08:42:29.564056 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m08:42:29.564862 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m08:42:29.565553 [info ] [Thread-2  ]: 24 of 35 START test not_null_user_activity_first_activity_date ................. [RUN]
[0m08:42:29.566710 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b, now test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac)
[0m08:42:29.567342 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m08:42:29.573368 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m08:42:29.582450 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (compile): 08:42:29.567874 => 08:42:29.582054
[0m08:42:29.583194 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m08:42:29.587012 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m08:42:29.596126 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:29.596802 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m08:42:29.597352 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where first_activity_date is null



      
    ) dbt_internal_test
[0m08:42:29.597906 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:30.272095 [debug] [Thread-2  ]: SQL status: OK in 0.6700000166893005 seconds
[0m08:42:30.276264 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (execute): 08:42:29.583676 => 08:42:30.275965
[0m08:42:30.277006 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: ROLLBACK
[0m08:42:30.277705 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:30.278378 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: Close
[0m08:42:30.395332 [info ] [Thread-2  ]: 24 of 35 PASS not_null_user_activity_first_activity_date ....................... [[32mPASS[0m in 0.83s]
[0m08:42:30.396852 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m08:42:30.397703 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m08:42:30.398454 [info ] [Thread-2  ]: 25 of 35 START test not_null_user_activity_influence_score ..................... [RUN]
[0m08:42:30.399689 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac, now test.my_dbt_project.not_null_user_activity_influence_score.758852d325)
[0m08:42:30.400439 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m08:42:30.406567 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m08:42:30.415832 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (compile): 08:42:30.401013 => 08:42:30.415447
[0m08:42:30.416692 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m08:42:30.429307 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m08:42:30.438857 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:30.439566 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m08:42:30.440156 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select influence_score
from `workspace`.`movielens_volume`.`user_activity`
where influence_score is null



      
    ) dbt_internal_test
[0m08:42:30.440765 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:31.132853 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m08:42:31.137277 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (execute): 08:42:30.417163 => 08:42:31.136958
[0m08:42:31.138063 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: ROLLBACK
[0m08:42:31.138785 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:31.139425 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: Close
[0m08:42:31.241403 [info ] [Thread-2  ]: 25 of 35 PASS not_null_user_activity_influence_score ........................... [[32mPASS[0m in 0.84s]
[0m08:42:31.242703 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m08:42:31.243437 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m08:42:31.244087 [info ] [Thread-2  ]: 26 of 35 START test not_null_user_activity_last_activity_date .................. [RUN]
[0m08:42:31.245220 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_influence_score.758852d325, now test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad)
[0m08:42:31.245787 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m08:42:31.254106 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m08:42:31.263462 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (compile): 08:42:31.246240 => 08:42:31.263056
[0m08:42:31.264192 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m08:42:31.268595 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m08:42:31.278048 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:31.278778 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m08:42:31.279473 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where last_activity_date is null



      
    ) dbt_internal_test
[0m08:42:31.280214 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:31.964046 [debug] [Thread-2  ]: SQL status: OK in 0.6800000071525574 seconds
[0m08:42:31.968130 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (execute): 08:42:31.264651 => 08:42:31.967759
[0m08:42:31.968946 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: ROLLBACK
[0m08:42:31.969753 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:31.970401 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: Close
[0m08:42:32.087733 [info ] [Thread-2  ]: 26 of 35 PASS not_null_user_activity_last_activity_date ........................ [[32mPASS[0m in 0.84s]
[0m08:42:32.089134 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m08:42:32.089900 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m08:42:32.090635 [info ] [Thread-2  ]: 27 of 35 START test not_null_user_activity_top_genre ........................... [RUN]
[0m08:42:32.091934 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad, now test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313)
[0m08:42:32.092576 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m08:42:32.101473 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m08:42:32.110614 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (compile): 08:42:32.093053 => 08:42:32.110175
[0m08:42:32.111356 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m08:42:32.115267 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m08:42:32.124319 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:32.124923 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m08:42:32.125449 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_genre
from `workspace`.`movielens_volume`.`user_activity`
where top_genre is null



      
    ) dbt_internal_test
[0m08:42:32.126028 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:32.886585 [debug] [Thread-2  ]: SQL status: OK in 0.7599999904632568 seconds
[0m08:42:32.890585 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (execute): 08:42:32.111818 => 08:42:32.890261
[0m08:42:32.891397 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: ROLLBACK
[0m08:42:32.892088 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:32.892754 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: Close
[0m08:42:33.002047 [info ] [Thread-2  ]: 27 of 35 PASS not_null_user_activity_top_genre ................................. [[32mPASS[0m in 0.91s]
[0m08:42:33.003381 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m08:42:33.004338 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m08:42:33.005230 [info ] [Thread-2  ]: 28 of 35 START test not_null_user_activity_user_id ............................. [RUN]
[0m08:42:33.006587 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313, now test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36)
[0m08:42:33.007380 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m08:42:33.015978 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m08:42:33.025337 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (compile): 08:42:33.007937 => 08:42:33.024967
[0m08:42:33.026038 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m08:42:33.031370 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m08:42:33.040133 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:33.040732 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m08:42:33.041272 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`user_activity`
where user_id is null



      
    ) dbt_internal_test
[0m08:42:33.042176 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:33.782822 [debug] [Thread-2  ]: SQL status: OK in 0.7400000095367432 seconds
[0m08:42:33.786835 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (execute): 08:42:33.026529 => 08:42:33.786514
[0m08:42:33.787895 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: ROLLBACK
[0m08:42:33.788865 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:33.789588 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: Close
[0m08:42:33.894879 [info ] [Thread-2  ]: 28 of 35 PASS not_null_user_activity_user_id ................................... [[32mPASS[0m in 0.89s]
[0m08:42:33.896375 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m08:42:33.897177 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m08:42:33.898004 [info ] [Thread-2  ]: 29 of 35 START test not_null_user_activity_user_rating_count ................... [RUN]
[0m08:42:33.899504 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36, now test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e)
[0m08:42:33.900164 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m08:42:33.908911 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m08:42:33.919098 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (compile): 08:42:33.900650 => 08:42:33.918396
[0m08:42:33.919877 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m08:42:33.923744 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m08:42:33.933252 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:33.933926 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m08:42:33.934472 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_rating_count
from `workspace`.`movielens_volume`.`user_activity`
where user_rating_count is null



      
    ) dbt_internal_test
[0m08:42:33.935034 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:34.648058 [debug] [Thread-2  ]: SQL status: OK in 0.7099999785423279 seconds
[0m08:42:34.652224 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (execute): 08:42:33.920383 => 08:42:34.651889
[0m08:42:34.652963 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: ROLLBACK
[0m08:42:34.653673 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:34.654395 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: Close
[0m08:42:34.761124 [info ] [Thread-2  ]: 29 of 35 PASS not_null_user_activity_user_rating_count ......................... [[32mPASS[0m in 0.86s]
[0m08:42:34.762565 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m08:42:34.763360 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:42:34.764222 [info ] [Thread-2  ]: 30 of 35 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:42:34.765674 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m08:42:34.766331 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:42:34.779236 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:42:34.789763 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 08:42:34.766788 => 08:42:34.789330
[0m08:42:34.790933 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:42:34.796262 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:42:34.806842 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:34.807567 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:42:34.808179 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:42:34.808983 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:35.341307 [debug] [Thread-2  ]: SQL status: OK in 0.5299999713897705 seconds
[0m08:42:35.345723 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 08:42:34.791861 => 08:42:35.345313
[0m08:42:35.346465 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m08:42:35.347251 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:35.347874 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m08:42:35.461120 [info ] [Thread-2  ]: 30 of 35 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.70s]
[0m08:42:35.462500 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:42:35.463371 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:42:35.464171 [info ] [Thread-2  ]: 31 of 35 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:42:35.465382 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m08:42:35.466018 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:42:35.473222 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:42:35.486605 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 08:42:35.466466 => 08:42:35.486294
[0m08:42:35.487227 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:42:35.491218 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:42:35.499971 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:35.500576 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:42:35.501239 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:42:35.501939 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:36.286352 [debug] [Thread-2  ]: SQL status: OK in 0.7799999713897705 seconds
[0m08:42:36.290884 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 08:42:35.487841 => 08:42:36.290505
[0m08:42:36.291715 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m08:42:36.292492 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:36.293165 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m08:42:36.433427 [info ] [Thread-2  ]: 31 of 35 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.97s]
[0m08:42:36.434773 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:42:36.435580 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m08:42:36.436345 [info ] [Thread-2  ]: 32 of 35 START test unique_genre_popularity_genres ............................. [RUN]
[0m08:42:36.437710 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075)
[0m08:42:36.438518 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m08:42:36.447903 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m08:42:36.457220 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (compile): 08:42:36.439019 => 08:42:36.456750
[0m08:42:36.457917 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m08:42:36.461768 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m08:42:36.470737 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:36.471333 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m08:42:36.471947 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    genres as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`genre_popularity`
where genres is not null
group by genres
having count(*) > 1



      
    ) dbt_internal_test
[0m08:42:36.472569 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:37.163536 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m08:42:37.167367 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (execute): 08:42:36.458454 => 08:42:37.167066
[0m08:42:37.168134 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: ROLLBACK
[0m08:42:37.169094 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:37.169803 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: Close
[0m08:42:37.271848 [info ] [Thread-2  ]: 32 of 35 PASS unique_genre_popularity_genres ................................... [[32mPASS[0m in 0.83s]
[0m08:42:37.273268 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m08:42:37.274186 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:42:37.274938 [info ] [Thread-2  ]: 33 of 35 START test unique_movie_enriched_movie_id ............................. [RUN]
[0m08:42:37.276090 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075, now test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a)
[0m08:42:37.276789 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:42:37.282916 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m08:42:37.292013 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (compile): 08:42:37.277346 => 08:42:37.291557
[0m08:42:37.292700 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:42:37.296978 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m08:42:37.306531 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:37.307340 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m08:42:37.308074 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:42:37.308740 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:38.025818 [debug] [Thread-2  ]: SQL status: OK in 0.7200000286102295 seconds
[0m08:42:38.030425 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (execute): 08:42:37.293170 => 08:42:38.030078
[0m08:42:38.031205 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: ROLLBACK
[0m08:42:38.031900 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:38.032539 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m08:42:38.134376 [info ] [Thread-2  ]: 33 of 35 PASS unique_movie_enriched_movie_id ................................... [[32mPASS[0m in 0.86s]
[0m08:42:38.135822 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:42:38.136604 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m08:42:38.137364 [info ] [Thread-2  ]: 34 of 35 START test unique_movie_performance_movie_id .......................... [RUN]
[0m08:42:38.138620 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a, now test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22)
[0m08:42:38.139247 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m08:42:38.145515 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m08:42:38.156066 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (compile): 08:42:38.139933 => 08:42:38.155594
[0m08:42:38.157100 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m08:42:38.161155 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m08:42:38.170608 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:38.177885 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m08:42:38.178832 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:42:38.179733 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:38.870383 [debug] [Thread-2  ]: SQL status: OK in 0.6899999976158142 seconds
[0m08:42:38.880627 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (execute): 08:42:38.157587 => 08:42:38.879182
[0m08:42:38.881380 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: ROLLBACK
[0m08:42:38.882251 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:38.882880 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: Close
[0m08:42:38.983765 [info ] [Thread-2  ]: 34 of 35 PASS unique_movie_performance_movie_id ................................ [[32mPASS[0m in 0.85s]
[0m08:42:38.985078 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m08:42:38.985883 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m08:42:38.986908 [info ] [Thread-2  ]: 35 of 35 START test unique_user_activity_user_id ............................... [RUN]
[0m08:42:38.988189 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22, now test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda)
[0m08:42:38.988951 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m08:42:38.994728 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m08:42:39.004370 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (compile): 08:42:38.989510 => 08:42:39.003958
[0m08:42:39.005069 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m08:42:39.009347 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m08:42:39.028783 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:39.029434 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m08:42:39.029993 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`user_activity`
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:42:39.030581 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:42:39.846604 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m08:42:39.850796 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (execute): 08:42:39.005538 => 08:42:39.850457
[0m08:42:39.851537 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: ROLLBACK
[0m08:42:39.852234 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:42:39.852848 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: Close
[0m08:42:39.957842 [info ] [Thread-2  ]: 35 of 35 PASS unique_user_activity_user_id ..................................... [[32mPASS[0m in 0.97s]
[0m08:42:39.959259 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m08:42:39.961906 [debug] [MainThread]: On master: ROLLBACK
[0m08:42:39.962455 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:42:40.294557 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:42:40.295458 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:40.296048 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:42:40.296722 [debug] [MainThread]: On master: ROLLBACK
[0m08:42:40.297279 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:42:40.297817 [debug] [MainThread]: On master: Close
[0m08:42:40.407472 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:42:40.408272 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda' was properly closed.
[0m08:42:40.410667 [info ] [MainThread]: 
[0m08:42:40.412157 [info ] [MainThread]: Finished running 35 tests in 0 hours 0 minutes and 34.44 seconds (34.44s).
[0m08:42:40.419516 [debug] [MainThread]: Command end result
[0m08:42:40.446017 [info ] [MainThread]: 
[0m08:42:40.446890 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:42:40.447678 [info ] [MainThread]: 
[0m08:42:40.448405 [info ] [MainThread]: Done. PASS=35 WARN=0 ERROR=0 SKIP=0 TOTAL=35
[0m08:42:40.449438 [debug] [MainThread]: Command `dbt test` succeeded at 08:42:40.449308 after 35.26 seconds
[0m08:42:40.450069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e4bef6640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e1dd80fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e1f07f4f0>]}
[0m08:42:40.450656 [debug] [MainThread]: Flushing usage events
[0m10:28:25.385880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4849d43550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48482b01c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48482b08e0>]}


============================== 10:28:25.400137 | 9ca08ee6-3b98-421f-8ede-b9c3031031ee ==============================
[0m10:28:25.400137 [info ] [MainThread]: Running with dbt=1.5.2
[0m10:28:25.401592 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/opt/airflow/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m10:28:27.124955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9ca08ee6-3b98-421f-8ede-b9c3031031ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48482b0160>]}
[0m10:28:27.170084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9ca08ee6-3b98-421f-8ede-b9c3031031ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4820ffb790>]}
[0m10:28:27.171631 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m10:28:27.252937 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m10:28:30.158131 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:28:30.159220 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:28:30.173395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9ca08ee6-3b98-421f-8ede-b9c3031031ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4820fa43a0>]}
[0m10:28:30.220863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9ca08ee6-3b98-421f-8ede-b9c3031031ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4820ed13d0>]}
[0m10:28:30.222008 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m10:28:30.222993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ca08ee6-3b98-421f-8ede-b9c3031031ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4820ed1430>]}
[0m10:28:30.227058 [info ] [MainThread]: 
[0m10:28:30.229507 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:28:30.232257 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m10:28:30.233276 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m10:28:30.234322 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m10:28:30.235742 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:28:30.193147 [debug] [ThreadPool]: SQL status: OK in -0.03999999910593033 seconds
[0m10:28:30.196458 [debug] [ThreadPool]: On list_workspace: Close
[0m10:28:30.506552 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m10:28:30.509101 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m10:28:30.528249 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:28:30.529617 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m10:28:30.531215 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m10:28:30.532375 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:28:32.355598 [debug] [ThreadPool]: SQL status: OK in 1.8200000524520874 seconds
[0m10:28:32.357482 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m10:28:32.358401 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m10:28:32.359357 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m10:28:32.360178 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m10:28:32.561007 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m10:28:32.574993 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m10:28:32.576974 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m10:28:32.578077 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:28:33.907640 [debug] [ThreadPool]: SQL status: OK in 1.3300000429153442 seconds
[0m10:28:33.911089 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m10:28:34.079708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ca08ee6-3b98-421f-8ede-b9c3031031ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f484b8dcd90>]}
[0m10:28:34.080876 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:28:34.081579 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:28:34.082991 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:28:34.084199 [info ] [MainThread]: 
[0m10:28:34.103182 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_links
[0m10:28:34.104302 [info ] [Thread-4  ]: 1 of 4 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m10:28:34.106078 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.silver_links)
[0m10:28:34.107086 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_links
[0m10:28:34.123086 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m10:28:34.134884 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (compile): 10:28:34.107632 => 10:28:34.134453
[0m10:28:34.135737 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_links
[0m10:28:34.161316 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:28:34.162217 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m10:28:34.162895 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

      describe extended `workspace`.`movielens_volume`.`silver_links`
  
[0m10:28:34.163801 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m10:28:37.370513 [debug] [Thread-4  ]: SQL status: OK in 3.2100000381469727 seconds
[0m10:28:37.464561 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m10:28:37.475855 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m10:28:37.476960 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
    using delta
      
      
      
      
      
      
      as
      

with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m10:28:46.837319 [debug] [Thread-4  ]: SQL status: OK in 9.359999656677246 seconds
[0m10:28:47.164136 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (execute): 10:28:34.136279 => 10:28:47.163727
[0m10:28:47.165892 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: ROLLBACK
[0m10:28:47.167631 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m10:28:47.168868 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: Close
[0m10:28:47.410827 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9ca08ee6-3b98-421f-8ede-b9c3031031ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f482033bf40>]}
[0m10:28:47.412652 [info ] [Thread-4  ]: 1 of 4 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 13.31s]
[0m10:28:47.414985 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_links
[0m10:28:47.416580 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_movies
[0m10:28:47.418107 [info ] [Thread-4  ]: 2 of 4 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m10:28:47.420324 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_links, now model.my_dbt_project.silver_movies)
[0m10:28:47.421711 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_movies
[0m10:28:47.432531 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m10:28:47.452350 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (compile): 10:28:47.423089 => 10:28:47.451631
[0m10:28:47.453796 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_movies
[0m10:28:47.473577 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:28:47.475498 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m10:28:47.476707 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

      describe extended `workspace`.`movielens_volume`.`silver_movies`
  
[0m10:28:47.478117 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m10:28:49.619065 [debug] [Thread-4  ]: SQL status: OK in 2.140000104904175 seconds
[0m10:28:49.630655 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m10:28:49.649861 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m10:28:49.651131 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m10:28:54.504172 [debug] [Thread-4  ]: SQL status: OK in 4.849999904632568 seconds
[0m10:28:54.510636 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (execute): 10:28:47.454747 => 10:28:54.510183
[0m10:28:54.512017 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: ROLLBACK
[0m10:28:54.513438 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m10:28:54.514620 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: Close
[0m10:28:54.668627 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9ca08ee6-3b98-421f-8ede-b9c3031031ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f482035d580>]}
[0m10:28:54.670794 [info ] [Thread-4  ]: 2 of 4 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 7.25s]
[0m10:28:54.673151 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_movies
[0m10:28:54.674933 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_ratings
[0m10:28:54.676665 [info ] [Thread-4  ]: 3 of 4 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m10:28:54.679496 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_movies, now model.my_dbt_project.silver_ratings)
[0m10:28:54.681224 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_ratings
[0m10:28:54.693159 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m10:28:54.713818 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (compile): 10:28:54.682768 => 10:28:54.713124
[0m10:28:54.715134 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_ratings
[0m10:28:54.728870 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:28:54.730213 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m10:28:54.731222 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

      describe extended `workspace`.`movielens_volume`.`silver_ratings`
  
[0m10:28:54.732270 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m10:28:56.322176 [debug] [Thread-4  ]: SQL status: OK in 1.590000033378601 seconds
[0m10:28:56.333934 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m10:28:56.350796 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m10:28:56.352463 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m10:29:01.385243 [debug] [Thread-4  ]: SQL status: OK in 5.03000020980835 seconds
[0m10:29:01.581734 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (execute): 10:28:54.716046 => 10:29:01.581331
[0m10:29:01.582995 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: ROLLBACK
[0m10:29:01.584284 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m10:29:01.585501 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: Close
[0m10:29:01.760598 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9ca08ee6-3b98-421f-8ede-b9c3031031ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f482036a490>]}
[0m10:29:01.762697 [info ] [Thread-4  ]: 3 of 4 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 7.08s]
[0m10:29:01.764737 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_ratings
[0m10:29:01.766338 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_tags
[0m10:29:01.767918 [info ] [Thread-4  ]: 4 of 4 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m10:29:01.770025 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_ratings, now model.my_dbt_project.silver_tags)
[0m10:29:01.771240 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_tags
[0m10:29:01.790535 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m10:29:01.806241 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (compile): 10:29:01.772113 => 10:29:01.805444
[0m10:29:01.807699 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_tags
[0m10:29:01.820691 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:29:01.822301 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m10:29:01.823461 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

      describe extended `workspace`.`movielens_volume`.`silver_tags`
  
[0m10:29:01.824623 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m10:29:03.615947 [debug] [Thread-4  ]: SQL status: OK in 1.7899999618530273 seconds
[0m10:29:03.625969 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m10:29:03.643363 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m10:29:03.644470 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m10:29:08.128437 [debug] [Thread-4  ]: SQL status: OK in 4.480000019073486 seconds
[0m10:29:08.136072 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (execute): 10:29:01.808577 => 10:29:08.135559
[0m10:29:08.137306 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: ROLLBACK
[0m10:29:08.138416 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m10:29:08.139697 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: Close
[0m10:29:08.297350 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9ca08ee6-3b98-421f-8ede-b9c3031031ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f482042e5e0>]}
[0m10:29:08.299330 [info ] [Thread-4  ]: 4 of 4 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 6.53s]
[0m10:29:08.301956 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_tags
[0m10:29:08.306661 [debug] [MainThread]: On master: ROLLBACK
[0m10:29:08.307877 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:29:08.892557 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:29:08.893760 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:29:08.894827 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:29:08.895776 [debug] [MainThread]: On master: ROLLBACK
[0m10:29:08.896640 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:29:08.897932 [debug] [MainThread]: On master: Close
[0m10:29:09.047560 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:29:09.049280 [debug] [MainThread]: Connection 'model.my_dbt_project.silver_tags' was properly closed.
[0m10:29:09.066494 [info ] [MainThread]: 
[0m10:29:09.087531 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 38.84 seconds (38.84s).
[0m10:29:09.092695 [debug] [MainThread]: Command end result
[0m10:29:09.199200 [info ] [MainThread]: 
[0m10:29:09.201851 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:29:09.205657 [info ] [MainThread]: 
[0m10:29:09.208101 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m10:29:09.210849 [debug] [MainThread]: Command `dbt run` succeeded at 10:29:09.210400 after 47.21 seconds
[0m10:29:09.213007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4849d43550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4820d5c3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4820dcfc10>]}
[0m10:29:09.214891 [debug] [MainThread]: Flushing usage events
[0m10:29:16.060741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f882da36640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f882bfaf1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f882bfaf940>]}


============================== 10:29:16.070421 | a161598c-c19e-424d-98cb-7a27223961bf ==============================
[0m10:29:16.070421 [info ] [MainThread]: Running with dbt=1.5.2
[0m10:29:16.071723 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/opt/airflow/dbt', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m10:29:17.432456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a161598c-c19e-424d-98cb-7a27223961bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f882bfaf190>]}
[0m10:29:17.472393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a161598c-c19e-424d-98cb-7a27223961bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8804ce0b80>]}
[0m10:29:17.473806 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m10:29:17.537349 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m10:29:19.228176 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:29:19.229624 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:29:19.246897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a161598c-c19e-424d-98cb-7a27223961bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8804a48400>]}
[0m10:29:19.301132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a161598c-c19e-424d-98cb-7a27223961bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8804bb2430>]}
[0m10:29:19.302520 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m10:29:19.304066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a161598c-c19e-424d-98cb-7a27223961bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8804bb2490>]}
[0m10:29:19.308358 [info ] [MainThread]: 
[0m10:29:19.310392 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:29:19.313977 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m10:29:19.315179 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m10:29:19.316228 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m10:29:19.317289 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:29:20.516003 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m10:29:20.519019 [debug] [ThreadPool]: On list_workspace: Close
[0m10:29:20.668036 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m10:29:20.671696 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m10:29:20.751613 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:29:20.753302 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m10:29:20.754674 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m10:29:20.755824 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:29:21.817393 [debug] [ThreadPool]: SQL status: OK in 1.059999942779541 seconds
[0m10:29:21.820224 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m10:29:21.821692 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m10:29:21.823390 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m10:29:21.824432 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m10:29:21.981417 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m10:29:21.997074 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m10:29:21.998353 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m10:29:21.999360 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:29:23.032805 [debug] [ThreadPool]: SQL status: OK in 1.0299999713897705 seconds
[0m10:29:23.037821 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m10:29:23.195228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a161598c-c19e-424d-98cb-7a27223961bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8804bb2160>]}
[0m10:29:23.197165 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:29:23.198324 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:29:23.204977 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:29:23.206613 [info ] [MainThread]: 
[0m10:29:23.257401 [debug] [Thread-4  ]: Began running node model.my_dbt_project.genre_popularity
[0m10:29:23.259274 [info ] [Thread-4  ]: 1 of 5 START sql incremental model movielens_volume.genre_popularity ........... [RUN]
[0m10:29:23.262047 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.genre_popularity)
[0m10:29:23.263188 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.genre_popularity
[0m10:29:23.300066 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.genre_popularity"
[0m10:29:23.315900 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (compile): 10:29:23.264016 => 10:29:23.315259
[0m10:29:23.317419 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.genre_popularity
[0m10:29:23.404015 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:29:23.405679 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:29:23.406918 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m10:29:23.408296 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m10:29:24.933171 [debug] [Thread-4  ]: SQL status: OK in 1.5199999809265137 seconds
[0m10:29:24.984259 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:29:24.985783 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

  
    create temporary view `genre_popularity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings with Genres
-- Join silver ratings with silver movies to attach genres
-- Prepare rating timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        r.rating,
        r.rating_date as rating_timestamp,  
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Aggregate Genre Metrics
-- For each genre, calculate total ratings, avg rating,
-- and distribution insights
-- =======================================================
genre_stats as (
    select
        genres,  -- genres already formatted with spacing
        count(*) as rating_count,
        round(avg(rating), 2) as avg_rating,  
        to_date(min(rating_timestamp)) as first_rating_date,  
        to_date(max(rating_timestamp)) as last_rating_date   
    from ratings_base
    group by genres
),

-- =======================================================
-- Step 3: Ranking Genres
-- Rank genres by popularity (# of ratings)
-- and quality (avg rating)
-- =======================================================
ranked_genres as (
    select
        genres,
        rating_count,
        avg_rating,
        first_rating_date,
        last_rating_date,
        rank() over (order by rating_count desc) as popularity_rank,
        rank() over (order by avg_rating desc) as quality_rank
    from genre_stats
)

-- =======================================================
-- Step 4: Final Selection
-- One row per genre with all metrics
-- =======================================================
select
    genres,
    rating_count,
    avg_rating,
    popularity_rank,
    quality_rank,
    first_rating_date,
    last_rating_date
from ranked_genres


  -- Only insert new genres on incremental runs
  where genres not in (select genres from `workspace`.`movielens_volume`.`genre_popularity`)

  
[0m10:29:25.861702 [debug] [Thread-4  ]: SQL status: OK in 0.8700000047683716 seconds
[0m10:29:25.926822 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:29:25.929632 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */

      describe extended `workspace`.`movielens_volume`.`genre_popularity`
  
[0m10:29:26.448143 [debug] [Thread-4  ]: SQL status: OK in 0.5199999809265137 seconds
[0m10:29:26.468351 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.genre_popularity"
[0m10:29:26.484796 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.genre_popularity"
[0m10:29:26.486122 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.genre_popularity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`genre_popularity` as DBT_INTERNAL_DEST
      using `genre_popularity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.genres = DBT_INTERNAL_DEST.genres
          

      when matched then update set
         * 

      when not matched then insert *

[0m10:29:34.240434 [debug] [Thread-4  ]: SQL status: OK in 7.75 seconds
[0m10:29:34.478121 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.genre_popularity (execute): 10:29:23.318349 => 10:29:34.477564
[0m10:29:34.479686 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: ROLLBACK
[0m10:29:34.480973 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m10:29:34.482134 [debug] [Thread-4  ]: On model.my_dbt_project.genre_popularity: Close
[0m10:29:34.638087 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a161598c-c19e-424d-98cb-7a27223961bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f88049d6ee0>]}
[0m10:29:34.640364 [info ] [Thread-4  ]: 1 of 5 OK created sql incremental model movielens_volume.genre_popularity ...... [[32mOK[0m in 11.38s]
[0m10:29:34.642400 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.genre_popularity
[0m10:29:34.643986 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_enriched
[0m10:29:34.645563 [info ] [Thread-4  ]: 2 of 5 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m10:29:34.647865 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.genre_popularity, now model.my_dbt_project.movie_enriched)
[0m10:29:34.648959 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_enriched
[0m10:29:34.659954 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m10:29:34.679308 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (compile): 10:29:34.650083 => 10:29:34.678591
[0m10:29:34.680549 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_enriched
[0m10:29:34.709972 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:29:34.711331 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m10:29:34.712398 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

      describe extended `workspace`.`movielens_volume`.`movie_enriched`
  
[0m10:29:34.713779 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m10:29:36.175690 [debug] [Thread-4  ]: SQL status: OK in 1.4600000381469727 seconds
[0m10:29:36.267719 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m10:29:36.287121 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m10:29:36.288975 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m10:29:39.802979 [debug] [Thread-4  ]: SQL status: OK in 3.509999990463257 seconds
[0m10:29:39.817003 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (execute): 10:29:34.681439 => 10:29:39.816502
[0m10:29:39.818531 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: ROLLBACK
[0m10:29:39.820446 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m10:29:39.821627 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: Close
[0m10:29:39.981819 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a161598c-c19e-424d-98cb-7a27223961bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f88041597f0>]}
[0m10:29:39.983678 [info ] [Thread-4  ]: 2 of 5 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 5.33s]
[0m10:29:39.986066 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_enriched
[0m10:29:39.987470 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_performance
[0m10:29:39.989056 [info ] [Thread-4  ]: 3 of 5 START sql incremental model movielens_volume.movie_performance .......... [RUN]
[0m10:29:39.991190 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_enriched, now model.my_dbt_project.movie_performance)
[0m10:29:39.992736 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_performance
[0m10:29:40.010380 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_performance"
[0m10:29:40.055920 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (compile): 10:29:39.993828 => 10:29:40.054831
[0m10:29:40.057951 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_performance
[0m10:29:40.094927 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:29:40.099552 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:29:40.101394 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m10:29:40.103119 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m10:29:41.795742 [debug] [Thread-4  ]: SQL status: OK in 1.690000057220459 seconds
[0m10:29:41.806378 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:29:41.807949 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

  
    create temporary view `movie_performance__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings Join
-- Attach movie metadata (title, genres) to ratings
-- =======================================================
with ratings_base as (
    select
        r.movie_id,
        m.title,
        regexp_replace(m.genres, '\\|', ' | ') as genres,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_date
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: Weighted Rating
-- Compute avg rating, rating count, weighted avg
-- =======================================================
weighted_rating as (
    select
        movie_id,
        title,
        genres,
        round(avg(rating), 2) as avg_rating,
        count(*) as rating_count,
        round(sum(rating) / count(*), 2) as weighted_avg_rating
    from ratings_base
    group by movie_id, title, genres
),

-- =======================================================
-- Step 3: Polarization Score
-- Standard deviation of ratings per movie
-- =======================================================
polarization as (
    select
        movie_id,
        round(coalesce(stddev(rating), 0.0), 2) as polarization_score
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 4: Top Rating Percentage
-- Percent of ratings >= 4.5
-- =======================================================
top_rating_pct as (
    select
        movie_id,
        round(100.0 * sum(case when rating >= 4.5 then 1 else 0 end) / count(*), 2) as top_rating_percentage
    from ratings_base
    group by movie_id
),

-- =======================================================
-- Step 5: Genre Ranking
-- Rank movies within each genre by weighted_avg_rating
-- =======================================================
genre_rank as (
    select
        movie_id,
        genres,
        weighted_avg_rating,
        rank() over (partition by genres order by weighted_avg_rating desc) as genre_rank
    from weighted_rating
),

-- =======================================================
-- Step 6: Tag Count
-- Count distinct tags per movie
-- =======================================================
tag_count as (
    select
        movie_id,
        count(distinct tag) as tag_count
    from `workspace`.`movielens_volume`.`silver_tags`
    group by movie_id
)

-- =======================================================
-- Step 7: Final Selection
-- Combine all metrics into the gold table
-- =======================================================
select
    w.movie_id,
    w.title,
    w.genres,
    w.weighted_avg_rating,
    w.rating_count,
    g.genre_rank,
    p.polarization_score,
    tr.top_rating_percentage,
    coalesce(tc.tag_count, 0) as tag_count
from weighted_rating w
join genre_rank g
    on w.movie_id = g.movie_id
join polarization p
    on w.movie_id = p.movie_id
join top_rating_pct tr
    on w.movie_id = tr.movie_id
left join tag_count tc
    on w.movie_id = tc.movie_id


-- Use NOT EXISTS for safer incremental inserts
where not exists (
    select 1 
    from `workspace`.`movielens_volume`.`movie_performance` t
    where t.movie_id = w.movie_id
)

  
[0m10:29:42.794157 [debug] [Thread-4  ]: SQL status: OK in 0.9800000190734863 seconds
[0m10:29:42.806800 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:29:42.808702 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */

      describe extended `workspace`.`movielens_volume`.`movie_performance`
  
[0m10:29:43.463143 [debug] [Thread-4  ]: SQL status: OK in 0.6499999761581421 seconds
[0m10:29:43.471578 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_performance"
[0m10:29:43.489060 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_performance"
[0m10:29:43.491484 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_performance"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`movie_performance` as DBT_INTERNAL_DEST
      using `movie_performance__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.movie_id = DBT_INTERNAL_DEST.movie_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m10:29:50.731358 [debug] [Thread-4  ]: SQL status: OK in 7.239999771118164 seconds
[0m10:29:50.929353 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_performance (execute): 10:29:40.059192 => 10:29:50.928697
[0m10:29:50.931233 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: ROLLBACK
[0m10:29:50.933253 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m10:29:50.934890 [debug] [Thread-4  ]: On model.my_dbt_project.movie_performance: Close
[0m10:29:51.110836 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a161598c-c19e-424d-98cb-7a27223961bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f88041598e0>]}
[0m10:29:51.113371 [info ] [Thread-4  ]: 3 of 5 OK created sql incremental model movielens_volume.movie_performance ..... [[32mOK[0m in 11.12s]
[0m10:29:51.116474 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_performance
[0m10:29:51.118603 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_activity
[0m10:29:51.120770 [info ] [Thread-4  ]: 4 of 5 START sql incremental model movielens_volume.user_activity .............. [RUN]
[0m10:29:51.123082 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_performance, now model.my_dbt_project.user_activity)
[0m10:29:51.124448 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_activity
[0m10:29:51.141819 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_activity"
[0m10:29:51.157906 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (compile): 10:29:51.125383 => 10:29:51.157177
[0m10:29:51.159450 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_activity
[0m10:29:51.171211 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:29:51.172436 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:29:51.173535 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m10:29:51.174961 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m10:29:52.891329 [debug] [Thread-4  ]: SQL status: OK in 1.7200000286102295 seconds
[0m10:29:52.901701 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:29:52.903829 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

  
    create temporary view `user_activity__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Join ratings with movies to attach genres and prepare timestamps
-- =======================================================
with ratings_base as (
    select
        r.user_id,
        r.movie_id,
        round(r.rating, 2) as rating,
        cast(r.rating_date as date) as rating_timestamp,
        regexp_replace(m.genres, '\\|', ' | ') as genres
    from `workspace`.`movielens_volume`.`silver_ratings` r
    join `workspace`.`movielens_volume`.`silver_movies` m
        on r.movie_id = m.movie_id
),

-- =======================================================
-- Step 2: User Activity (General)
-- Calculate total ratings, average rating, first and last activity dates per user
-- =======================================================
user_activity as (
    select
        user_id,
        count(*) as user_rating_count,
        round(avg(rating), 2) as avg_user_rating,
        min(rating_timestamp) as first_activity_date,
        max(rating_timestamp) as last_activity_date
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 3: User Influence Score
-- Compute influence based on total ratings (more ratings = higher influence)
-- =======================================================
user_influence as (
    select
        user_id,
        user_rating_count as influence_score
    from user_activity
),

-- =======================================================
-- Step 4: Active Users by Genre
-- Count ratings per genre and rank genres per user
-- =======================================================
user_genre_activity as (
    select
        user_id,
        regexp_replace(genres, '\\|', ' | ') as genres,
        count(*) as genre_rating_count,
        rank() over (partition by user_id order by count(*) desc) as genre_rank
    from ratings_base
    group by user_id, genres
),

-- =======================================================
-- Step 5: New vs Loyal Users
-- Categorize users as new if first activity < 30 days ago, else loyal
-- =======================================================
user_type as (
    select
        user_id,
        case
            when min(rating_timestamp) >= date_sub(current_date(), 30)
                then 'new_user'
            else 'loyal_user'
        end as user_category
    from ratings_base
    group by user_id
),

-- =======================================================
-- Step 6: Top Genre per User
-- Identify the most active genre for each user
-- =======================================================
top_genre as (
    select
        user_id,
        max(genres) as top_genre
    from user_genre_activity
    group by user_id
)

-- =======================================================
-- Final Selection
-- Combine general activity, influence, type, and top genre per user
-- =======================================================
select
    ua.user_id,
    ua.user_rating_count,
    ua.avg_user_rating,
    ua.first_activity_date,
    ua.last_activity_date,
    ui.influence_score,
    ut.user_category,
    tg.top_genre
from user_activity ua
join user_influence ui on ua.user_id = ui.user_id
join user_type ut on ua.user_id = ut.user_id
left join top_genre tg on ua.user_id = tg.user_id


  where ua.user_id not in (select user_id from `workspace`.`movielens_volume`.`user_activity`)

  
[0m10:29:53.857981 [debug] [Thread-4  ]: SQL status: OK in 0.949999988079071 seconds
[0m10:29:53.879300 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:29:53.880736 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */

      describe extended `workspace`.`movielens_volume`.`user_activity`
  
[0m10:29:54.224104 [debug] [Thread-4  ]: SQL status: OK in 0.3400000035762787 seconds
[0m10:29:54.232107 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_activity"
[0m10:29:54.252234 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_activity"
[0m10:29:54.253450 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_activity"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_activity` as DBT_INTERNAL_DEST
      using `user_activity__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m10:30:01.193311 [debug] [Thread-4  ]: SQL status: OK in 6.940000057220459 seconds
[0m10:30:01.373327 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_activity (execute): 10:29:51.160400 => 10:30:01.372889
[0m10:30:01.374870 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: ROLLBACK
[0m10:30:01.376375 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:01.377427 [debug] [Thread-4  ]: On model.my_dbt_project.user_activity: Close
[0m10:30:01.527298 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a161598c-c19e-424d-98cb-7a27223961bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f880408bfd0>]}
[0m10:30:01.529515 [info ] [Thread-4  ]: 4 of 5 OK created sql incremental model movielens_volume.user_activity ......... [[32mOK[0m in 10.40s]
[0m10:30:01.531967 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_activity
[0m10:30:01.533511 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_preferences
[0m10:30:01.534853 [info ] [Thread-4  ]: 5 of 5 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m10:30:01.536747 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.user_activity, now model.my_dbt_project.user_preferences)
[0m10:30:01.538292 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_preferences
[0m10:30:01.550279 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m10:30:01.568894 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (compile): 10:30:01.539170 => 10:30:01.568308
[0m10:30:01.570290 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_preferences
[0m10:30:01.583010 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:01.584180 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:30:01.585072 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m10:30:01.586276 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m10:30:03.167686 [debug] [Thread-4  ]: SQL status: OK in 1.5800000429153442 seconds
[0m10:30:03.178065 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:30:03.180287 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create temporary view `user_preferences__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score,  -- influence based on total ratings
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )

  
[0m10:30:03.900184 [debug] [Thread-4  ]: SQL status: OK in 0.7200000286102295 seconds
[0m10:30:03.909592 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:30:03.910752 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m10:30:04.330148 [debug] [Thread-4  ]: SQL status: OK in 0.41999998688697815 seconds
[0m10:30:04.338286 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m10:30:04.355041 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m10:30:04.356102 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
      using `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m10:30:12.930751 [debug] [Thread-4  ]: SQL status: OK in 8.569999694824219 seconds
[0m10:30:13.137603 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (execute): 10:30:01.571344 => 10:30:13.137083
[0m10:30:13.139440 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: ROLLBACK
[0m10:30:13.141159 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:13.142788 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: Close
[0m10:30:13.302523 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a161598c-c19e-424d-98cb-7a27223961bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8804183cd0>]}
[0m10:30:13.305010 [info ] [Thread-4  ]: 5 of 5 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 11.77s]
[0m10:30:13.307313 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_preferences
[0m10:30:13.312665 [debug] [MainThread]: On master: ROLLBACK
[0m10:30:13.313906 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:30:13.932720 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:30:13.934205 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:13.935327 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:30:13.936770 [debug] [MainThread]: On master: ROLLBACK
[0m10:30:13.937886 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:30:13.938878 [debug] [MainThread]: On master: Close
[0m10:30:14.118358 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:30:14.119705 [debug] [MainThread]: Connection 'model.my_dbt_project.user_preferences' was properly closed.
[0m10:30:14.124961 [info ] [MainThread]: 
[0m10:30:14.128252 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 54.81 seconds (54.81s).
[0m10:30:14.132347 [debug] [MainThread]: Command end result
[0m10:30:14.186178 [info ] [MainThread]: 
[0m10:30:14.188039 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:30:14.189815 [info ] [MainThread]: 
[0m10:30:14.191412 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m10:30:14.193614 [debug] [MainThread]: Command `dbt run` succeeded at 10:30:14.193305 after 61.33 seconds
[0m10:30:14.195064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f882da36640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8804cf6670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8804010640>]}
[0m10:30:14.196403 [debug] [MainThread]: Flushing usage events
[0m10:30:21.938182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57fed07580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57fd2af1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57fd2af910>]}


============================== 10:30:21.953763 | 9d5bd275-bd4a-434b-b5dc-4534b6793061 ==============================
[0m10:30:21.953763 [info ] [MainThread]: Running with dbt=1.5.2
[0m10:30:21.955590 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m10:30:23.674002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9d5bd275-bd4a-434b-b5dc-4534b6793061', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57fd2af190>]}
[0m10:30:23.733336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9d5bd275-bd4a-434b-b5dc-4534b6793061', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57d1ff9b80>]}
[0m10:30:23.734937 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m10:30:23.827694 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m10:30:24.066794 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:30:24.067735 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:30:24.085642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9d5bd275-bd4a-434b-b5dc-4534b6793061', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57d1fb13d0>]}
[0m10:30:24.139602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9d5bd275-bd4a-434b-b5dc-4534b6793061', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57d1ede400>]}
[0m10:30:24.140871 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m10:30:24.142010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9d5bd275-bd4a-434b-b5dc-4534b6793061', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57d1ede460>]}
[0m10:30:24.148090 [info ] [MainThread]: 
[0m10:30:24.150135 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:30:24.153327 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m10:30:24.164289 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m10:30:24.165137 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m10:30:24.165817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:30:25.654288 [debug] [ThreadPool]: SQL status: OK in 1.4900000095367432 seconds
[0m10:30:25.658116 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m10:30:25.826325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9d5bd275-bd4a-434b-b5dc-4534b6793061', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57d1ede070>]}
[0m10:30:25.827772 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:25.828687 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:30:25.830187 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:30:25.831156 [info ] [MainThread]: 
[0m10:30:25.856977 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m10:30:25.858367 [info ] [Thread-2  ]: 1 of 22 START test accepted_rating_range_silver_ratings_rating__5__0 ........... [RUN]
[0m10:30:25.860045 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff)
[0m10:30:25.860918 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m10:30:25.876113 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m10:30:25.895293 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (compile): 10:30:25.861540 => 10:30:25.894782
[0m10:30:25.896400 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m10:30:25.945846 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m10:30:25.961107 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:25.962058 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m10:30:25.962894 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`silver_ratings`
where rating < 0
   or rating > 5


      
    ) dbt_internal_test
[0m10:30:25.963827 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:28.518940 [debug] [Thread-2  ]: SQL status: OK in 2.559999942779541 seconds
[0m10:30:28.534747 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (execute): 10:30:25.897231 => 10:30:28.534100
[0m10:30:28.536148 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: ROLLBACK
[0m10:30:28.537352 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:28.538482 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: Close
[0m10:30:28.687687 [info ] [Thread-2  ]: 1 of 22 PASS accepted_rating_range_silver_ratings_rating__5__0 ................. [[32mPASS[0m in 2.83s]
[0m10:30:28.690107 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m10:30:28.691516 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m10:30:28.692772 [info ] [Thread-2  ]: 2 of 22 START test not_null_silver_links_imdb_id ............................... [RUN]
[0m10:30:28.695022 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff, now test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51)
[0m10:30:28.696460 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m10:30:28.713259 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m10:30:28.732930 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (compile): 10:30:28.697401 => 10:30:28.731974
[0m10:30:28.734721 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m10:30:28.742704 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m10:30:28.762383 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:28.763984 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m10:30:28.765395 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



      
    ) dbt_internal_test
[0m10:30:28.766675 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:30.383321 [debug] [Thread-2  ]: SQL status: OK in 1.6200000047683716 seconds
[0m10:30:30.390912 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (execute): 10:30:28.735894 => 10:30:30.390411
[0m10:30:30.392664 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: ROLLBACK
[0m10:30:30.395276 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:30.396626 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m10:30:30.561425 [info ] [Thread-2  ]: 2 of 22 PASS not_null_silver_links_imdb_id ..................................... [[32mPASS[0m in 1.87s]
[0m10:30:30.564630 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m10:30:30.566105 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m10:30:30.567301 [info ] [Thread-2  ]: 3 of 22 START test not_null_silver_links_movie_id .............................. [RUN]
[0m10:30:30.569502 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51, now test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d)
[0m10:30:30.570653 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m10:30:30.586371 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m10:30:30.604214 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (compile): 10:30:30.571645 => 10:30:30.603564
[0m10:30:30.605399 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m10:30:30.611615 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m10:30:30.629755 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:30.631437 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m10:30:30.632593 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



      
    ) dbt_internal_test
[0m10:30:30.633664 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:31.998632 [debug] [Thread-2  ]: SQL status: OK in 1.3600000143051147 seconds
[0m10:30:32.006327 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (execute): 10:30:30.606169 => 10:30:32.005598
[0m10:30:32.008944 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: ROLLBACK
[0m10:30:32.013895 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:32.015344 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m10:30:32.200580 [info ] [Thread-2  ]: 3 of 22 PASS not_null_silver_links_movie_id .................................... [[32mPASS[0m in 1.63s]
[0m10:30:32.202905 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m10:30:32.204611 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m10:30:32.205957 [info ] [Thread-2  ]: 4 of 22 START test not_null_silver_links_tmdb_id ............................... [RUN]
[0m10:30:32.208125 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d, now test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f)
[0m10:30:32.209332 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m10:30:32.229752 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m10:30:32.254755 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (compile): 10:30:32.210461 => 10:30:32.254202
[0m10:30:32.256010 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m10:30:32.262942 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m10:30:32.283488 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:32.284746 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m10:30:32.285716 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



      
    ) dbt_internal_test
[0m10:30:32.287153 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:33.651577 [debug] [Thread-2  ]: SQL status: OK in 1.3600000143051147 seconds
[0m10:30:33.659008 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (execute): 10:30:32.257023 => 10:30:33.658481
[0m10:30:33.660245 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: ROLLBACK
[0m10:30:33.661447 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:33.663013 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m10:30:33.831335 [info ] [Thread-2  ]: 4 of 22 PASS not_null_silver_links_tmdb_id ..................................... [[32mPASS[0m in 1.62s]
[0m10:30:33.834227 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m10:30:33.835673 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m10:30:33.836964 [info ] [Thread-2  ]: 5 of 22 START test not_null_silver_movies_genres ............................... [RUN]
[0m10:30:33.839124 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f, now test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63)
[0m10:30:33.840298 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m10:30:33.855700 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m10:30:33.874906 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (compile): 10:30:33.841275 => 10:30:33.874379
[0m10:30:33.876060 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m10:30:33.885554 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m10:30:33.904902 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:33.906526 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m10:30:33.907746 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



      
    ) dbt_internal_test
[0m10:30:33.908972 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:35.205126 [debug] [Thread-2  ]: SQL status: OK in 1.2999999523162842 seconds
[0m10:30:35.212899 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (execute): 10:30:33.877030 => 10:30:35.212347
[0m10:30:35.214280 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: ROLLBACK
[0m10:30:35.216613 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:35.217952 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m10:30:35.387603 [info ] [Thread-2  ]: 5 of 22 PASS not_null_silver_movies_genres ..................................... [[32mPASS[0m in 1.55s]
[0m10:30:35.390697 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m10:30:35.392409 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m10:30:35.393983 [info ] [Thread-2  ]: 6 of 22 START test not_null_silver_movies_movie_id ............................. [RUN]
[0m10:30:35.396281 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63, now test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e)
[0m10:30:35.397487 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m10:30:35.409931 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m10:30:35.432841 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (compile): 10:30:35.398288 => 10:30:35.431560
[0m10:30:35.434353 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m10:30:35.441702 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m10:30:35.458983 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:35.460100 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m10:30:35.461105 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



      
    ) dbt_internal_test
[0m10:30:35.462204 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:37.088324 [debug] [Thread-2  ]: SQL status: OK in 1.6299999952316284 seconds
[0m10:30:37.096475 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (execute): 10:30:35.435459 => 10:30:37.095940
[0m10:30:37.097767 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: ROLLBACK
[0m10:30:37.099120 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:37.100241 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m10:30:37.283012 [info ] [Thread-2  ]: 6 of 22 PASS not_null_silver_movies_movie_id ................................... [[32mPASS[0m in 1.89s]
[0m10:30:37.285466 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m10:30:37.286992 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m10:30:37.288372 [info ] [Thread-2  ]: 7 of 22 START test not_null_silver_movies_title ................................ [RUN]
[0m10:30:37.290434 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e, now test.my_dbt_project.not_null_silver_movies_title.3a779bd2de)
[0m10:30:37.291796 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m10:30:37.314543 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m10:30:37.336082 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (compile): 10:30:37.292735 => 10:30:37.335282
[0m10:30:37.337612 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m10:30:37.345312 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m10:30:37.363777 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:37.365627 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m10:30:37.366826 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



      
    ) dbt_internal_test
[0m10:30:37.368024 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:38.644843 [debug] [Thread-2  ]: SQL status: OK in 1.2799999713897705 seconds
[0m10:30:38.653247 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (execute): 10:30:37.338554 => 10:30:38.652652
[0m10:30:38.654533 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: ROLLBACK
[0m10:30:38.655721 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:38.656856 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m10:30:38.810426 [info ] [Thread-2  ]: 7 of 22 PASS not_null_silver_movies_title ...................................... [[32mPASS[0m in 1.52s]
[0m10:30:38.812915 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m10:30:38.814210 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m10:30:38.815495 [info ] [Thread-2  ]: 8 of 22 START test not_null_silver_ratings_movie_id ............................ [RUN]
[0m10:30:38.817837 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_title.3a779bd2de, now test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36)
[0m10:30:38.819183 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m10:30:38.829496 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m10:30:38.854624 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (compile): 10:30:38.820010 => 10:30:38.853957
[0m10:30:38.855882 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m10:30:38.864286 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m10:30:38.884631 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:38.885818 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m10:30:38.886880 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



      
    ) dbt_internal_test
[0m10:30:38.888063 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:40.108008 [debug] [Thread-2  ]: SQL status: OK in 1.2200000286102295 seconds
[0m10:30:40.116129 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (execute): 10:30:38.856809 => 10:30:40.115345
[0m10:30:40.117279 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: ROLLBACK
[0m10:30:40.118493 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:40.119567 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m10:30:40.268012 [info ] [Thread-2  ]: 8 of 22 PASS not_null_silver_ratings_movie_id .................................. [[32mPASS[0m in 1.45s]
[0m10:30:40.270329 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m10:30:40.271548 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m10:30:40.272716 [info ] [Thread-2  ]: 9 of 22 START test not_null_silver_ratings_rating .............................. [RUN]
[0m10:30:40.274489 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36, now test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a)
[0m10:30:40.275567 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m10:30:40.285021 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m10:30:40.303947 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (compile): 10:30:40.276550 => 10:30:40.303252
[0m10:30:40.305193 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m10:30:40.312441 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m10:30:40.335714 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:40.337117 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m10:30:40.338117 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



      
    ) dbt_internal_test
[0m10:30:40.339217 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:41.542990 [debug] [Thread-2  ]: SQL status: OK in 1.2000000476837158 seconds
[0m10:30:41.553061 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (execute): 10:30:40.305966 => 10:30:41.552243
[0m10:30:41.555155 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: ROLLBACK
[0m10:30:41.556542 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:41.557808 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m10:30:41.717300 [info ] [Thread-2  ]: 9 of 22 PASS not_null_silver_ratings_rating .................................... [[32mPASS[0m in 1.44s]
[0m10:30:41.720402 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m10:30:41.722127 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m10:30:41.723674 [info ] [Thread-2  ]: 10 of 22 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m10:30:41.726521 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a, now test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f)
[0m10:30:41.727860 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m10:30:41.757063 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m10:30:41.781123 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (compile): 10:30:41.729031 => 10:30:41.780244
[0m10:30:41.782774 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m10:30:41.790835 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m10:30:41.814507 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:41.815912 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m10:30:41.817126 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



      
    ) dbt_internal_test
[0m10:30:41.818437 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:43.084758 [debug] [Thread-2  ]: SQL status: OK in 1.2699999809265137 seconds
[0m10:30:43.094835 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (execute): 10:30:41.783782 => 10:30:43.093739
[0m10:30:43.096691 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: ROLLBACK
[0m10:30:43.098182 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:43.099641 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m10:30:43.248518 [info ] [Thread-2  ]: 10 of 22 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 1.52s]
[0m10:30:43.251440 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m10:30:43.252744 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m10:30:43.254030 [info ] [Thread-2  ]: 11 of 22 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m10:30:43.256062 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f, now test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3)
[0m10:30:43.257279 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m10:30:43.293163 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m10:30:43.315855 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (compile): 10:30:43.258266 => 10:30:43.314927
[0m10:30:43.317399 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m10:30:43.324627 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m10:30:43.346435 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:43.348146 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m10:30:43.349222 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



      
    ) dbt_internal_test
[0m10:30:43.350393 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:44.687706 [debug] [Thread-2  ]: SQL status: OK in 1.340000033378601 seconds
[0m10:30:44.695132 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (execute): 10:30:43.318476 => 10:30:44.694283
[0m10:30:44.696651 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: ROLLBACK
[0m10:30:44.697826 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:44.699075 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m10:30:44.855242 [info ] [Thread-2  ]: 11 of 22 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 1.60s]
[0m10:30:44.857734 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m10:30:44.859247 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m10:30:44.860679 [info ] [Thread-2  ]: 12 of 22 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m10:30:44.863029 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3, now test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3)
[0m10:30:44.864530 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m10:30:44.879051 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m10:30:44.903445 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (compile): 10:30:44.865515 => 10:30:44.902817
[0m10:30:44.905016 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m10:30:44.912439 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m10:30:44.945721 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:44.946967 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m10:30:44.948261 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



      
    ) dbt_internal_test
[0m10:30:44.949656 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:46.283170 [debug] [Thread-2  ]: SQL status: OK in 1.3300000429153442 seconds
[0m10:30:46.291496 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (execute): 10:30:44.906026 => 10:30:46.290237
[0m10:30:46.293092 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: ROLLBACK
[0m10:30:46.294271 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:46.295324 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m10:30:46.461349 [info ] [Thread-2  ]: 12 of 22 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 1.60s]
[0m10:30:46.463735 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m10:30:46.465223 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m10:30:46.466461 [info ] [Thread-2  ]: 13 of 22 START test not_null_silver_tags_tag ................................... [RUN]
[0m10:30:46.468492 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3, now test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8)
[0m10:30:46.469615 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m10:30:46.488377 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m10:30:46.508291 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (compile): 10:30:46.470496 => 10:30:46.507549
[0m10:30:46.509766 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m10:30:46.516956 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m10:30:46.546751 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:46.548095 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m10:30:46.549323 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



      
    ) dbt_internal_test
[0m10:30:46.550840 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:47.911999 [debug] [Thread-2  ]: SQL status: OK in 1.3600000143051147 seconds
[0m10:30:47.918163 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (execute): 10:30:46.510583 => 10:30:47.917454
[0m10:30:47.920565 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: ROLLBACK
[0m10:30:47.921998 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:47.923292 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m10:30:48.093475 [info ] [Thread-2  ]: 13 of 22 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 1.63s]
[0m10:30:48.095913 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m10:30:48.097128 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m10:30:48.098276 [info ] [Thread-2  ]: 14 of 22 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m10:30:48.100223 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8, now test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86)
[0m10:30:48.101223 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m10:30:48.111693 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m10:30:48.128548 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (compile): 10:30:48.101938 => 10:30:48.127847
[0m10:30:48.129938 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m10:30:48.136980 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m10:30:48.155009 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:48.156321 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m10:30:48.157436 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



      
    ) dbt_internal_test
[0m10:30:48.158719 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:49.232835 [debug] [Thread-2  ]: SQL status: OK in 1.0700000524520874 seconds
[0m10:30:49.238553 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (execute): 10:30:48.130755 => 10:30:49.238076
[0m10:30:49.239768 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: ROLLBACK
[0m10:30:49.240822 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:49.241848 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m10:30:49.390382 [info ] [Thread-2  ]: 14 of 22 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 1.29s]
[0m10:30:49.392523 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m10:30:49.393763 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m10:30:49.394932 [info ] [Thread-2  ]: 15 of 22 START test not_null_silver_tags_user_id ............................... [RUN]
[0m10:30:49.396772 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86, now test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3)
[0m10:30:49.397989 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m10:30:49.407272 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m10:30:49.424200 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (compile): 10:30:49.399029 => 10:30:49.423654
[0m10:30:49.425407 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m10:30:49.431423 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m10:30:49.448917 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:49.450026 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m10:30:49.451122 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



      
    ) dbt_internal_test
[0m10:30:49.452289 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:50.637157 [debug] [Thread-2  ]: SQL status: OK in 1.190000057220459 seconds
[0m10:30:50.655032 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (execute): 10:30:49.426103 => 10:30:50.654092
[0m10:30:50.656886 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: ROLLBACK
[0m10:30:50.658624 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:50.667857 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m10:30:50.825410 [info ] [Thread-2  ]: 15 of 22 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 1.43s]
[0m10:30:50.827895 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m10:30:50.829376 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:30:50.830776 [info ] [Thread-2  ]: 16 of 22 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m10:30:50.832711 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m10:30:50.834179 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:30:50.878300 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m10:30:50.910297 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 10:30:50.835429 => 10:30:50.909547
[0m10:30:50.911691 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:30:50.918911 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m10:30:50.939623 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:50.941590 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m10:30:50.943009 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m10:30:50.944891 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:52.607853 [debug] [Thread-2  ]: SQL status: OK in 1.659999966621399 seconds
[0m10:30:52.614906 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 10:30:50.912629 => 10:30:52.614326
[0m10:30:52.616170 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m10:30:52.617696 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:52.618949 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m10:30:52.589884 [info ] [Thread-2  ]: 16 of 22 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.76s]
[0m10:30:52.592349 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:30:52.593793 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m10:30:52.595067 [info ] [Thread-2  ]: 17 of 22 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m10:30:52.597228 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m10:30:52.598378 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m10:30:52.613441 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m10:30:51.390788 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 10:30:52.599447 => 10:30:51.389572
[0m10:30:51.392693 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m10:30:51.408380 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m10:30:51.428457 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:51.429906 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m10:30:51.431216 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m10:30:51.432737 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:52.803337 [debug] [Thread-2  ]: SQL status: OK in 1.3700000047683716 seconds
[0m10:30:52.809528 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 10:30:51.393804 => 10:30:52.809048
[0m10:30:52.810747 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m10:30:52.812429 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:52.813728 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m10:30:53.021288 [info ] [Thread-2  ]: 17 of 22 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.42s]
[0m10:30:53.023561 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m10:30:53.025045 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m10:30:53.026274 [info ] [Thread-2  ]: 18 of 22 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m10:30:53.028706 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32)
[0m10:30:53.030012 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m10:30:53.044788 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m10:30:53.075647 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (compile): 10:30:53.030900 => 10:30:53.074685
[0m10:30:53.077053 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m10:30:53.085569 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m10:30:53.105262 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:53.106938 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m10:30:53.108089 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m10:30:53.109362 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:54.897149 [debug] [Thread-2  ]: SQL status: OK in 1.7899999618530273 seconds
[0m10:30:54.904996 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (execute): 10:30:53.077894 => 10:30:54.904293
[0m10:30:54.906873 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: ROLLBACK
[0m10:30:54.907995 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:54.909111 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m10:30:55.098111 [info ] [Thread-2  ]: 18 of 22 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 2.07s]
[0m10:30:55.100909 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m10:30:55.102506 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m10:30:55.104741 [info ] [Thread-2  ]: 19 of 22 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m10:30:55.107187 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32, now test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd)
[0m10:30:55.108371 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m10:30:55.145587 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m10:30:55.179357 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (compile): 10:30:55.109172 => 10:30:55.178509
[0m10:30:55.180735 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m10:30:55.188485 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m10:30:55.209119 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:55.210389 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m10:30:55.211429 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m10:30:55.212667 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:57.008919 [debug] [Thread-2  ]: SQL status: OK in 1.7999999523162842 seconds
[0m10:30:57.015819 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (execute): 10:30:55.181663 => 10:30:57.015078
[0m10:30:57.017230 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: ROLLBACK
[0m10:30:57.018937 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:57.020531 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m10:30:57.197249 [info ] [Thread-2  ]: 19 of 22 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 2.09s]
[0m10:30:57.199573 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m10:30:57.200935 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m10:30:57.202162 [info ] [Thread-2  ]: 20 of 22 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m10:30:57.204752 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd, now test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719)
[0m10:30:57.206025 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m10:30:57.218942 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m10:30:57.238536 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (compile): 10:30:57.206820 => 10:30:57.237817
[0m10:30:57.239978 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m10:30:57.246538 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m10:30:57.264162 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:57.265281 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m10:30:57.266643 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m10:30:57.268116 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:30:59.024811 [debug] [Thread-2  ]: SQL status: OK in 1.7599999904632568 seconds
[0m10:30:59.031018 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (execute): 10:30:57.240728 => 10:30:59.030536
[0m10:30:59.032283 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: ROLLBACK
[0m10:30:59.033582 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:30:59.034688 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m10:30:59.187907 [info ] [Thread-2  ]: 20 of 22 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 1.98s]
[0m10:30:59.190153 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m10:30:59.191633 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m10:30:59.192993 [info ] [Thread-2  ]: 21 of 22 START test unique_silver_links_movie_id ............................... [RUN]
[0m10:30:59.195156 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719, now test.my_dbt_project.unique_silver_links_movie_id.c422609054)
[0m10:30:59.196282 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m10:30:59.214634 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m10:30:59.234694 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (compile): 10:30:59.197261 => 10:30:59.233944
[0m10:30:59.236165 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m10:30:59.244521 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m10:30:59.263557 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:30:59.264837 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m10:30:59.266265 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m10:30:59.267703 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:00.321113 [debug] [Thread-2  ]: SQL status: OK in 1.0499999523162842 seconds
[0m10:31:00.342305 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (execute): 10:30:59.237179 => 10:31:00.341397
[0m10:31:00.344506 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: ROLLBACK
[0m10:31:00.347429 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:00.351887 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m10:31:00.506366 [info ] [Thread-2  ]: 21 of 22 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 1.31s]
[0m10:31:00.510374 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m10:31:00.512683 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m10:31:00.514664 [info ] [Thread-2  ]: 22 of 22 START test unique_silver_movies_movie_id .............................. [RUN]
[0m10:31:00.519625 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_silver_links_movie_id.c422609054, now test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085)
[0m10:31:00.522213 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m10:31:00.535525 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m10:31:00.554594 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (compile): 10:31:00.523348 => 10:31:00.553984
[0m10:31:00.555806 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m10:31:00.577990 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m10:31:00.596023 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:00.597240 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m10:31:00.598382 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m10:31:00.599601 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:01.936952 [debug] [Thread-2  ]: SQL status: OK in 1.340000033378601 seconds
[0m10:31:01.944018 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (execute): 10:31:00.556641 => 10:31:01.943500
[0m10:31:01.945400 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: ROLLBACK
[0m10:31:01.946429 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:01.947428 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m10:31:02.104558 [info ] [Thread-2  ]: 22 of 22 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 1.59s]
[0m10:31:02.106787 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m10:31:02.110710 [debug] [MainThread]: On master: ROLLBACK
[0m10:31:02.111728 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:31:02.643466 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:31:02.645204 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:02.646275 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:31:02.647337 [debug] [MainThread]: On master: ROLLBACK
[0m10:31:02.648365 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:31:02.649237 [debug] [MainThread]: On master: Close
[0m10:31:02.869638 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:31:02.871105 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085' was properly closed.
[0m10:31:02.875228 [info ] [MainThread]: 
[0m10:31:02.877719 [info ] [MainThread]: Finished running 22 tests in 0 hours 0 minutes and 38.73 seconds (38.73s).
[0m10:31:02.887789 [debug] [MainThread]: Command end result
[0m10:31:02.965559 [info ] [MainThread]: 
[0m10:31:02.967572 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:31:02.969412 [info ] [MainThread]: 
[0m10:31:02.972175 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=0 SKIP=0 TOTAL=22
[0m10:31:02.974869 [debug] [MainThread]: Command `dbt test` succeeded at 10:31:02.974511 after 44.12 seconds
[0m10:31:02.976275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57fed07580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57d1ff9b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57d203d250>]}
[0m10:31:02.977719 [debug] [MainThread]: Flushing usage events
[0m10:31:11.158603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9c2c3875b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9c2a8ef220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9c2a8ef940>]}


============================== 10:31:11.173869 | 2af9aa14-2212-441c-8f0c-bf9106b4f414 ==============================
[0m10:31:11.173869 [info ] [MainThread]: Running with dbt=1.5.2
[0m10:31:11.175416 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/opt/airflow/dbt/logs', 'version_check': 'True', 'profiles_dir': '/opt/airflow/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m10:31:12.545632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2af9aa14-2212-441c-8f0c-bf9106b4f414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9c2a8ef1c0>]}
[0m10:31:12.581376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2af9aa14-2212-441c-8f0c-bf9106b4f414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9bff615c40>]}
[0m10:31:12.582270 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m10:31:12.633297 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m10:31:14.192860 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:31:14.193921 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:31:14.211154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2af9aa14-2212-441c-8f0c-bf9106b4f414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9bff5cb400>]}
[0m10:31:14.258471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2af9aa14-2212-441c-8f0c-bf9106b4f414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9bff4f7430>]}
[0m10:31:14.259502 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m10:31:14.260815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2af9aa14-2212-441c-8f0c-bf9106b4f414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9bff4f7490>]}
[0m10:31:14.266806 [info ] [MainThread]: 
[0m10:31:14.268981 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:31:14.271923 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m10:31:14.279901 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m10:31:14.280672 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m10:31:14.281547 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:31:15.292007 [debug] [ThreadPool]: SQL status: OK in 1.0099999904632568 seconds
[0m10:31:15.300153 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m10:31:15.464408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2af9aa14-2212-441c-8f0c-bf9106b4f414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9bff4f7310>]}
[0m10:31:15.466068 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:15.467101 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:31:15.468713 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:31:15.470485 [info ] [MainThread]: 
[0m10:31:15.496904 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m10:31:15.498429 [info ] [Thread-2  ]: 1 of 35 START test accepted_rating_range_genre_popularity_avg_rating__5__0 ..... [RUN]
[0m10:31:15.500821 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413)
[0m10:31:15.501906 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m10:31:15.532565 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m10:31:15.567446 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (compile): 10:31:15.502842 => 10:31:15.566361
[0m10:31:15.569156 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m10:31:15.628711 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m10:31:15.649169 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:15.650282 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"
[0m10:31:15.651266 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m10:31:15.652316 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:17.124557 [debug] [Thread-2  ]: SQL status: OK in 1.4700000286102295 seconds
[0m10:31:17.140249 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413 (execute): 10:31:15.570766 => 10:31:17.139463
[0m10:31:17.141945 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: ROLLBACK
[0m10:31:17.143997 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:17.145301 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413: Close
[0m10:31:17.310138 [info ] [Thread-2  ]: 1 of 35 PASS accepted_rating_range_genre_popularity_avg_rating__5__0 ........... [[32mPASS[0m in 1.81s]
[0m10:31:17.312772 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413
[0m10:31:17.314448 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m10:31:17.315645 [info ] [Thread-2  ]: 2 of 35 START test accepted_rating_range_movie_enriched_avg_rating__5__0 ....... [RUN]
[0m10:31:17.317710 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_genre_popularity_avg_rating__5__0.bf6b04f413, now test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8)
[0m10:31:17.318872 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m10:31:17.330722 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m10:31:17.350289 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (compile): 10:31:17.320858 => 10:31:17.349374
[0m10:31:17.351857 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m10:31:17.360482 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m10:31:17.379268 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:17.380821 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m10:31:17.381891 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m10:31:17.383338 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:18.525003 [debug] [Thread-2  ]: SQL status: OK in 1.1399999856948853 seconds
[0m10:31:18.532758 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (execute): 10:31:17.353036 => 10:31:18.532213
[0m10:31:18.534638 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: ROLLBACK
[0m10:31:18.536180 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:18.537735 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: Close
[0m10:31:18.683420 [info ] [Thread-2  ]: 2 of 35 PASS accepted_rating_range_movie_enriched_avg_rating__5__0 ............. [[32mPASS[0m in 1.37s]
[0m10:31:18.686232 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m10:31:18.687694 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m10:31:18.688926 [info ] [Thread-2  ]: 3 of 35 START test accepted_rating_range_movie_performance_weighted_avg_rating__5__0  [RUN]
[0m10:31:18.691501 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8, now test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083)
[0m10:31:18.692787 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m10:31:18.713522 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m10:31:18.739687 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (compile): 10:31:18.693807 => 10:31:18.738525
[0m10:31:18.741819 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m10:31:18.751101 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m10:31:18.770867 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:18.772303 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"
[0m10:31:18.773696 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating < 0
   or weighted_avg_rating > 5


      
    ) dbt_internal_test
[0m10:31:18.775113 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:20.040582 [debug] [Thread-2  ]: SQL status: OK in 1.2699999809265137 seconds
[0m10:31:20.049682 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083 (execute): 10:31:18.743446 => 10:31:20.049063
[0m10:31:20.051082 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: ROLLBACK
[0m10:31:20.052343 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:20.053396 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083: Close
[0m10:31:20.214827 [info ] [Thread-2  ]: 3 of 35 PASS accepted_rating_range_movie_performance_weighted_avg_rating__5__0 . [[32mPASS[0m in 1.52s]
[0m10:31:20.216885 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083
[0m10:31:20.218305 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m10:31:20.219505 [info ] [Thread-2  ]: 4 of 35 START test accepted_rating_range_user_activity_avg_user_rating__5__0 ... [RUN]
[0m10:31:20.221514 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_performance_weighted_avg_rating__5__0.111a09e083, now test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a)
[0m10:31:20.222704 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m10:31:20.245454 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m10:31:20.265601 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (compile): 10:31:20.223972 => 10:31:20.265031
[0m10:31:20.266889 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m10:31:20.274001 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m10:31:20.292262 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:20.293493 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"
[0m10:31:20.294740 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating < 0
   or avg_user_rating > 5


      
    ) dbt_internal_test
[0m10:31:20.296187 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:19.955993 [debug] [Thread-2  ]: SQL status: OK in -0.3400000035762787 seconds
[0m10:31:19.965035 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a (execute): 10:31:20.267753 => 10:31:19.964213
[0m10:31:19.967031 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: ROLLBACK
[0m10:31:19.968538 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:19.969689 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a: Close
[0m10:31:20.144789 [info ] [Thread-2  ]: 4 of 35 PASS accepted_rating_range_user_activity_avg_user_rating__5__0 ......... [[32mPASS[0m in -0.08s]
[0m10:31:20.147651 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a
[0m10:31:20.149140 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m10:31:20.150582 [info ] [Thread-2  ]: 5 of 35 START test not_null_genre_popularity_avg_rating ........................ [RUN]
[0m10:31:20.152794 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_user_activity_avg_user_rating__5__0.130fc1f83a, now test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9)
[0m10:31:20.153849 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m10:31:20.169702 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m10:31:20.200379 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (compile): 10:31:20.154776 => 10:31:20.199631
[0m10:31:20.202085 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m10:31:20.209101 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m10:31:20.229105 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:20.230316 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"
[0m10:31:20.231540 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`genre_popularity`
where avg_rating is null



      
    ) dbt_internal_test
[0m10:31:20.232966 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:21.248391 [debug] [Thread-2  ]: SQL status: OK in 1.0199999809265137 seconds
[0m10:31:21.254649 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9 (execute): 10:31:20.203195 => 10:31:21.254194
[0m10:31:21.255869 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: ROLLBACK
[0m10:31:21.256950 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:21.258426 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9: Close
[0m10:31:21.463569 [info ] [Thread-2  ]: 5 of 35 PASS not_null_genre_popularity_avg_rating .............................. [[32mPASS[0m in 1.31s]
[0m10:31:21.466026 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9
[0m10:31:21.467558 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m10:31:21.468855 [info ] [Thread-2  ]: 6 of 35 START test not_null_genre_popularity_first_rating_date ................. [RUN]
[0m10:31:21.472266 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_avg_rating.cf98b623c9, now test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32)
[0m10:31:21.474605 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m10:31:21.487468 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m10:31:21.518390 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (compile): 10:31:21.475937 => 10:31:21.517556
[0m10:31:21.519622 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m10:31:21.525599 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m10:31:21.543623 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:21.544701 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"
[0m10:31:21.545675 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where first_rating_date is null



      
    ) dbt_internal_test
[0m10:31:21.546815 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:23.171509 [debug] [Thread-2  ]: SQL status: OK in 1.6200000047683716 seconds
[0m10:31:23.177869 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32 (execute): 10:31:21.520495 => 10:31:23.177395
[0m10:31:23.179557 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: ROLLBACK
[0m10:31:23.180692 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:23.181747 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32: Close
[0m10:31:23.332807 [info ] [Thread-2  ]: 6 of 35 PASS not_null_genre_popularity_first_rating_date ....................... [[32mPASS[0m in 1.86s]
[0m10:31:23.334791 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32
[0m10:31:23.336150 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m10:31:23.337470 [info ] [Thread-2  ]: 7 of 35 START test not_null_genre_popularity_genres ............................ [RUN]
[0m10:31:23.339452 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_first_rating_date.fa7038de32, now test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa)
[0m10:31:23.340618 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m10:31:23.364434 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m10:31:23.395587 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (compile): 10:31:23.341459 => 10:31:23.394858
[0m10:31:23.397074 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m10:31:23.404247 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m10:31:23.422259 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:23.423177 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"
[0m10:31:23.424607 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`genre_popularity`
where genres is null



      
    ) dbt_internal_test
[0m10:31:23.425798 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:24.627694 [debug] [Thread-2  ]: SQL status: OK in 1.2000000476837158 seconds
[0m10:31:24.635194 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa (execute): 10:31:23.397882 => 10:31:24.634673
[0m10:31:24.636528 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: ROLLBACK
[0m10:31:24.637825 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:24.638990 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa: Close
[0m10:31:24.808153 [info ] [Thread-2  ]: 7 of 35 PASS not_null_genre_popularity_genres .................................. [[32mPASS[0m in 1.47s]
[0m10:31:24.810641 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa
[0m10:31:24.812078 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m10:31:24.813410 [info ] [Thread-2  ]: 8 of 35 START test not_null_genre_popularity_last_rating_date .................. [RUN]
[0m10:31:24.815726 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_genres.ba9ec845fa, now test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432)
[0m10:31:24.817217 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m10:31:24.834022 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m10:31:24.853746 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (compile): 10:31:24.818136 => 10:31:24.852991
[0m10:31:24.855180 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m10:31:24.863710 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m10:31:24.892532 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:24.893926 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"
[0m10:31:24.895027 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_rating_date
from `workspace`.`movielens_volume`.`genre_popularity`
where last_rating_date is null



      
    ) dbt_internal_test
[0m10:31:24.896257 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:25.982797 [debug] [Thread-2  ]: SQL status: OK in 1.090000033378601 seconds
[0m10:31:25.989881 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432 (execute): 10:31:24.856015 => 10:31:25.989226
[0m10:31:25.991351 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: ROLLBACK
[0m10:31:25.992719 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:25.993774 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432: Close
[0m10:31:26.145809 [info ] [Thread-2  ]: 8 of 35 PASS not_null_genre_popularity_last_rating_date ........................ [[32mPASS[0m in 1.33s]
[0m10:31:26.147878 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432
[0m10:31:26.149392 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m10:31:26.150742 [info ] [Thread-2  ]: 9 of 35 START test not_null_genre_popularity_popularity_rank ................... [RUN]
[0m10:31:26.152806 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_last_rating_date.6c5d46b432, now test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc)
[0m10:31:26.154013 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m10:31:26.169478 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m10:31:26.188483 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (compile): 10:31:26.155176 => 10:31:26.187812
[0m10:31:26.189663 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m10:31:26.197508 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m10:31:26.216495 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:26.217830 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"
[0m10:31:26.219120 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select popularity_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where popularity_rank is null



      
    ) dbt_internal_test
[0m10:31:26.220231 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:27.276772 [debug] [Thread-2  ]: SQL status: OK in 1.059999942779541 seconds
[0m10:31:27.283187 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc (execute): 10:31:26.190466 => 10:31:27.282627
[0m10:31:27.284286 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: ROLLBACK
[0m10:31:27.285226 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:27.286160 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc: Close
[0m10:31:27.460254 [info ] [Thread-2  ]: 9 of 35 PASS not_null_genre_popularity_popularity_rank ......................... [[32mPASS[0m in 1.31s]
[0m10:31:27.462351 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc
[0m10:31:27.463842 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m10:31:27.465073 [info ] [Thread-2  ]: 10 of 35 START test not_null_genre_popularity_quality_rank ..................... [RUN]
[0m10:31:27.467075 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_popularity_rank.0cbade4ebc, now test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a)
[0m10:31:27.468412 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m10:31:27.483993 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m10:31:27.499530 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (compile): 10:31:27.469327 => 10:31:27.498867
[0m10:31:27.500891 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m10:31:27.507496 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m10:31:27.536609 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:27.538193 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"
[0m10:31:27.539505 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select quality_rank
from `workspace`.`movielens_volume`.`genre_popularity`
where quality_rank is null



      
    ) dbt_internal_test
[0m10:31:27.540799 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:28.862958 [debug] [Thread-2  ]: SQL status: OK in 1.3200000524520874 seconds
[0m10:31:28.870076 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a (execute): 10:31:27.501930 => 10:31:28.869321
[0m10:31:28.871342 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: ROLLBACK
[0m10:31:28.873185 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:28.874541 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a: Close
[0m10:31:29.032060 [info ] [Thread-2  ]: 10 of 35 PASS not_null_genre_popularity_quality_rank ........................... [[32mPASS[0m in 1.57s]
[0m10:31:29.034641 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a
[0m10:31:29.036046 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m10:31:29.039229 [info ] [Thread-2  ]: 11 of 35 START test not_null_genre_popularity_rating_count ..................... [RUN]
[0m10:31:29.046600 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_quality_rank.1b2b3c9e8a, now test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856)
[0m10:31:29.048991 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m10:31:29.064191 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m10:31:29.083447 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (compile): 10:31:29.050165 => 10:31:29.082723
[0m10:31:29.084753 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m10:31:29.092884 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m10:31:29.123038 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:29.124158 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"
[0m10:31:29.125052 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`genre_popularity`
where rating_count is null



      
    ) dbt_internal_test
[0m10:31:29.126163 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:30.255289 [debug] [Thread-2  ]: SQL status: OK in 1.1299999952316284 seconds
[0m10:31:30.263798 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856 (execute): 10:31:29.085801 => 10:31:30.262849
[0m10:31:30.265384 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: ROLLBACK
[0m10:31:30.266735 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:30.267826 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856: Close
[0m10:31:30.433544 [info ] [Thread-2  ]: 11 of 35 PASS not_null_genre_popularity_rating_count ........................... [[32mPASS[0m in 1.39s]
[0m10:31:30.435715 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856
[0m10:31:30.437189 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m10:31:30.438598 [info ] [Thread-2  ]: 12 of 35 START test not_null_movie_enriched_avg_rating ......................... [RUN]
[0m10:31:30.441037 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_genre_popularity_rating_count.0200f95856, now test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd)
[0m10:31:30.442404 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m10:31:30.452476 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m10:31:30.473547 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (compile): 10:31:30.443604 => 10:31:30.472829
[0m10:31:30.474903 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m10:31:30.482088 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m10:31:30.503047 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:30.504614 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m10:31:30.505769 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating is null



      
    ) dbt_internal_test
[0m10:31:30.507161 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:31.809945 [debug] [Thread-2  ]: SQL status: OK in 1.2999999523162842 seconds
[0m10:31:31.817606 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (execute): 10:31:30.475876 => 10:31:31.816829
[0m10:31:31.819585 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: ROLLBACK
[0m10:31:31.820972 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:31.822088 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: Close
[0m10:31:31.994428 [info ] [Thread-2  ]: 12 of 35 PASS not_null_movie_enriched_avg_rating ............................... [[32mPASS[0m in 1.55s]
[0m10:31:31.996763 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m10:31:31.998303 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m10:31:31.999724 [info ] [Thread-2  ]: 13 of 35 START test not_null_movie_enriched_genres ............................. [RUN]
[0m10:31:32.001721 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd, now test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27)
[0m10:31:32.002900 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m10:31:32.012297 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m10:31:32.035182 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (compile): 10:31:32.003747 => 10:31:32.034357
[0m10:31:32.036663 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m10:31:32.059937 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m10:31:32.079092 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:32.080268 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m10:31:32.081423 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`movie_enriched`
where genres is null



      
    ) dbt_internal_test
[0m10:31:32.082954 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:33.253246 [debug] [Thread-2  ]: SQL status: OK in 1.1699999570846558 seconds
[0m10:31:33.263193 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (execute): 10:31:32.037575 => 10:31:33.262043
[0m10:31:33.264827 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: ROLLBACK
[0m10:31:33.266230 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:33.267360 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: Close
[0m10:31:33.428209 [info ] [Thread-2  ]: 13 of 35 PASS not_null_movie_enriched_genres ................................... [[32mPASS[0m in 1.43s]
[0m10:31:33.430560 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m10:31:33.432091 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m10:31:33.433280 [info ] [Thread-2  ]: 14 of 35 START test not_null_movie_enriched_movie_id ........................... [RUN]
[0m10:31:33.435281 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27, now test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86)
[0m10:31:33.436548 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m10:31:33.447141 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m10:31:33.470351 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (compile): 10:31:33.437389 => 10:31:33.469615
[0m10:31:33.471666 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m10:31:33.478985 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m10:31:33.500047 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:33.501249 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m10:31:33.502447 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is null



      
    ) dbt_internal_test
[0m10:31:33.503801 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:34.595284 [debug] [Thread-2  ]: SQL status: OK in 1.090000033378601 seconds
[0m10:31:34.602635 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (execute): 10:31:33.472677 => 10:31:34.602022
[0m10:31:34.603926 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: ROLLBACK
[0m10:31:34.605146 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:34.606291 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: Close
[0m10:31:34.782068 [info ] [Thread-2  ]: 14 of 35 PASS not_null_movie_enriched_movie_id ................................. [[32mPASS[0m in 1.35s]
[0m10:31:34.784605 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m10:31:34.785766 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m10:31:34.787021 [info ] [Thread-2  ]: 15 of 35 START test not_null_movie_enriched_title .............................. [RUN]
[0m10:31:34.789225 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86, now test.my_dbt_project.not_null_movie_enriched_title.d20f00c292)
[0m10:31:34.790420 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m10:31:34.800324 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m10:31:34.829290 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (compile): 10:31:34.791148 => 10:31:34.828517
[0m10:31:34.831129 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m10:31:34.840514 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m10:31:34.859543 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:34.860688 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m10:31:34.861781 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`movie_enriched`
where title is null



      
    ) dbt_internal_test
[0m10:31:34.863042 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:36.025486 [debug] [Thread-2  ]: SQL status: OK in 1.159999966621399 seconds
[0m10:31:36.031294 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (execute): 10:31:34.832288 => 10:31:36.030898
[0m10:31:36.032349 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: ROLLBACK
[0m10:31:36.033372 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:36.034432 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: Close
[0m10:31:36.185641 [info ] [Thread-2  ]: 15 of 35 PASS not_null_movie_enriched_title .................................... [[32mPASS[0m in 1.40s]
[0m10:31:36.187599 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m10:31:36.188717 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m10:31:36.189649 [info ] [Thread-2  ]: 16 of 35 START test not_null_movie_enriched_total_ratings ...................... [RUN]
[0m10:31:36.191236 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_title.d20f00c292, now test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04)
[0m10:31:36.192199 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m10:31:36.200989 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m10:31:36.215111 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (compile): 10:31:36.192833 => 10:31:36.214641
[0m10:31:36.216094 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m10:31:36.235983 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m10:31:36.249653 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:36.250753 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m10:31:36.251478 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select total_ratings
from `workspace`.`movielens_volume`.`movie_enriched`
where total_ratings is null



      
    ) dbt_internal_test
[0m10:31:36.252372 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:37.574673 [debug] [Thread-2  ]: SQL status: OK in 1.3200000524520874 seconds
[0m10:31:37.581167 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (execute): 10:31:36.216799 => 10:31:37.580580
[0m10:31:37.582588 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: ROLLBACK
[0m10:31:37.584074 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:37.585046 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: Close
[0m10:31:37.745309 [info ] [Thread-2  ]: 16 of 35 PASS not_null_movie_enriched_total_ratings ............................ [[32mPASS[0m in 1.55s]
[0m10:31:37.747626 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m10:31:37.749092 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m10:31:37.750431 [info ] [Thread-2  ]: 17 of 35 START test not_null_movie_performance_genre_rank ...................... [RUN]
[0m10:31:37.752670 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04, now test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e)
[0m10:31:37.754008 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m10:31:37.770122 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m10:31:37.791389 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (compile): 10:31:37.754951 => 10:31:37.790791
[0m10:31:37.792671 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m10:31:37.800183 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m10:31:37.819771 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:37.821091 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"
[0m10:31:37.822234 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genre_rank
from `workspace`.`movielens_volume`.`movie_performance`
where genre_rank is null



      
    ) dbt_internal_test
[0m10:31:37.823702 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:39.006413 [debug] [Thread-2  ]: SQL status: OK in 1.1799999475479126 seconds
[0m10:31:39.013429 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e (execute): 10:31:37.793481 => 10:31:39.012914
[0m10:31:39.014786 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: ROLLBACK
[0m10:31:39.016320 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:39.017470 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e: Close
[0m10:31:39.176698 [info ] [Thread-2  ]: 17 of 35 PASS not_null_movie_performance_genre_rank ............................ [[32mPASS[0m in 1.42s]
[0m10:31:39.180479 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e
[0m10:31:39.182544 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m10:31:39.184299 [info ] [Thread-2  ]: 18 of 35 START test not_null_movie_performance_movie_id ........................ [RUN]
[0m10:31:39.188684 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_genre_rank.36967e006e, now test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52)
[0m10:31:39.190311 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m10:31:39.209705 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m10:31:39.240206 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (compile): 10:31:39.191840 => 10:31:39.239173
[0m10:31:39.241789 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m10:31:39.251802 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m10:31:39.272278 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:39.273750 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"
[0m10:31:39.274816 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is null



      
    ) dbt_internal_test
[0m10:31:39.276465 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:40.344543 [debug] [Thread-2  ]: SQL status: OK in 1.0700000524520874 seconds
[0m10:31:40.353027 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52 (execute): 10:31:39.242764 => 10:31:40.352285
[0m10:31:40.354499 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: ROLLBACK
[0m10:31:40.355883 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:40.356988 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52: Close
[0m10:31:40.521032 [info ] [Thread-2  ]: 18 of 35 PASS not_null_movie_performance_movie_id .............................. [[32mPASS[0m in 1.33s]
[0m10:31:40.539820 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52
[0m10:31:40.541643 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m10:31:40.543203 [info ] [Thread-2  ]: 19 of 35 START test not_null_movie_performance_polarization_score .............. [RUN]
[0m10:31:40.545782 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_movie_id.a9443f9a52, now test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6)
[0m10:31:40.547001 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m10:31:40.567727 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m10:31:40.597122 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (compile): 10:31:40.547802 => 10:31:40.596050
[0m10:31:40.602064 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m10:31:40.616530 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m10:31:40.640023 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:40.641705 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"
[0m10:31:40.643217 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select polarization_score
from `workspace`.`movielens_volume`.`movie_performance`
where polarization_score is null



      
    ) dbt_internal_test
[0m10:31:40.644813 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:41.858441 [debug] [Thread-2  ]: SQL status: OK in 1.2100000381469727 seconds
[0m10:31:41.866096 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6 (execute): 10:31:40.603325 => 10:31:41.865572
[0m10:31:41.868016 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: ROLLBACK
[0m10:31:41.869308 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:41.870449 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6: Close
[0m10:31:42.030786 [info ] [Thread-2  ]: 19 of 35 PASS not_null_movie_performance_polarization_score .................... [[32mPASS[0m in 1.49s]
[0m10:31:42.033182 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6
[0m10:31:42.034629 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m10:31:42.036125 [info ] [Thread-2  ]: 20 of 35 START test not_null_movie_performance_rating_count .................... [RUN]
[0m10:31:42.038040 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_polarization_score.bbfcfb89d6, now test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac)
[0m10:31:42.039316 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m10:31:42.055995 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m10:31:42.087268 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (compile): 10:31:42.040109 => 10:31:42.086344
[0m10:31:42.088902 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m10:31:42.096267 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m10:31:42.119078 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:42.120646 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"
[0m10:31:42.121732 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_count
from `workspace`.`movielens_volume`.`movie_performance`
where rating_count is null



      
    ) dbt_internal_test
[0m10:31:42.123061 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:43.281160 [debug] [Thread-2  ]: SQL status: OK in 1.159999966621399 seconds
[0m10:31:43.288633 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac (execute): 10:31:42.089832 => 10:31:43.288083
[0m10:31:43.289955 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: ROLLBACK
[0m10:31:43.291327 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:43.292901 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac: Close
[0m10:31:43.683474 [info ] [Thread-2  ]: 20 of 35 PASS not_null_movie_performance_rating_count .......................... [[32mPASS[0m in 1.65s]
[0m10:31:43.686666 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac
[0m10:31:43.688644 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m10:31:43.690319 [info ] [Thread-2  ]: 21 of 35 START test not_null_movie_performance_top_rating_percentage ........... [RUN]
[0m10:31:43.693471 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_rating_count.015d1d21ac, now test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb)
[0m10:31:43.694779 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m10:31:43.709350 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m10:31:43.741646 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (compile): 10:31:43.695867 => 10:31:43.740538
[0m10:31:43.743034 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m10:31:43.751396 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m10:31:43.771013 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:43.772649 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"
[0m10:31:43.774067 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_rating_percentage
from `workspace`.`movielens_volume`.`movie_performance`
where top_rating_percentage is null



      
    ) dbt_internal_test
[0m10:31:43.775695 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:44.853855 [debug] [Thread-2  ]: SQL status: OK in 1.0800000429153442 seconds
[0m10:31:44.859944 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb (execute): 10:31:43.743876 => 10:31:44.859514
[0m10:31:44.861177 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: ROLLBACK
[0m10:31:44.862476 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:44.863593 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb: Close
[0m10:31:45.007534 [info ] [Thread-2  ]: 21 of 35 PASS not_null_movie_performance_top_rating_percentage ................. [[32mPASS[0m in 1.32s]
[0m10:31:45.009506 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb
[0m10:31:45.011150 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m10:31:45.012763 [info ] [Thread-2  ]: 22 of 35 START test not_null_movie_performance_weighted_avg_rating ............. [RUN]
[0m10:31:45.014813 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_top_rating_percentage.c4bb7061fb, now test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5)
[0m10:31:45.015920 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m10:31:45.025079 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m10:31:45.043732 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (compile): 10:31:45.016770 => 10:31:45.043201
[0m10:31:45.045143 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m10:31:45.063377 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m10:31:45.081303 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:45.082591 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"
[0m10:31:45.083739 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select weighted_avg_rating
from `workspace`.`movielens_volume`.`movie_performance`
where weighted_avg_rating is null



      
    ) dbt_internal_test
[0m10:31:45.085011 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:46.462585 [debug] [Thread-2  ]: SQL status: OK in 1.3799999952316284 seconds
[0m10:31:46.470242 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5 (execute): 10:31:45.046008 => 10:31:46.469693
[0m10:31:46.471454 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: ROLLBACK
[0m10:31:46.472664 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:46.473775 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5: Close
[0m10:31:46.644671 [info ] [Thread-2  ]: 22 of 35 PASS not_null_movie_performance_weighted_avg_rating ................... [[32mPASS[0m in 1.63s]
[0m10:31:46.647308 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5
[0m10:31:46.648638 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m10:31:46.649874 [info ] [Thread-2  ]: 23 of 35 START test not_null_user_activity_avg_user_rating ..................... [RUN]
[0m10:31:46.652414 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_performance_weighted_avg_rating.1892c5f3c5, now test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b)
[0m10:31:46.653474 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m10:31:46.664749 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m10:31:46.685824 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (compile): 10:31:46.654387 => 10:31:46.685027
[0m10:31:46.687426 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m10:31:46.697344 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m10:31:46.716197 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:46.717470 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"
[0m10:31:46.718612 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_user_rating
from `workspace`.`movielens_volume`.`user_activity`
where avg_user_rating is null



      
    ) dbt_internal_test
[0m10:31:46.719853 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:48.739795 [debug] [Thread-2  ]: SQL status: OK in 2.0199999809265137 seconds
[0m10:31:48.747953 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b (execute): 10:31:46.688535 => 10:31:48.747421
[0m10:31:48.749393 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: ROLLBACK
[0m10:31:48.750995 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:48.752865 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b: Close
[0m10:31:48.918589 [info ] [Thread-2  ]: 23 of 35 PASS not_null_user_activity_avg_user_rating ........................... [[32mPASS[0m in 2.27s]
[0m10:31:48.921150 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b
[0m10:31:48.922773 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m10:31:48.924112 [info ] [Thread-2  ]: 24 of 35 START test not_null_user_activity_first_activity_date ................. [RUN]
[0m10:31:48.926169 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_avg_user_rating.a0416d189b, now test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac)
[0m10:31:48.927378 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m10:31:48.938745 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m10:31:48.961682 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (compile): 10:31:48.928276 => 10:31:48.960963
[0m10:31:48.963086 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m10:31:48.972136 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m10:31:48.995187 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:48.996550 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"
[0m10:31:48.998000 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select first_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where first_activity_date is null



      
    ) dbt_internal_test
[0m10:31:48.999586 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:48.747899 [debug] [Thread-2  ]: SQL status: OK in -0.25 seconds
[0m10:31:48.756132 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac (execute): 10:31:48.964249 => 10:31:48.755549
[0m10:31:48.757463 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: ROLLBACK
[0m10:31:48.758562 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:48.759634 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac: Close
[0m10:31:48.963881 [info ] [Thread-2  ]: 24 of 35 PASS not_null_user_activity_first_activity_date ....................... [[32mPASS[0m in 0.04s]
[0m10:31:48.966320 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac
[0m10:31:48.967841 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m10:31:48.969853 [info ] [Thread-2  ]: 25 of 35 START test not_null_user_activity_influence_score ..................... [RUN]
[0m10:31:48.971991 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_first_activity_date.97cd5c21ac, now test.my_dbt_project.not_null_user_activity_influence_score.758852d325)
[0m10:31:48.973915 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m10:31:48.993402 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m10:31:49.023127 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (compile): 10:31:48.974863 => 10:31:49.022015
[0m10:31:49.025257 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m10:31:49.059932 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m10:31:49.092877 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:49.094734 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"
[0m10:31:49.096038 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_influence_score.758852d325"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select influence_score
from `workspace`.`movielens_volume`.`user_activity`
where influence_score is null



      
    ) dbt_internal_test
[0m10:31:49.100198 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:50.255315 [debug] [Thread-2  ]: SQL status: OK in 1.159999966621399 seconds
[0m10:31:50.262580 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_influence_score.758852d325 (execute): 10:31:49.027066 => 10:31:50.262057
[0m10:31:50.264111 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: ROLLBACK
[0m10:31:50.265327 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:50.266553 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_influence_score.758852d325: Close
[0m10:31:50.636462 [info ] [Thread-2  ]: 25 of 35 PASS not_null_user_activity_influence_score ........................... [[32mPASS[0m in 1.66s]
[0m10:31:50.638666 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_influence_score.758852d325
[0m10:31:50.640329 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m10:31:50.641525 [info ] [Thread-2  ]: 26 of 35 START test not_null_user_activity_last_activity_date .................. [RUN]
[0m10:31:50.644913 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_influence_score.758852d325, now test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad)
[0m10:31:50.646441 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m10:31:50.663718 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m10:31:50.683059 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (compile): 10:31:50.647465 => 10:31:50.682567
[0m10:31:50.684252 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m10:31:50.691704 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m10:31:50.721484 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:50.722572 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"
[0m10:31:50.723686 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select last_activity_date
from `workspace`.`movielens_volume`.`user_activity`
where last_activity_date is null



      
    ) dbt_internal_test
[0m10:31:50.724693 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:52.094663 [debug] [Thread-2  ]: SQL status: OK in 1.3700000047683716 seconds
[0m10:31:52.102179 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad (execute): 10:31:50.685138 => 10:31:52.101468
[0m10:31:52.103458 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: ROLLBACK
[0m10:31:52.104711 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:52.105786 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad: Close
[0m10:31:52.262163 [info ] [Thread-2  ]: 26 of 35 PASS not_null_user_activity_last_activity_date ........................ [[32mPASS[0m in 1.62s]
[0m10:31:52.264298 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad
[0m10:31:52.265978 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m10:31:52.267460 [info ] [Thread-2  ]: 27 of 35 START test not_null_user_activity_top_genre ........................... [RUN]
[0m10:31:52.269630 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_last_activity_date.ef19ea1cad, now test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313)
[0m10:31:52.270906 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m10:31:52.284605 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m10:31:52.307092 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (compile): 10:31:52.272052 => 10:31:52.306510
[0m10:31:52.308368 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m10:31:52.315647 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m10:31:52.349739 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:52.350896 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"
[0m10:31:52.352019 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select top_genre
from `workspace`.`movielens_volume`.`user_activity`
where top_genre is null



      
    ) dbt_internal_test
[0m10:31:52.353554 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:53.436816 [debug] [Thread-2  ]: SQL status: OK in 1.0800000429153442 seconds
[0m10:31:53.443432 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313 (execute): 10:31:52.309338 => 10:31:53.442923
[0m10:31:53.444738 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: ROLLBACK
[0m10:31:53.446199 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:53.447540 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313: Close
[0m10:31:53.604316 [info ] [Thread-2  ]: 27 of 35 PASS not_null_user_activity_top_genre ................................. [[32mPASS[0m in 1.34s]
[0m10:31:53.606697 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313
[0m10:31:53.608079 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m10:31:53.610150 [info ] [Thread-2  ]: 28 of 35 START test not_null_user_activity_user_id ............................. [RUN]
[0m10:31:53.612765 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_top_genre.5c4dd20313, now test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36)
[0m10:31:53.615006 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m10:31:53.631616 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m10:31:53.646748 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (compile): 10:31:53.616006 => 10:31:53.645979
[0m10:31:53.648137 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m10:31:53.658246 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m10:31:53.673823 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:53.674832 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"
[0m10:31:53.676081 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`user_activity`
where user_id is null



      
    ) dbt_internal_test
[0m10:31:53.677369 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:55.191515 [debug] [Thread-2  ]: SQL status: OK in 1.5099999904632568 seconds
[0m10:31:55.199475 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36 (execute): 10:31:53.649118 => 10:31:55.198927
[0m10:31:55.200683 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: ROLLBACK
[0m10:31:55.202002 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:55.203063 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36: Close
[0m10:31:55.404130 [info ] [Thread-2  ]: 28 of 35 PASS not_null_user_activity_user_id ................................... [[32mPASS[0m in 1.79s]
[0m10:31:55.406778 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36
[0m10:31:55.408180 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m10:31:55.409853 [info ] [Thread-2  ]: 29 of 35 START test not_null_user_activity_user_rating_count ................... [RUN]
[0m10:31:55.412008 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_id.d394f9fa36, now test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e)
[0m10:31:55.413682 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m10:31:55.429585 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m10:31:55.447962 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (compile): 10:31:55.414517 => 10:31:55.447209
[0m10:31:55.449268 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m10:31:55.456112 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m10:31:55.472306 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:55.473710 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"
[0m10:31:55.475338 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_rating_count
from `workspace`.`movielens_volume`.`user_activity`
where user_rating_count is null



      
    ) dbt_internal_test
[0m10:31:55.476971 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:56.904154 [debug] [Thread-2  ]: SQL status: OK in 1.4299999475479126 seconds
[0m10:31:56.913457 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e (execute): 10:31:55.450319 => 10:31:56.912633
[0m10:31:56.914891 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: ROLLBACK
[0m10:31:56.916838 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:56.918330 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e: Close
[0m10:31:57.066015 [info ] [Thread-2  ]: 29 of 35 PASS not_null_user_activity_user_rating_count ......................... [[32mPASS[0m in 1.65s]
[0m10:31:57.068177 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e
[0m10:31:57.069771 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:31:57.071187 [info ] [Thread-2  ]: 30 of 35 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m10:31:57.073440 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_user_activity_user_rating_count.3a9829e62e, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m10:31:57.074810 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:31:57.094481 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m10:31:57.115905 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 10:31:57.075975 => 10:31:57.115116
[0m10:31:57.117258 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:31:57.123582 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m10:31:57.144567 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:57.145840 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m10:31:57.147113 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m10:31:57.148362 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:58.206840 [debug] [Thread-2  ]: SQL status: OK in 1.059999942779541 seconds
[0m10:31:58.213706 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 10:31:57.118128 => 10:31:58.213070
[0m10:31:58.215261 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m10:31:58.216581 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:58.218068 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m10:31:58.420155 [info ] [Thread-2  ]: 30 of 35 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.35s]
[0m10:31:58.422539 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:31:58.423857 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m10:31:58.425062 [info ] [Thread-2  ]: 31 of 35 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m10:31:58.427096 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m10:31:58.428350 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m10:31:58.441392 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m10:31:58.461164 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 10:31:58.429474 => 10:31:58.460513
[0m10:31:58.473675 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m10:31:58.480229 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m10:31:58.499638 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:58.500719 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m10:31:58.501778 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m10:31:58.503101 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:31:59.529956 [debug] [Thread-2  ]: SQL status: OK in 1.0299999713897705 seconds
[0m10:31:59.536973 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 10:31:58.474815 => 10:31:59.536447
[0m10:31:59.538365 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m10:31:59.539787 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:31:59.540896 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m10:31:59.716345 [info ] [Thread-2  ]: 31 of 35 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.29s]
[0m10:31:59.718704 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m10:31:59.719927 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m10:31:59.721215 [info ] [Thread-2  ]: 32 of 35 START test unique_genre_popularity_genres ............................. [RUN]
[0m10:31:59.723585 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075)
[0m10:31:59.724714 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m10:31:59.744122 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m10:31:59.776298 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (compile): 10:31:59.725633 => 10:31:59.775485
[0m10:31:59.778535 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m10:31:59.786284 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m10:31:59.806236 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:31:59.807680 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"
[0m10:31:59.808633 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    genres as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`genre_popularity`
where genres is not null
group by genres
having count(*) > 1



      
    ) dbt_internal_test
[0m10:31:59.809951 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:32:00.927067 [debug] [Thread-2  ]: SQL status: OK in 1.1200000047683716 seconds
[0m10:32:00.933894 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075 (execute): 10:31:59.779537 => 10:32:00.933351
[0m10:32:00.935254 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: ROLLBACK
[0m10:32:00.936665 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:32:00.937720 [debug] [Thread-2  ]: On test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075: Close
[0m10:32:01.102311 [info ] [Thread-2  ]: 32 of 35 PASS unique_genre_popularity_genres ................................... [[32mPASS[0m in 1.38s]
[0m10:32:01.104325 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075
[0m10:32:01.105714 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m10:32:01.107062 [info ] [Thread-2  ]: 33 of 35 START test unique_movie_enriched_movie_id ............................. [RUN]
[0m10:32:01.108804 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_genre_popularity_genres.c4cc2a9075, now test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a)
[0m10:32:01.110048 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m10:32:01.119131 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m10:32:01.154060 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (compile): 10:32:01.110915 => 10:32:01.153446
[0m10:32:01.156263 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m10:32:01.164923 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m10:32:01.183733 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:32:01.184774 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m10:32:01.185803 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m10:32:01.186840 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:32:02.414072 [debug] [Thread-2  ]: SQL status: OK in 1.2300000190734863 seconds
[0m10:32:02.420266 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (execute): 10:32:01.157323 => 10:32:02.419790
[0m10:32:02.421397 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: ROLLBACK
[0m10:32:02.422496 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:32:02.423678 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m10:32:02.580438 [info ] [Thread-2  ]: 33 of 35 PASS unique_movie_enriched_movie_id ................................... [[32mPASS[0m in 1.47s]
[0m10:32:02.582816 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m10:32:02.584149 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m10:32:02.585361 [info ] [Thread-2  ]: 34 of 35 START test unique_movie_performance_movie_id .......................... [RUN]
[0m10:32:02.587613 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a, now test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22)
[0m10:32:02.588959 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m10:32:02.598703 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m10:32:02.621157 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (compile): 10:32:02.589809 => 10:32:02.620402
[0m10:32:02.622691 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m10:32:02.630287 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m10:32:02.664096 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:32:02.665332 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"
[0m10:32:02.666433 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_performance`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m10:32:02.667628 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:32:03.767921 [debug] [Thread-2  ]: SQL status: OK in 1.100000023841858 seconds
[0m10:32:03.786480 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22 (execute): 10:32:02.623777 => 10:32:03.784244
[0m10:32:03.787963 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: ROLLBACK
[0m10:32:03.789140 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:32:03.790247 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22: Close
[0m10:32:03.945963 [info ] [Thread-2  ]: 34 of 35 PASS unique_movie_performance_movie_id ................................ [[32mPASS[0m in 1.36s]
[0m10:32:03.948566 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22
[0m10:32:03.950248 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m10:32:03.951622 [info ] [Thread-2  ]: 35 of 35 START test unique_user_activity_user_id ............................... [RUN]
[0m10:32:03.954505 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_movie_performance_movie_id.c66e5c3a22, now test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda)
[0m10:32:03.956370 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m10:32:03.968284 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m10:32:03.988098 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (compile): 10:32:03.958497 => 10:32:03.987122
[0m10:32:03.989709 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m10:32:03.997600 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m10:32:04.016617 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:32:04.017977 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"
[0m10:32:04.019335 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`user_activity`
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m10:32:04.020618 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:32:05.391284 [debug] [Thread-2  ]: SQL status: OK in 1.3700000047683716 seconds
[0m10:32:05.405410 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda (execute): 10:32:03.990560 => 10:32:05.404463
[0m10:32:05.407406 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: ROLLBACK
[0m10:32:05.408690 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:32:05.409950 [debug] [Thread-2  ]: On test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda: Close
[0m10:32:05.570201 [info ] [Thread-2  ]: 35 of 35 PASS unique_user_activity_user_id ..................................... [[32mPASS[0m in 1.62s]
[0m10:32:05.572631 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda
[0m10:32:05.577319 [debug] [MainThread]: On master: ROLLBACK
[0m10:32:05.578495 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:32:06.153997 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:32:06.155473 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:32:06.156796 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:32:06.157694 [debug] [MainThread]: On master: ROLLBACK
[0m10:32:06.158852 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:32:06.160045 [debug] [MainThread]: On master: Close
[0m10:32:06.384925 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:32:06.386328 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_user_activity_user_id.46d4b3ebda' was properly closed.
[0m10:32:06.391547 [info ] [MainThread]: 
[0m10:32:06.393934 [info ] [MainThread]: Finished running 35 tests in 0 hours 0 minutes and 52.12 seconds (52.12s).
[0m10:32:06.407758 [debug] [MainThread]: Command end result
[0m10:32:06.462889 [info ] [MainThread]: 
[0m10:32:06.466424 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:32:06.468541 [info ] [MainThread]: 
[0m10:32:06.470359 [info ] [MainThread]: Done. PASS=35 WARN=0 ERROR=0 SKIP=0 TOTAL=35
[0m10:32:06.473237 [debug] [MainThread]: Command `dbt test` succeeded at 10:32:06.472902 after 58.59 seconds
[0m10:32:06.474500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9c2c3875b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9bff615c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9bff328460>]}
[0m10:32:06.475528 [debug] [MainThread]: Flushing usage events
[0m09:57:08.925233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ec2c2550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ea870160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ea870880>]}


============================== 09:57:08.935098 | 27c9dbfe-52ad-4cb4-bd46-6b38565dcf30 ==============================
[0m09:57:08.935098 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:57:08.936199 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/opt/airflow/dbt/logs', 'debug': 'False', 'profiles_dir': '/opt/airflow/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:57:10.489251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '27c9dbfe-52ad-4cb4-bd46-6b38565dcf30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ea870100>]}
[0m09:57:10.519891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '27c9dbfe-52ad-4cb4-bd46-6b38565dcf30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7bf5968b0>]}
[0m09:57:10.520877 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m09:57:10.578024 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:57:12.516368 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m09:57:12.517989 [debug] [MainThread]: Partial parsing: updated file: my_dbt_project://models/gold/genre_popularity.sql
[0m09:57:12.519241 [debug] [MainThread]: Partial parsing: updated file: my_dbt_project://models/gold/movie_performance.sql
[0m09:57:12.520188 [debug] [MainThread]: Partial parsing: updated file: my_dbt_project://models/gold/user_preferences.sql
[0m09:57:12.520977 [debug] [MainThread]: Partial parsing: updated file: my_dbt_project://models/gold/user_activity.sql
[0m09:57:12.565157 [debug] [MainThread]: 1699: static parser successfully parsed gold/genre_popularity.sql
[0m09:57:12.583590 [debug] [MainThread]: 1699: static parser successfully parsed gold/movie_performance.sql
[0m09:57:12.588670 [debug] [MainThread]: 1603: static parser failed on gold/user_preferences.sql
[0m09:57:12.602006 [debug] [MainThread]: 1602: parser fallback to jinja rendering on gold/user_preferences.sql
[0m09:57:12.605687 [debug] [MainThread]: 1699: static parser successfully parsed gold/user_activity.sql
[0m09:57:12.813321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '27c9dbfe-52ad-4cb4-bd46-6b38565dcf30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7bd9f7790>]}
[0m09:57:12.848570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '27c9dbfe-52ad-4cb4-bd46-6b38565dcf30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7bf463400>]}
[0m09:57:12.849580 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m09:57:12.850371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '27c9dbfe-52ad-4cb4-bd46-6b38565dcf30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7bf463640>]}
[0m09:57:12.852931 [info ] [MainThread]: 
[0m09:57:12.854816 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:57:12.856878 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m09:57:12.857531 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m09:57:12.858041 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m09:57:12.858608 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:58:13.905064 [debug] [ThreadPool]: SQL status: OK in 61.04999923706055 seconds
[0m09:58:13.910537 [debug] [ThreadPool]: On list_workspace: Close
[0m09:58:14.028402 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m09:58:14.030820 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m09:58:14.047260 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:14.048089 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m09:58:14.048774 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m09:58:14.049436 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:58:15.045451 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m09:58:15.047357 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m09:58:15.047993 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m09:58:15.048595 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m09:58:15.049153 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m09:58:15.159401 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m09:58:15.169269 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m09:58:15.169979 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m09:58:15.170506 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:58:15.946428 [debug] [ThreadPool]: SQL status: OK in 0.7799999713897705 seconds
[0m09:58:15.949956 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m09:58:16.077077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '27c9dbfe-52ad-4cb4-bd46-6b38565dcf30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ede8ad90>]}
[0m09:58:16.078085 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:16.078691 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:58:16.079960 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:58:16.080863 [info ] [MainThread]: 
[0m09:58:16.103977 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_links
[0m09:58:16.105053 [info ] [Thread-4  ]: 1 of 4 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m09:58:16.106302 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.silver_links)
[0m09:58:16.107026 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_links
[0m09:58:16.112090 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m09:58:16.121141 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (compile): 09:58:16.107474 => 09:58:16.120731
[0m09:58:16.121894 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_links
[0m09:58:16.146152 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:16.146980 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m09:58:16.147648 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

      describe extended `workspace`.`movielens_volume`.`silver_links`
  
[0m09:58:16.148260 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m09:58:17.962856 [debug] [Thread-4  ]: SQL status: OK in 1.809999942779541 seconds
[0m09:58:18.040923 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m09:58:18.050314 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m09:58:18.051172 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
    using delta
      
      
      
      
      
      
      as
      

with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m09:58:24.715642 [debug] [Thread-4  ]: SQL status: OK in 6.659999847412109 seconds
[0m09:58:24.947498 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (execute): 09:58:16.122408 => 09:58:24.946979
[0m09:58:24.948469 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: ROLLBACK
[0m09:58:24.949468 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m09:58:24.950214 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: Close
[0m09:58:25.067331 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27c9dbfe-52ad-4cb4-bd46-6b38565dcf30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7bc806c70>]}
[0m09:58:25.068650 [info ] [Thread-4  ]: 1 of 4 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 8.96s]
[0m09:58:25.069965 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_links
[0m09:58:25.070892 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_movies
[0m09:58:25.071752 [info ] [Thread-4  ]: 2 of 4 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m09:58:25.073055 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_links, now model.my_dbt_project.silver_movies)
[0m09:58:25.073772 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_movies
[0m09:58:25.079624 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m09:58:25.088539 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (compile): 09:58:25.074291 => 09:58:25.088100
[0m09:58:25.089332 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_movies
[0m09:58:25.095722 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:25.096457 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m09:58:25.097110 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

      describe extended `workspace`.`movielens_volume`.`silver_movies`
  
[0m09:58:25.097801 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m09:58:26.362078 [debug] [Thread-4  ]: SQL status: OK in 1.2599999904632568 seconds
[0m09:58:26.371497 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m09:58:26.379981 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m09:58:26.380830 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m09:58:30.250627 [debug] [Thread-4  ]: SQL status: OK in 3.869999885559082 seconds
[0m09:58:30.254666 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (execute): 09:58:25.089791 => 09:58:30.254395
[0m09:58:30.255507 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: ROLLBACK
[0m09:58:30.256226 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m09:58:30.256877 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: Close
[0m09:58:30.362239 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27c9dbfe-52ad-4cb4-bd46-6b38565dcf30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7bc86bd60>]}
[0m09:58:30.363410 [info ] [Thread-4  ]: 2 of 4 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 5.29s]
[0m09:58:30.364492 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_movies
[0m09:58:30.365177 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_ratings
[0m09:58:30.365923 [info ] [Thread-4  ]: 3 of 4 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m09:58:30.367172 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_movies, now model.my_dbt_project.silver_ratings)
[0m09:58:30.367847 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_ratings
[0m09:58:30.380024 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m09:58:30.389074 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (compile): 09:58:30.368277 => 09:58:30.388680
[0m09:58:30.389897 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_ratings
[0m09:58:30.396959 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:30.397778 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m09:58:30.398354 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

      describe extended `workspace`.`movielens_volume`.`silver_ratings`
  
[0m09:58:30.399018 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m09:58:31.743599 [debug] [Thread-4  ]: SQL status: OK in 1.340000033378601 seconds
[0m09:58:31.750177 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m09:58:31.757872 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m09:58:31.758617 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m09:58:39.109258 [debug] [Thread-4  ]: SQL status: OK in 7.349999904632568 seconds
[0m09:58:39.262691 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (execute): 09:58:30.390494 => 09:58:39.262399
[0m09:58:39.263680 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: ROLLBACK
[0m09:58:39.264375 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m09:58:39.265157 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: Close
[0m09:58:39.377236 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27c9dbfe-52ad-4cb4-bd46-6b38565dcf30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7bc86bd60>]}
[0m09:58:39.378300 [info ] [Thread-4  ]: 3 of 4 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 9.01s]
[0m09:58:39.379442 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_ratings
[0m09:58:39.380270 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_tags
[0m09:58:39.381096 [info ] [Thread-4  ]: 4 of 4 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m09:58:39.382242 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_ratings, now model.my_dbt_project.silver_tags)
[0m09:58:39.382962 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_tags
[0m09:58:39.389381 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m09:58:39.400640 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (compile): 09:58:39.383461 => 09:58:39.399940
[0m09:58:39.401558 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_tags
[0m09:58:39.410474 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:39.411539 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m09:58:39.412277 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

      describe extended `workspace`.`movielens_volume`.`silver_tags`
  
[0m09:58:39.413152 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m09:58:40.796446 [debug] [Thread-4  ]: SQL status: OK in 1.3799999952316284 seconds
[0m09:58:40.802597 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m09:58:40.811382 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m09:58:40.812176 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m09:58:44.960778 [debug] [Thread-4  ]: SQL status: OK in 4.150000095367432 seconds
[0m09:58:44.965280 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (execute): 09:58:39.402047 => 09:58:44.964921
[0m09:58:44.966081 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: ROLLBACK
[0m09:58:44.966873 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m09:58:44.967783 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: Close
[0m09:58:45.090669 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27c9dbfe-52ad-4cb4-bd46-6b38565dcf30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7bc86bd30>]}
[0m09:58:45.091889 [info ] [Thread-4  ]: 4 of 4 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 5.71s]
[0m09:58:45.093124 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_tags
[0m09:58:45.095994 [debug] [MainThread]: On master: ROLLBACK
[0m09:58:45.096616 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:58:45.443506 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:58:45.444349 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:45.444944 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:58:45.445632 [debug] [MainThread]: On master: ROLLBACK
[0m09:58:45.446247 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:58:45.446968 [debug] [MainThread]: On master: Close
[0m09:58:45.559373 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:58:45.560210 [debug] [MainThread]: Connection 'model.my_dbt_project.silver_tags' was properly closed.
[0m09:58:45.563251 [info ] [MainThread]: 
[0m09:58:45.564775 [info ] [MainThread]: Finished running 4 table models in 0 hours 1 minutes and 32.71 seconds (92.71s).
[0m09:58:45.567197 [debug] [MainThread]: Command end result
[0m09:58:45.601909 [info ] [MainThread]: 
[0m09:58:45.602831 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:58:45.603608 [info ] [MainThread]: 
[0m09:58:45.604415 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m09:58:45.605708 [debug] [MainThread]: Command `dbt run` succeeded at 09:58:45.605551 after 96.93 seconds
[0m09:58:45.606466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7ec2c2550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7bf5e1970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7bf7b6fa0>]}
[0m09:58:45.607092 [debug] [MainThread]: Flushing usage events
[0m09:58:50.050611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f2186b610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f1fdaf220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f1fdaf970>]}


============================== 09:58:50.057696 | bc93bc23-822c-43d6-bfb2-9c3edb7bd0dc ==============================
[0m09:58:50.057696 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:58:50.058546 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/opt/airflow/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:58:51.037114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc93bc23-822c-43d6-bfb2-9c3edb7bd0dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f1fdaf1c0>]}
[0m09:58:51.067314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bc93bc23-822c-43d6-bfb2-9c3edb7bd0dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ef8b16520>]}
[0m09:58:51.068290 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m09:58:51.110300 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:58:52.339669 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:58:52.340579 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:58:52.352077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc93bc23-822c-43d6-bfb2-9c3edb7bd0dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ef8abe400>]}
[0m09:58:52.386414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc93bc23-822c-43d6-bfb2-9c3edb7bd0dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ef89e8430>]}
[0m09:58:52.387409 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m09:58:52.388225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc93bc23-822c-43d6-bfb2-9c3edb7bd0dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ef89e8490>]}
[0m09:58:52.390797 [info ] [MainThread]: 
[0m09:58:52.392041 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:58:52.394899 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m09:58:52.395626 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m09:58:52.396182 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m09:58:52.396773 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:58:52.939344 [debug] [ThreadPool]: SQL status: OK in 0.5400000214576721 seconds
[0m09:58:52.941203 [debug] [ThreadPool]: On list_workspace: Close
[0m09:58:53.063308 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m09:58:53.064584 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m09:58:53.076980 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:53.077695 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m09:58:53.078329 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m09:58:53.078900 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:58:53.772431 [debug] [ThreadPool]: SQL status: OK in 0.6899999976158142 seconds
[0m09:58:53.774087 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m09:58:53.774710 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m09:58:53.775250 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m09:58:53.775802 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m09:58:53.892981 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m09:58:53.902398 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m09:58:53.903191 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m09:58:53.903937 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:58:54.521013 [debug] [ThreadPool]: SQL status: OK in 0.6200000047683716 seconds
[0m09:58:54.524307 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m09:58:54.641440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc93bc23-822c-43d6-bfb2-9c3edb7bd0dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f233eadf0>]}
[0m09:58:54.642496 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:54.643165 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:58:54.644133 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:58:54.644970 [info ] [MainThread]: 
[0m09:58:54.661029 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_enriched
[0m09:58:54.661981 [info ] [Thread-4  ]: 1 of 2 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m09:58:54.663170 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.movie_enriched)
[0m09:58:54.663831 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_enriched
[0m09:58:54.676721 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m09:58:54.685620 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (compile): 09:58:54.664455 => 09:58:54.685242
[0m09:58:54.686365 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_enriched
[0m09:58:54.707366 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:54.708115 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m09:58:54.708859 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

      describe extended `workspace`.`movielens_volume`.`movie_enriched`
  
[0m09:58:54.710015 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m09:58:56.170252 [debug] [Thread-4  ]: SQL status: OK in 1.4600000381469727 seconds
[0m09:58:56.229695 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m09:58:56.238366 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m09:58:56.239207 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m09:59:00.147829 [debug] [Thread-4  ]: SQL status: OK in 3.9100000858306885 seconds
[0m09:59:00.183211 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (execute): 09:58:54.686891 => 09:59:00.182911
[0m09:59:00.184125 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: ROLLBACK
[0m09:59:00.184937 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:00.185546 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: Close
[0m09:59:00.298006 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc93bc23-822c-43d6-bfb2-9c3edb7bd0dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ef8656d00>]}
[0m09:59:00.299210 [info ] [Thread-4  ]: 1 of 2 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 5.64s]
[0m09:59:00.300518 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_enriched
[0m09:59:00.301421 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_preferences
[0m09:59:00.302337 [info ] [Thread-4  ]: 2 of 2 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m09:59:00.303778 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_enriched, now model.my_dbt_project.user_preferences)
[0m09:59:00.304446 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_preferences
[0m09:59:00.319167 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m09:59:00.327933 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (compile): 09:59:00.304976 => 09:59:00.327499
[0m09:59:00.328628 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_preferences
[0m09:59:00.373100 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:00.373914 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m09:59:00.374568 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m09:59:00.375320 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m09:59:01.508141 [debug] [Thread-4  ]: SQL status: OK in 1.1299999952316284 seconds
[0m09:59:01.519051 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m09:59:01.519966 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create temporary view `user_preferences__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score, 
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )

  
[0m09:59:02.306514 [debug] [Thread-4  ]: SQL status: OK in 0.7900000214576721 seconds
[0m09:59:02.334561 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m09:59:02.335382 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m09:59:02.726389 [debug] [Thread-4  ]: SQL status: OK in 0.38999998569488525 seconds
[0m09:59:02.739364 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m09:59:02.748472 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m09:59:02.749214 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
      using `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m09:59:14.722781 [debug] [Thread-4  ]: SQL status: OK in 11.970000267028809 seconds
[0m09:59:14.848810 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (execute): 09:59:00.329078 => 09:59:14.848358
[0m09:59:14.849727 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: ROLLBACK
[0m09:59:14.850451 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:14.851220 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: Close
[0m09:59:14.963554 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc93bc23-822c-43d6-bfb2-9c3edb7bd0dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ef89e85b0>]}
[0m09:59:14.965108 [info ] [Thread-4  ]: 2 of 2 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 14.66s]
[0m09:59:14.966375 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_preferences
[0m09:59:14.969478 [debug] [MainThread]: On master: ROLLBACK
[0m09:59:14.970296 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:59:15.621698 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:59:15.622658 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:15.623288 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:59:15.624006 [debug] [MainThread]: On master: ROLLBACK
[0m09:59:15.624660 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:59:15.625357 [debug] [MainThread]: On master: Close
[0m09:59:15.747338 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:59:15.748149 [debug] [MainThread]: Connection 'model.my_dbt_project.user_preferences' was properly closed.
[0m09:59:15.750642 [info ] [MainThread]: 
[0m09:59:15.752125 [info ] [MainThread]: Finished running 1 table model, 1 incremental model in 0 hours 0 minutes and 23.36 seconds (23.36s).
[0m09:59:15.753713 [debug] [MainThread]: Command end result
[0m09:59:15.786853 [info ] [MainThread]: 
[0m09:59:15.787789 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:59:15.788488 [info ] [MainThread]: 
[0m09:59:15.789303 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m09:59:15.790524 [debug] [MainThread]: Command `dbt run` succeeded at 09:59:15.790384 after 25.81 seconds
[0m09:59:15.791197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f2186b610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ef89ee8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ef89ee640>]}
[0m09:59:15.791856 [debug] [MainThread]: Flushing usage events
[0m09:59:20.010163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54ee2c75b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54ec870220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54ec870940>]}


============================== 09:59:20.017850 | bb459388-afac-45f0-b038-b0834eb90528 ==============================
[0m09:59:20.017850 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:59:20.018701 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'profiles_dir': '/opt/airflow/dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:59:20.957762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bb459388-afac-45f0-b038-b0834eb90528', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54ec8701c0>]}
[0m09:59:20.988715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bb459388-afac-45f0-b038-b0834eb90528', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54c55abc40>]}
[0m09:59:20.989702 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m09:59:21.033697 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:59:22.187101 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:59:22.187900 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:59:22.200112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bb459388-afac-45f0-b038-b0834eb90528', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54c55603a0>]}
[0m09:59:22.236864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bb459388-afac-45f0-b038-b0834eb90528', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54c54923d0>]}
[0m09:59:22.237829 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m09:59:22.238607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bb459388-afac-45f0-b038-b0834eb90528', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54c5492430>]}
[0m09:59:22.242281 [info ] [MainThread]: 
[0m09:59:22.243943 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:59:22.246363 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m09:59:22.252997 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m09:59:22.253768 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m09:59:22.254338 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:59:22.901273 [debug] [ThreadPool]: SQL status: OK in 0.6499999761581421 seconds
[0m09:59:22.904644 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m09:59:23.017974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bb459388-afac-45f0-b038-b0834eb90528', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54c5492040>]}
[0m09:59:23.019007 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:23.019719 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:59:23.020906 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:59:23.021703 [info ] [MainThread]: 
[0m09:59:23.038031 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m09:59:23.039025 [info ] [Thread-2  ]: 1 of 22 START test accepted_rating_range_silver_ratings_rating__5__0 ........... [RUN]
[0m09:59:23.040268 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff)
[0m09:59:23.040934 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m09:59:23.055516 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m09:59:23.065724 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (compile): 09:59:23.041470 => 09:59:23.065282
[0m09:59:23.066709 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m09:59:23.096368 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m09:59:23.105869 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:23.106569 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m09:59:23.107248 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`silver_ratings`
where rating < 0
   or rating > 5


      
    ) dbt_internal_test
[0m09:59:23.108048 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:24.051256 [debug] [Thread-2  ]: SQL status: OK in 0.9399999976158142 seconds
[0m09:59:24.061043 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (execute): 09:59:23.067332 => 09:59:24.060686
[0m09:59:24.061823 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: ROLLBACK
[0m09:59:24.062536 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:24.063206 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: Close
[0m09:59:24.178702 [info ] [Thread-2  ]: 1 of 22 PASS accepted_rating_range_silver_ratings_rating__5__0 ................. [[32mPASS[0m in 1.14s]
[0m09:59:24.180005 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m09:59:24.180756 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m09:59:24.181556 [info ] [Thread-2  ]: 2 of 22 START test not_null_silver_links_imdb_id ............................... [RUN]
[0m09:59:24.182821 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff, now test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51)
[0m09:59:24.183462 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m09:59:24.193400 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m09:59:24.203109 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (compile): 09:59:24.183886 => 09:59:24.202717
[0m09:59:24.203916 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m09:59:24.208005 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m09:59:24.218001 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:24.218835 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m09:59:24.219467 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



      
    ) dbt_internal_test
[0m09:59:24.220206 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:25.194033 [debug] [Thread-2  ]: SQL status: OK in 0.9700000286102295 seconds
[0m09:59:25.198135 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (execute): 09:59:24.204388 => 09:59:25.197798
[0m09:59:25.199082 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: ROLLBACK
[0m09:59:25.199842 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:25.200607 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m09:59:25.313223 [info ] [Thread-2  ]: 2 of 22 PASS not_null_silver_links_imdb_id ..................................... [[32mPASS[0m in 1.13s]
[0m09:59:25.314615 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m09:59:25.315730 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m09:59:25.316529 [info ] [Thread-2  ]: 3 of 22 START test not_null_silver_links_movie_id .............................. [RUN]
[0m09:59:25.317872 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51, now test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d)
[0m09:59:25.318592 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m09:59:25.327629 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m09:59:25.337138 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (compile): 09:59:25.319194 => 09:59:25.336724
[0m09:59:25.337874 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m09:59:25.342022 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m09:59:25.351380 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:25.352037 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m09:59:25.352601 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



      
    ) dbt_internal_test
[0m09:59:25.353242 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:26.202893 [debug] [Thread-2  ]: SQL status: OK in 0.8500000238418579 seconds
[0m09:59:26.207010 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (execute): 09:59:25.338381 => 09:59:26.206682
[0m09:59:26.207759 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: ROLLBACK
[0m09:59:26.208441 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:26.209201 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m09:59:26.314180 [info ] [Thread-2  ]: 3 of 22 PASS not_null_silver_links_movie_id .................................... [[32mPASS[0m in 1.00s]
[0m09:59:26.315664 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m09:59:26.316490 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m09:59:26.317270 [info ] [Thread-2  ]: 4 of 22 START test not_null_silver_links_tmdb_id ............................... [RUN]
[0m09:59:26.318814 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d, now test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f)
[0m09:59:26.319696 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m09:59:26.332018 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m09:59:26.341955 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (compile): 09:59:26.320234 => 09:59:26.341531
[0m09:59:26.342762 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m09:59:26.346759 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m09:59:26.356348 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:26.357055 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m09:59:26.357721 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



      
    ) dbt_internal_test
[0m09:59:26.358370 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:27.183345 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m09:59:27.187538 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (execute): 09:59:26.343294 => 09:59:27.187208
[0m09:59:27.188331 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: ROLLBACK
[0m09:59:27.189173 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:27.189936 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m09:59:27.304875 [info ] [Thread-2  ]: 4 of 22 PASS not_null_silver_links_tmdb_id ..................................... [[32mPASS[0m in 0.99s]
[0m09:59:27.306324 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m09:59:27.307160 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m09:59:27.307953 [info ] [Thread-2  ]: 5 of 22 START test not_null_silver_movies_genres ............................... [RUN]
[0m09:59:27.309197 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f, now test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63)
[0m09:59:27.309897 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m09:59:27.318265 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m09:59:27.327706 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (compile): 09:59:27.310385 => 09:59:27.327354
[0m09:59:27.328442 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m09:59:27.332563 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m09:59:27.342093 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:27.342832 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m09:59:27.343483 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



      
    ) dbt_internal_test
[0m09:59:27.344138 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:28.181887 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m09:59:28.186183 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (execute): 09:59:27.329062 => 09:59:28.185853
[0m09:59:28.186980 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: ROLLBACK
[0m09:59:28.187791 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:28.188583 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m09:59:28.294635 [info ] [Thread-2  ]: 5 of 22 PASS not_null_silver_movies_genres ..................................... [[32mPASS[0m in 0.99s]
[0m09:59:28.296081 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m09:59:28.296972 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m09:59:28.297708 [info ] [Thread-2  ]: 6 of 22 START test not_null_silver_movies_movie_id ............................. [RUN]
[0m09:59:28.299032 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63, now test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e)
[0m09:59:28.299865 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m09:59:28.306686 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m09:59:28.316700 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (compile): 09:59:28.300423 => 09:59:28.316319
[0m09:59:28.317459 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m09:59:28.321627 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m09:59:28.331144 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:28.331772 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m09:59:28.332389 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



      
    ) dbt_internal_test
[0m09:59:28.333207 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:29.156527 [debug] [Thread-2  ]: SQL status: OK in 0.8199999928474426 seconds
[0m09:59:29.160887 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (execute): 09:59:28.317985 => 09:59:29.160510
[0m09:59:29.161732 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: ROLLBACK
[0m09:59:29.162554 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:29.163244 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m09:59:29.282643 [info ] [Thread-2  ]: 6 of 22 PASS not_null_silver_movies_movie_id ................................... [[32mPASS[0m in 0.98s]
[0m09:59:29.283872 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m09:59:29.284584 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m09:59:29.285306 [info ] [Thread-2  ]: 7 of 22 START test not_null_silver_movies_title ................................ [RUN]
[0m09:59:29.286638 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e, now test.my_dbt_project.not_null_silver_movies_title.3a779bd2de)
[0m09:59:29.287299 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m09:59:29.299051 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m09:59:29.313344 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (compile): 09:59:29.287781 => 09:59:29.312899
[0m09:59:29.314180 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m09:59:29.319360 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m09:59:29.330255 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:29.331107 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m09:59:29.331762 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



      
    ) dbt_internal_test
[0m09:59:29.332445 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:30.190306 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m09:59:30.194807 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (execute): 09:59:29.314687 => 09:59:30.194446
[0m09:59:30.195543 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: ROLLBACK
[0m09:59:30.196254 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:30.196880 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m09:59:30.310296 [info ] [Thread-2  ]: 7 of 22 PASS not_null_silver_movies_title ...................................... [[32mPASS[0m in 1.02s]
[0m09:59:30.311673 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m09:59:30.312573 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m09:59:30.313448 [info ] [Thread-2  ]: 8 of 22 START test not_null_silver_ratings_movie_id ............................ [RUN]
[0m09:59:30.314837 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_title.3a779bd2de, now test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36)
[0m09:59:30.315540 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m09:59:30.322030 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m09:59:30.331401 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (compile): 09:59:30.316042 => 09:59:30.331027
[0m09:59:30.332132 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m09:59:30.336237 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m09:59:30.345624 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:30.346271 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m09:59:30.346836 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



      
    ) dbt_internal_test
[0m09:59:30.347499 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:31.229984 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m09:59:31.240608 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (execute): 09:59:30.332670 => 09:59:31.239959
[0m09:59:31.241575 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: ROLLBACK
[0m09:59:31.242442 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:31.243170 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m09:59:31.355964 [info ] [Thread-2  ]: 8 of 22 PASS not_null_silver_ratings_movie_id .................................. [[32mPASS[0m in 1.04s]
[0m09:59:31.357368 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m09:59:31.358387 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m09:59:31.359503 [info ] [Thread-2  ]: 9 of 22 START test not_null_silver_ratings_rating .............................. [RUN]
[0m09:59:31.360871 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36, now test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a)
[0m09:59:31.361598 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m09:59:31.370465 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m09:59:31.379926 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (compile): 09:59:31.362318 => 09:59:31.379485
[0m09:59:31.380740 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m09:59:31.384974 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m09:59:31.394617 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:31.395268 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m09:59:31.395925 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



      
    ) dbt_internal_test
[0m09:59:31.396523 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:32.290300 [debug] [Thread-2  ]: SQL status: OK in 0.8899999856948853 seconds
[0m09:59:32.294793 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (execute): 09:59:31.381251 => 09:59:32.294458
[0m09:59:32.295646 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: ROLLBACK
[0m09:59:32.296371 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:32.297285 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m09:59:32.405133 [info ] [Thread-2  ]: 9 of 22 PASS not_null_silver_ratings_rating .................................... [[32mPASS[0m in 1.04s]
[0m09:59:32.406628 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m09:59:32.407661 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m09:59:32.408394 [info ] [Thread-2  ]: 10 of 22 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m09:59:32.409707 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a, now test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f)
[0m09:59:32.410402 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m09:59:32.424714 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m09:59:32.434554 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (compile): 09:59:32.410887 => 09:59:32.434116
[0m09:59:32.435322 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m09:59:32.440583 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m09:59:32.451231 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:32.451919 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m09:59:32.452472 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



      
    ) dbt_internal_test
[0m09:59:32.453079 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:33.613304 [debug] [Thread-2  ]: SQL status: OK in 1.159999966621399 seconds
[0m09:59:33.617686 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (execute): 09:59:32.435870 => 09:59:33.617260
[0m09:59:33.618761 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: ROLLBACK
[0m09:59:33.619639 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:33.620326 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m09:59:33.731398 [info ] [Thread-2  ]: 10 of 22 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 1.32s]
[0m09:59:33.732784 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m09:59:33.733595 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m09:59:33.734461 [info ] [Thread-2  ]: 11 of 22 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m09:59:33.735638 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f, now test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3)
[0m09:59:33.736260 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m09:59:33.745676 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m09:59:33.755870 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (compile): 09:59:33.736741 => 09:59:33.755400
[0m09:59:33.756675 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m09:59:33.760922 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m09:59:33.770657 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:33.771454 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m09:59:33.772091 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



      
    ) dbt_internal_test
[0m09:59:33.772730 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:34.580638 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m09:59:34.584699 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (execute): 09:59:33.757158 => 09:59:34.584317
[0m09:59:34.585538 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: ROLLBACK
[0m09:59:34.586299 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:34.587021 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m09:59:34.700187 [info ] [Thread-2  ]: 11 of 22 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 0.96s]
[0m09:59:34.701469 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m09:59:34.702275 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m09:59:34.702957 [info ] [Thread-2  ]: 12 of 22 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m09:59:34.704025 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3, now test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3)
[0m09:59:34.704722 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m09:59:34.713306 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m09:59:34.723190 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (compile): 09:59:34.705299 => 09:59:34.722745
[0m09:59:34.723983 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m09:59:34.728310 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m09:59:34.737930 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:34.738634 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m09:59:34.739281 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



      
    ) dbt_internal_test
[0m09:59:34.750744 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:35.582919 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m09:59:35.587029 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (execute): 09:59:34.724519 => 09:59:35.586673
[0m09:59:35.587810 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: ROLLBACK
[0m09:59:35.588425 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:35.589101 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m09:59:35.707561 [info ] [Thread-2  ]: 12 of 22 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 1.00s]
[0m09:59:35.712154 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m09:59:35.713102 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m09:59:35.713812 [info ] [Thread-2  ]: 13 of 22 START test not_null_silver_tags_tag ................................... [RUN]
[0m09:59:35.715118 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3, now test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8)
[0m09:59:35.715816 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m09:59:35.726264 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m09:59:35.736245 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (compile): 09:59:35.716268 => 09:59:35.735785
[0m09:59:35.737115 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m09:59:35.741214 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m09:59:35.750923 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:35.751698 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m09:59:35.752359 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



      
    ) dbt_internal_test
[0m09:59:35.752958 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:36.553894 [debug] [Thread-2  ]: SQL status: OK in 0.800000011920929 seconds
[0m09:59:36.557772 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (execute): 09:59:35.737708 => 09:59:36.557403
[0m09:59:36.558552 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: ROLLBACK
[0m09:59:36.559321 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:36.559887 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m09:59:36.675437 [info ] [Thread-2  ]: 13 of 22 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 0.96s]
[0m09:59:36.676937 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m09:59:36.677947 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m09:59:36.679005 [info ] [Thread-2  ]: 14 of 22 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m09:59:36.680434 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8, now test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86)
[0m09:59:36.681122 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m09:59:36.687569 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m09:59:36.697371 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (compile): 09:59:36.681599 => 09:59:36.696821
[0m09:59:36.698178 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m09:59:36.702406 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m09:59:36.712346 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:36.713047 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m09:59:36.713612 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



      
    ) dbt_internal_test
[0m09:59:36.714334 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:37.526989 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m09:59:37.531115 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (execute): 09:59:36.698726 => 09:59:37.530757
[0m09:59:37.532019 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: ROLLBACK
[0m09:59:37.532782 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:37.533439 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m09:59:37.643232 [info ] [Thread-2  ]: 14 of 22 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 0.96s]
[0m09:59:37.644682 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m09:59:37.645516 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m09:59:37.646299 [info ] [Thread-2  ]: 15 of 22 START test not_null_silver_tags_user_id ............................... [RUN]
[0m09:59:37.647459 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86, now test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3)
[0m09:59:37.648075 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m09:59:37.654035 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m09:59:37.664147 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (compile): 09:59:37.648565 => 09:59:37.663713
[0m09:59:37.664894 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m09:59:37.669302 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m09:59:37.679352 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:37.680434 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m09:59:37.681223 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



      
    ) dbt_internal_test
[0m09:59:37.681994 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:38.508677 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m09:59:38.512819 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (execute): 09:59:37.665696 => 09:59:38.512443
[0m09:59:38.513664 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: ROLLBACK
[0m09:59:38.514311 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:38.514920 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m09:59:38.621397 [info ] [Thread-2  ]: 15 of 22 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 0.97s]
[0m09:59:38.622730 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m09:59:38.623690 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m09:59:38.624428 [info ] [Thread-2  ]: 16 of 22 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m09:59:38.625618 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m09:59:38.626227 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m09:59:38.647793 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m09:59:38.658091 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 09:59:38.626725 => 09:59:38.657494
[0m09:59:38.658932 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m09:59:38.663010 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m09:59:38.673390 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:38.674120 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m09:59:38.674683 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m09:59:38.675305 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:39.838194 [debug] [Thread-2  ]: SQL status: OK in 1.159999966621399 seconds
[0m09:59:39.842672 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 09:59:38.659435 => 09:59:39.842280
[0m09:59:39.843576 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m09:59:39.844235 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:39.844890 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m09:59:39.962322 [info ] [Thread-2  ]: 16 of 22 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.34s]
[0m09:59:39.963898 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m09:59:39.964885 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m09:59:39.965694 [info ] [Thread-2  ]: 17 of 22 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m09:59:39.966973 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m09:59:39.967740 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m09:59:39.976154 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m09:59:39.986896 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 09:59:39.968265 => 09:59:39.986456
[0m09:59:39.987915 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m09:59:39.992862 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m09:59:40.003727 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:40.004478 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m09:59:40.005157 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m09:59:40.005836 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:41.934711 [debug] [Thread-2  ]: SQL status: OK in 1.9299999475479126 seconds
[0m09:59:41.939241 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 09:59:39.988410 => 09:59:41.938911
[0m09:59:41.940071 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m09:59:41.940806 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:41.941410 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m09:59:42.057869 [info ] [Thread-2  ]: 17 of 22 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 2.09s]
[0m09:59:42.059669 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m09:59:42.060583 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m09:59:42.061309 [info ] [Thread-2  ]: 18 of 22 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m09:59:42.062556 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32)
[0m09:59:42.063119 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m09:59:42.071237 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m09:59:42.080031 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (compile): 09:59:42.063564 => 09:59:42.079562
[0m09:59:42.080817 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m09:59:42.085178 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m09:59:42.094512 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:42.095260 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m09:59:42.096021 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m09:59:42.096699 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:43.338204 [debug] [Thread-2  ]: SQL status: OK in 1.2400000095367432 seconds
[0m09:59:43.342574 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (execute): 09:59:42.081332 => 09:59:43.342215
[0m09:59:43.343417 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: ROLLBACK
[0m09:59:43.344245 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:43.345085 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m09:59:43.455470 [info ] [Thread-2  ]: 18 of 22 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.39s]
[0m09:59:43.457042 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m09:59:43.457878 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m09:59:43.458675 [info ] [Thread-2  ]: 19 of 22 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m09:59:43.460005 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32, now test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd)
[0m09:59:43.460822 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m09:59:43.477323 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m09:59:43.487028 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (compile): 09:59:43.461346 => 09:59:43.486670
[0m09:59:43.487778 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m09:59:43.492011 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m09:59:43.502098 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:43.502942 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m09:59:43.503602 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m09:59:43.504232 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:44.816705 [debug] [Thread-2  ]: SQL status: OK in 1.309999942779541 seconds
[0m09:59:44.820981 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (execute): 09:59:43.488311 => 09:59:44.820655
[0m09:59:44.821773 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: ROLLBACK
[0m09:59:44.822705 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:44.823417 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m09:59:44.940476 [info ] [Thread-2  ]: 19 of 22 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.48s]
[0m09:59:44.941937 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m09:59:44.942828 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m09:59:44.943561 [info ] [Thread-2  ]: 20 of 22 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m09:59:44.944785 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd, now test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719)
[0m09:59:44.945496 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m09:59:44.954200 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m09:59:44.964073 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (compile): 09:59:44.946104 => 09:59:44.963718
[0m09:59:44.964774 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m09:59:44.968775 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m09:59:44.978235 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:44.978866 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m09:59:44.979535 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m09:59:44.980287 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:46.220591 [debug] [Thread-2  ]: SQL status: OK in 1.2400000095367432 seconds
[0m09:59:46.224539 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (execute): 09:59:44.965264 => 09:59:46.224242
[0m09:59:46.225391 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: ROLLBACK
[0m09:59:46.226340 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:46.226956 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m09:59:46.339328 [info ] [Thread-2  ]: 20 of 22 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 1.39s]
[0m09:59:46.340733 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m09:59:46.341596 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m09:59:46.342487 [info ] [Thread-2  ]: 21 of 22 START test unique_silver_links_movie_id ............................... [RUN]
[0m09:59:46.344026 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719, now test.my_dbt_project.unique_silver_links_movie_id.c422609054)
[0m09:59:46.344703 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m09:59:46.353910 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m09:59:46.363353 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (compile): 09:59:46.345193 => 09:59:46.362975
[0m09:59:46.364152 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m09:59:46.368052 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m09:59:46.377572 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:46.378266 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m09:59:46.378974 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m09:59:46.379641 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:47.172808 [debug] [Thread-2  ]: SQL status: OK in 0.7900000214576721 seconds
[0m09:59:47.182518 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (execute): 09:59:46.364664 => 09:59:47.182064
[0m09:59:47.183511 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: ROLLBACK
[0m09:59:47.184545 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:47.185267 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m09:59:47.297304 [info ] [Thread-2  ]: 21 of 22 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 0.95s]
[0m09:59:47.298793 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m09:59:47.299664 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m09:59:47.300491 [info ] [Thread-2  ]: 22 of 22 START test unique_silver_movies_movie_id .............................. [RUN]
[0m09:59:47.301814 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_silver_links_movie_id.c422609054, now test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085)
[0m09:59:47.302726 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m09:59:47.310916 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m09:59:47.320716 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (compile): 09:59:47.303296 => 09:59:47.320299
[0m09:59:47.321406 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m09:59:47.336575 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m09:59:47.346965 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:47.347660 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m09:59:47.348469 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m09:59:47.349075 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:48.211925 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m09:59:48.216067 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (execute): 09:59:47.321921 => 09:59:48.215705
[0m09:59:48.216877 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: ROLLBACK
[0m09:59:48.217663 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:48.218476 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m09:59:48.330011 [info ] [Thread-2  ]: 22 of 22 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 1.03s]
[0m09:59:48.331458 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m09:59:48.334778 [debug] [MainThread]: On master: ROLLBACK
[0m09:59:48.335403 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:59:48.680853 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:59:48.681768 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:48.682427 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:59:48.683047 [debug] [MainThread]: On master: ROLLBACK
[0m09:59:48.683773 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:59:48.684384 [debug] [MainThread]: On master: Close
[0m09:59:48.801491 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:59:48.802316 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085' was properly closed.
[0m09:59:48.804907 [info ] [MainThread]: 
[0m09:59:48.806435 [info ] [MainThread]: Finished running 22 tests in 0 hours 0 minutes and 26.56 seconds (26.56s).
[0m09:59:48.812586 [debug] [MainThread]: Command end result
[0m09:59:48.841446 [info ] [MainThread]: 
[0m09:59:48.842580 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:59:48.843364 [info ] [MainThread]: 
[0m09:59:48.844247 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=0 SKIP=0 TOTAL=22
[0m09:59:48.845805 [debug] [MainThread]: Command `dbt test` succeeded at 09:59:48.845657 after 28.91 seconds
[0m09:59:48.846570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54ee2c75b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54c55abc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54c55e8e80>]}
[0m09:59:48.847464 [debug] [MainThread]: Flushing usage events
[0m09:59:53.346146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe894207580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8927af190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8927af8e0>]}


============================== 09:59:53.353484 | 3a1e279d-f72a-4fbd-8905-cd16b4c88445 ==============================
[0m09:59:53.353484 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:59:53.354319 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:59:54.295286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3a1e279d-f72a-4fbd-8905-cd16b4c88445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8927af130>]}
[0m09:59:54.326960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3a1e279d-f72a-4fbd-8905-cd16b4c88445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8674dab80>]}
[0m09:59:54.328019 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m09:59:54.372264 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:59:55.653818 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:59:55.654539 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:59:55.665992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3a1e279d-f72a-4fbd-8905-cd16b4c88445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe867495370>]}
[0m09:59:55.699304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3a1e279d-f72a-4fbd-8905-cd16b4c88445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8673c73a0>]}
[0m09:59:55.700220 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m09:59:55.701075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a1e279d-f72a-4fbd-8905-cd16b4c88445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8673c7400>]}
[0m09:59:55.703743 [info ] [MainThread]: 
[0m09:59:55.705099 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:59:55.707753 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m09:59:55.714140 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m09:59:55.714872 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m09:59:55.715444 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:59:56.346805 [debug] [ThreadPool]: SQL status: OK in 0.6299999952316284 seconds
[0m09:59:56.349976 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m09:59:56.469564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a1e279d-f72a-4fbd-8905-cd16b4c88445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8673c7160>]}
[0m09:59:56.470718 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:56.471386 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:59:56.472425 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:59:56.473334 [info ] [MainThread]: 
[0m09:59:56.492205 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m09:59:56.493160 [info ] [Thread-2  ]: 1 of 8 START test accepted_rating_range_movie_enriched_avg_rating__5__0 ........ [RUN]
[0m09:59:56.494585 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8)
[0m09:59:56.495222 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m09:59:56.506709 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m09:59:56.514852 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (compile): 09:59:56.495739 => 09:59:56.514444
[0m09:59:56.515595 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m09:59:56.544736 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m09:59:56.552974 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:56.553602 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m09:59:56.554171 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m09:59:56.554859 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:57.392060 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m09:59:57.400402 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (execute): 09:59:56.516069 => 09:59:57.400084
[0m09:59:57.401168 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: ROLLBACK
[0m09:59:57.401856 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:57.402413 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: Close
[0m09:59:57.518211 [info ] [Thread-2  ]: 1 of 8 PASS accepted_rating_range_movie_enriched_avg_rating__5__0 .............. [[32mPASS[0m in 1.02s]
[0m09:59:57.519673 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m09:59:57.520653 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m09:59:57.521505 [info ] [Thread-2  ]: 2 of 8 START test not_null_movie_enriched_avg_rating ........................... [RUN]
[0m09:59:57.522740 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8, now test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd)
[0m09:59:57.523493 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m09:59:57.532942 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m09:59:57.542554 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (compile): 09:59:57.523988 => 09:59:57.542203
[0m09:59:57.543266 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m09:59:57.548083 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m09:59:57.557514 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:57.558219 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m09:59:57.558775 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating is null



      
    ) dbt_internal_test
[0m09:59:57.559437 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:58.326201 [debug] [Thread-2  ]: SQL status: OK in 0.7699999809265137 seconds
[0m09:59:58.330285 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (execute): 09:59:57.543781 => 09:59:58.329949
[0m09:59:58.331165 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: ROLLBACK
[0m09:59:58.332029 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:58.332939 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: Close
[0m09:59:58.448919 [info ] [Thread-2  ]: 2 of 8 PASS not_null_movie_enriched_avg_rating ................................. [[32mPASS[0m in 0.93s]
[0m09:59:58.450336 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m09:59:58.451203 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m09:59:58.452001 [info ] [Thread-2  ]: 3 of 8 START test not_null_movie_enriched_genres ............................... [RUN]
[0m09:59:58.453654 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd, now test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27)
[0m09:59:58.454556 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m09:59:58.463854 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m09:59:58.474041 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (compile): 09:59:58.455178 => 09:59:58.473620
[0m09:59:58.474815 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m09:59:58.479119 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m09:59:58.488428 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:58.489080 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m09:59:58.489748 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`movie_enriched`
where genres is null



      
    ) dbt_internal_test
[0m09:59:58.490473 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m09:59:59.321074 [debug] [Thread-2  ]: SQL status: OK in 0.8299999833106995 seconds
[0m09:59:59.325238 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (execute): 09:59:58.475386 => 09:59:59.324850
[0m09:59:59.326154 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: ROLLBACK
[0m09:59:59.326882 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m09:59:59.327537 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: Close
[0m09:59:59.441718 [info ] [Thread-2  ]: 3 of 8 PASS not_null_movie_enriched_genres ..................................... [[32mPASS[0m in 0.99s]
[0m09:59:59.443210 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m09:59:59.444157 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m09:59:59.444904 [info ] [Thread-2  ]: 4 of 8 START test not_null_movie_enriched_movie_id ............................. [RUN]
[0m09:59:59.446350 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27, now test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86)
[0m09:59:59.447137 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m09:59:59.461548 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m09:59:59.470629 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (compile): 09:59:59.447721 => 09:59:59.470166
[0m09:59:59.471360 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m09:59:59.478148 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m09:59:59.487568 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:59:59.488252 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m09:59:59.488836 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is null



      
    ) dbt_internal_test
[0m09:59:59.489740 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:00:00.484516 [debug] [Thread-2  ]: SQL status: OK in 0.9900000095367432 seconds
[0m10:00:00.488602 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (execute): 09:59:59.471845 => 10:00:00.488272
[0m10:00:00.489375 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: ROLLBACK
[0m10:00:00.490107 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:00:00.490874 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: Close
[0m10:00:00.615580 [info ] [Thread-2  ]: 4 of 8 PASS not_null_movie_enriched_movie_id ................................... [[32mPASS[0m in 1.17s]
[0m10:00:00.616947 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m10:00:00.617744 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m10:00:00.618592 [info ] [Thread-2  ]: 5 of 8 START test not_null_movie_enriched_title ................................ [RUN]
[0m10:00:00.620041 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86, now test.my_dbt_project.not_null_movie_enriched_title.d20f00c292)
[0m10:00:00.620981 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m10:00:00.629991 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m10:00:00.639777 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (compile): 10:00:00.621525 => 10:00:00.639325
[0m10:00:00.640550 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m10:00:00.644508 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m10:00:00.654483 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:00:00.655154 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m10:00:00.655974 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`movie_enriched`
where title is null



      
    ) dbt_internal_test
[0m10:00:00.656650 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:00:01.514659 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m10:00:01.518852 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (execute): 10:00:00.641085 => 10:00:01.518498
[0m10:00:01.519637 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: ROLLBACK
[0m10:00:01.520317 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:00:01.520974 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: Close
[0m10:00:01.650146 [info ] [Thread-2  ]: 5 of 8 PASS not_null_movie_enriched_title ...................................... [[32mPASS[0m in 1.03s]
[0m10:00:01.651758 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m10:00:01.652878 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m10:00:01.653651 [info ] [Thread-2  ]: 6 of 8 START test not_null_movie_enriched_total_ratings ........................ [RUN]
[0m10:00:01.654856 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_title.d20f00c292, now test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04)
[0m10:00:01.655521 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m10:00:01.661854 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m10:00:01.671761 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (compile): 10:00:01.656032 => 10:00:01.671345
[0m10:00:01.672501 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m10:00:01.676521 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m10:00:01.686665 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:00:01.687354 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m10:00:01.688014 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select total_ratings
from `workspace`.`movielens_volume`.`movie_enriched`
where total_ratings is null



      
    ) dbt_internal_test
[0m10:00:01.688730 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:00:02.497989 [debug] [Thread-2  ]: SQL status: OK in 0.8100000023841858 seconds
[0m10:00:02.502337 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (execute): 10:00:01.672949 => 10:00:02.502005
[0m10:00:02.503230 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: ROLLBACK
[0m10:00:02.503997 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:00:02.504711 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: Close
[0m10:00:02.605937 [info ] [Thread-2  ]: 6 of 8 PASS not_null_movie_enriched_total_ratings .............................. [[32mPASS[0m in 0.95s]
[0m10:00:02.607309 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m10:00:02.608106 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:00:02.608900 [info ] [Thread-2  ]: 7 of 8 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m10:00:02.610473 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m10:00:02.611161 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:00:02.630278 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m10:00:02.640550 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 10:00:02.611803 => 10:00:02.640164
[0m10:00:02.641389 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:00:02.645523 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m10:00:02.654768 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:00:02.655426 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m10:00:02.656005 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m10:00:02.656749 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:00:03.326102 [debug] [Thread-2  ]: SQL status: OK in 0.6700000166893005 seconds
[0m10:00:03.330300 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 10:00:02.642033 => 10:00:03.329955
[0m10:00:03.331141 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m10:00:03.331919 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:00:03.332559 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m10:00:03.459582 [info ] [Thread-2  ]: 7 of 8 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.85s]
[0m10:00:03.460889 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m10:00:03.461742 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m10:00:03.462434 [info ] [Thread-2  ]: 8 of 8 START test unique_movie_enriched_movie_id ............................... [RUN]
[0m10:00:03.463504 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a)
[0m10:00:03.464077 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m10:00:03.472769 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m10:00:03.482224 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (compile): 10:00:03.464519 => 10:00:03.481880
[0m10:00:03.482908 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m10:00:03.487128 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m10:00:03.497181 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m10:00:03.497953 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m10:00:03.498680 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m10:00:03.499466 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m10:00:04.402046 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m10:00:04.406300 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (execute): 10:00:03.483554 => 10:00:04.405959
[0m10:00:04.407147 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: ROLLBACK
[0m10:00:04.407945 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m10:00:04.408610 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m10:00:04.522855 [info ] [Thread-2  ]: 8 of 8 PASS unique_movie_enriched_movie_id ..................................... [[32mPASS[0m in 1.06s]
[0m10:00:04.524328 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m10:00:04.527001 [debug] [MainThread]: On master: ROLLBACK
[0m10:00:04.527584 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:00:04.884565 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:00:04.885465 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:00:04.886202 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:00:04.886882 [debug] [MainThread]: On master: ROLLBACK
[0m10:00:04.887543 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:00:04.888216 [debug] [MainThread]: On master: Close
[0m10:00:04.998682 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:00:04.999578 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a' was properly closed.
[0m10:00:05.001982 [info ] [MainThread]: 
[0m10:00:05.003382 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 9.30 seconds (9.30s).
[0m10:00:05.006044 [debug] [MainThread]: Command end result
[0m10:00:05.037029 [info ] [MainThread]: 
[0m10:00:05.037999 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:00:05.038736 [info ] [MainThread]: 
[0m10:00:05.039489 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8
[0m10:00:05.040586 [debug] [MainThread]: Command `dbt test` succeeded at 10:00:05.040420 after 11.72 seconds
[0m10:00:05.041190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe894207580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8674dab80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe86615bfa0>]}
[0m10:00:05.041809 [debug] [MainThread]: Flushing usage events
[0m08:23:04.296838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9783a43580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9781fef190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9781fef8e0>]}


============================== 08:23:04.308028 | 52018fea-456b-4d60-84d3-f74176c10a04 ==============================
[0m08:23:04.308028 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:23:04.309198 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/dbt/logs', 'profiles_dir': '/opt/airflow/dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:23:05.898651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '52018fea-456b-4d60-84d3-f74176c10a04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9781fef130>]}
[0m08:23:05.931121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '52018fea-456b-4d60-84d3-f74176c10a04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9756d1eac0>]}
[0m08:23:05.932251 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:23:05.994667 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:23:08.071147 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:23:08.071961 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:23:08.084175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '52018fea-456b-4d60-84d3-f74176c10a04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9756cc4370>]}
[0m08:23:08.125482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '52018fea-456b-4d60-84d3-f74176c10a04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9756bf03a0>]}
[0m08:23:08.126404 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:23:08.127168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52018fea-456b-4d60-84d3-f74176c10a04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9756bf0400>]}
[0m08:23:08.130071 [info ] [MainThread]: 
[0m08:23:08.131620 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:23:08.134241 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:23:08.135218 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:23:08.135825 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m08:23:08.136395 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:24:09.483086 [debug] [ThreadPool]: SQL status: OK in 61.349998474121094 seconds
[0m08:24:09.487627 [debug] [ThreadPool]: On list_workspace: Close
[0m08:24:09.632452 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m08:24:09.634550 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m08:24:09.653924 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m08:24:09.654736 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m08:24:09.655532 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m08:24:09.656243 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:24:11.997478 [debug] [ThreadPool]: SQL status: OK in 2.3399999141693115 seconds
[0m08:24:11.999660 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m08:24:12.000467 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m08:24:12.001084 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m08:24:12.001674 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m08:24:12.163024 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m08:24:12.173984 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:24:12.174762 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:24:12.175371 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:24:13.546095 [debug] [ThreadPool]: SQL status: OK in 1.3700000047683716 seconds
[0m08:24:13.550194 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:24:13.673717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52018fea-456b-4d60-84d3-f74176c10a04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9756bf0250>]}
[0m08:24:13.675287 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:24:13.676002 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:24:13.678809 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:24:13.679785 [info ] [MainThread]: 
[0m08:24:13.701649 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_links
[0m08:24:13.702757 [info ] [Thread-4  ]: 1 of 4 START sql table model movielens_volume.silver_links ..................... [RUN]
[0m08:24:13.704030 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.silver_links)
[0m08:24:13.704872 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_links
[0m08:24:13.720188 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_links"
[0m08:24:13.732466 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (compile): 08:24:13.705502 => 08:24:13.732019
[0m08:24:13.733400 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_links
[0m08:24:13.755915 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:24:13.756756 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m08:24:13.757378 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

      describe extended `workspace`.`movielens_volume`.`silver_links`
  
[0m08:24:13.758114 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:24:17.478632 [debug] [Thread-4  ]: SQL status: OK in 3.7200000286102295 seconds
[0m08:24:17.562127 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_links"
[0m08:24:17.571838 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_links"
[0m08:24:17.572670 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_links"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_links`
      
      
    using delta
      
      
      
      
      
      
      as
      

with deduplicated as (

    select
        movie_id,
        imdbId as imdb_id,
        tmdbId as tmdb_id,
        row_number() over (partition by movie_id order by imdbId, tmdbId) as rn
    from `workspace`.`movielens_volume`.`bronze_links`
    where imdbId is not null
      and tmdbId is not null

)

select
    movie_id,
    imdb_id,
    tmdb_id
from deduplicated
where rn = 1
  
[0m08:24:27.758968 [debug] [Thread-4  ]: SQL status: OK in 10.1899995803833 seconds
[0m08:24:27.950385 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_links (execute): 08:24:13.734066 => 08:24:27.950115
[0m08:24:27.951284 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: ROLLBACK
[0m08:24:27.952064 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:24:27.952962 [debug] [Thread-4  ]: On model.my_dbt_project.silver_links: Close
[0m08:24:28.067616 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52018fea-456b-4d60-84d3-f74176c10a04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9756a1df10>]}
[0m08:24:28.068872 [info ] [Thread-4  ]: 1 of 4 OK created sql table model movielens_volume.silver_links ................ [[32mOK[0m in 14.36s]
[0m08:24:28.070067 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_links
[0m08:24:28.070852 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_movies
[0m08:24:28.071613 [info ] [Thread-4  ]: 2 of 4 START sql table model movielens_volume.silver_movies .................... [RUN]
[0m08:24:28.072860 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_links, now model.my_dbt_project.silver_movies)
[0m08:24:28.073515 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_movies
[0m08:24:28.078705 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_movies"
[0m08:24:28.088325 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (compile): 08:24:28.073952 => 08:24:28.087858
[0m08:24:28.089180 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_movies
[0m08:24:28.101032 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:24:28.101893 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m08:24:28.102525 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

      describe extended `workspace`.`movielens_volume`.`silver_movies`
  
[0m08:24:28.103248 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:24:29.475161 [debug] [Thread-4  ]: SQL status: OK in 1.3700000047683716 seconds
[0m08:24:29.481843 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_movies"
[0m08:24:29.491811 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_movies"
[0m08:24:29.492685 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_movies"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_movies`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    movie_id,
    trim(title) as title,
    replace(trim(genres), '|', ' | ') as genres
from `workspace`.`movielens_volume`.`bronze_movies`
where movie_id is not null
  and movie_id > 0
  and title is not null and trim(title) <> ''
  and genres is not null and trim(genres) <> ''
  
[0m08:24:33.659106 [debug] [Thread-4  ]: SQL status: OK in 4.170000076293945 seconds
[0m08:24:33.663191 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_movies (execute): 08:24:28.089874 => 08:24:33.662895
[0m08:24:33.664028 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: ROLLBACK
[0m08:24:33.664726 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:24:33.665385 [debug] [Thread-4  ]: On model.my_dbt_project.silver_movies: Close
[0m08:24:33.787321 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52018fea-456b-4d60-84d3-f74176c10a04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9756a502e0>]}
[0m08:24:33.788566 [info ] [Thread-4  ]: 2 of 4 OK created sql table model movielens_volume.silver_movies ............... [[32mOK[0m in 5.71s]
[0m08:24:33.790071 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_movies
[0m08:24:33.791068 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_ratings
[0m08:24:33.791994 [info ] [Thread-4  ]: 3 of 4 START sql table model movielens_volume.silver_ratings ................... [RUN]
[0m08:24:33.793273 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_movies, now model.my_dbt_project.silver_ratings)
[0m08:24:33.793929 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_ratings
[0m08:24:33.799876 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_ratings"
[0m08:24:33.809395 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (compile): 08:24:33.794466 => 08:24:33.808936
[0m08:24:33.810120 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_ratings
[0m08:24:33.816551 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:24:33.817317 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m08:24:33.817907 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

      describe extended `workspace`.`movielens_volume`.`silver_ratings`
  
[0m08:24:33.818757 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:24:35.108725 [debug] [Thread-4  ]: SQL status: OK in 1.2899999618530273 seconds
[0m08:24:35.115644 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_ratings"
[0m08:24:35.125344 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_ratings"
[0m08:24:35.126137 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_ratings"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_ratings`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    round(rating, 2) as rating,           
    to_date(cast(rated_at as timestamp)) as rating_date  
from `workspace`.`movielens_volume`.`bronze_ratings`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and rating is not null
  and rating >= 0
  and rating <= 5
  
[0m08:24:41.753211 [debug] [Thread-4  ]: SQL status: OK in 6.630000114440918 seconds
[0m08:24:41.931963 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_ratings (execute): 08:24:33.810580 => 08:24:41.931587
[0m08:24:41.932882 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: ROLLBACK
[0m08:24:41.933593 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:24:41.934412 [debug] [Thread-4  ]: On model.my_dbt_project.silver_ratings: Close
[0m08:24:42.060189 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52018fea-456b-4d60-84d3-f74176c10a04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f975588c490>]}
[0m08:24:42.062087 [info ] [Thread-4  ]: 3 of 4 OK created sql table model movielens_volume.silver_ratings .............. [[32mOK[0m in 8.27s]
[0m08:24:42.063464 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_ratings
[0m08:24:42.064514 [debug] [Thread-4  ]: Began running node model.my_dbt_project.silver_tags
[0m08:24:42.065899 [info ] [Thread-4  ]: 4 of 4 START sql table model movielens_volume.silver_tags ...................... [RUN]
[0m08:24:42.067688 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.silver_ratings, now model.my_dbt_project.silver_tags)
[0m08:24:42.068450 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.silver_tags
[0m08:24:42.087689 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.silver_tags"
[0m08:24:42.097661 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (compile): 08:24:42.068937 => 08:24:42.097004
[0m08:24:42.098464 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.silver_tags
[0m08:24:42.106647 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:24:42.107485 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m08:24:42.108231 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

      describe extended `workspace`.`movielens_volume`.`silver_tags`
  
[0m08:24:42.109078 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:24:43.638655 [debug] [Thread-4  ]: SQL status: OK in 1.5299999713897705 seconds
[0m08:24:43.657631 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.silver_tags"
[0m08:24:43.667846 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.silver_tags"
[0m08:24:43.668685 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.silver_tags"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`silver_tags`
      
      
    using delta
      
      
      
      
      
      
      as
      

select
    user_id,
    movie_id,
    trim(tag) as tag,
    to_date(cast(tagged_at as timestamp)) as tagged_date  
from `workspace`.`movielens_volume`.`bronze_tags`
where user_id is not null
  and user_id > 0
  and movie_id is not null
  and movie_id > 0
  and tag is not null
  and trim(tag) <> ''
  and tagged_at is not null
  
[0m08:24:48.830082 [debug] [Thread-4  ]: SQL status: OK in 5.159999847412109 seconds
[0m08:24:48.835246 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.silver_tags (execute): 08:24:42.099051 => 08:24:48.834922
[0m08:24:48.836189 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: ROLLBACK
[0m08:24:48.836939 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:24:48.837656 [debug] [Thread-4  ]: On model.my_dbt_project.silver_tags: Close
[0m08:24:48.959982 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52018fea-456b-4d60-84d3-f74176c10a04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f975598d550>]}
[0m08:24:48.961523 [info ] [Thread-4  ]: 4 of 4 OK created sql table model movielens_volume.silver_tags ................. [[32mOK[0m in 6.89s]
[0m08:24:48.963011 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.silver_tags
[0m08:24:48.967462 [debug] [MainThread]: On master: ROLLBACK
[0m08:24:48.968467 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:24:49.391876 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:24:49.392786 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:24:49.393513 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:24:49.394394 [debug] [MainThread]: On master: ROLLBACK
[0m08:24:49.395051 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:24:49.395856 [debug] [MainThread]: On master: Close
[0m08:24:49.517912 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:24:49.519137 [debug] [MainThread]: Connection 'model.my_dbt_project.silver_tags' was properly closed.
[0m08:24:49.521968 [info ] [MainThread]: 
[0m08:24:49.523599 [info ] [MainThread]: Finished running 4 table models in 0 hours 1 minutes and 41.39 seconds (101.39s).
[0m08:24:49.525971 [debug] [MainThread]: Command end result
[0m08:24:49.562106 [info ] [MainThread]: 
[0m08:24:49.563142 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:24:49.564035 [info ] [MainThread]: 
[0m08:24:49.564922 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m08:24:49.566097 [debug] [MainThread]: Command `dbt run` succeeded at 08:24:49.565956 after 108.47 seconds
[0m08:24:49.566748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9783a43580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9756d306a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9756a75d00>]}
[0m08:24:49.567465 [debug] [MainThread]: Flushing usage events
[0m08:24:54.627487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d71083610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d6f5ee280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d6f5ee9a0>]}


============================== 08:24:54.635787 | f0d37e54-dc1a-475b-a2da-c4cb0919dbb1 ==============================
[0m08:24:54.635787 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:24:54.636791 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/opt/airflow/dbt/logs', 'profiles_dir': '/opt/airflow/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:24:55.860135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f0d37e54-dc1a-475b-a2da-c4cb0919dbb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d6f5ee220>]}
[0m08:24:55.891742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f0d37e54-dc1a-475b-a2da-c4cb0919dbb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d48374550>]}
[0m08:24:55.892912 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:24:55.940207 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:24:57.238651 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:24:57.239418 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:24:57.252668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f0d37e54-dc1a-475b-a2da-c4cb0919dbb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d482ff400>]}
[0m08:24:57.290479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f0d37e54-dc1a-475b-a2da-c4cb0919dbb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d4822e430>]}
[0m08:24:57.291409 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:24:57.292344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f0d37e54-dc1a-475b-a2da-c4cb0919dbb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d4822e490>]}
[0m08:24:57.295147 [info ] [MainThread]: 
[0m08:24:57.296739 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:24:57.298811 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:24:57.299539 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:24:57.300174 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m08:24:57.300984 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:24:56.832274 [debug] [ThreadPool]: SQL status: OK in -0.4699999988079071 seconds
[0m08:24:56.834764 [debug] [ThreadPool]: On list_workspace: Close
[0m08:24:56.953026 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_movielens_volume)
[0m08:24:56.954424 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "movielens_volume"
"
[0m08:24:56.971432 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m08:24:56.972335 [debug] [ThreadPool]: Using databricks connection "create_workspace_movielens_volume"
[0m08:24:56.972986 [debug] [ThreadPool]: On create_workspace_movielens_volume: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "connection_name": "create_workspace_movielens_volume"} */
create schema if not exists `workspace`.`movielens_volume`
  
[0m08:24:56.973624 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:24:57.670539 [debug] [ThreadPool]: SQL status: OK in 0.699999988079071 seconds
[0m08:24:57.672121 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m08:24:57.673068 [debug] [ThreadPool]: On create_workspace_movielens_volume: ROLLBACK
[0m08:24:57.673843 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m08:24:57.674508 [debug] [ThreadPool]: On create_workspace_movielens_volume: Close
[0m08:24:57.793374 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_workspace_movielens_volume, now list_workspace_movielens_volume)
[0m08:24:57.803186 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:24:57.803965 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:24:57.804826 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:24:58.539514 [debug] [ThreadPool]: SQL status: OK in 0.7300000190734863 seconds
[0m08:24:58.543118 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:24:58.666841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f0d37e54-dc1a-475b-a2da-c4cb0919dbb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d72c28df0>]}
[0m08:24:58.668140 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:24:58.668853 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:24:58.669914 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:24:58.670791 [info ] [MainThread]: 
[0m08:24:58.687597 [debug] [Thread-4  ]: Began running node model.my_dbt_project.movie_enriched
[0m08:24:58.688677 [info ] [Thread-4  ]: 1 of 2 START sql table model movielens_volume.movie_enriched ................... [RUN]
[0m08:24:58.690053 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now model.my_dbt_project.movie_enriched)
[0m08:24:58.690769 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.movie_enriched
[0m08:24:58.702301 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.movie_enriched"
[0m08:24:58.712813 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (compile): 08:24:58.691374 => 08:24:58.712365
[0m08:24:58.713644 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.movie_enriched
[0m08:24:58.734938 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:24:58.735839 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m08:24:58.736506 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

      describe extended `workspace`.`movielens_volume`.`movie_enriched`
  
[0m08:24:58.737291 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:25:00.155907 [debug] [Thread-4  ]: SQL status: OK in 1.4199999570846558 seconds
[0m08:25:00.224113 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.movie_enriched"
[0m08:25:00.235156 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.movie_enriched"
[0m08:25:00.236264 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.movie_enriched"} */

  
    
        create or replace table `workspace`.`movielens_volume`.`movie_enriched`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- =======================================================
-- Step 1: Aggregate Ratings
-- Calculate total ratings and average rating per movie
-- =======================================================
with ratings as (
    select
        movie_id,
        count(*) as total_ratings,
        round(avg(rating), 2) as avg_rating
    from `workspace`.`movielens_volume`.`silver_ratings`
    group by movie_id
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie titles and genres from silver_movies
-- =======================================================
movies as (
    select
        movie_id,
        title,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
)

-- =======================================================
-- Step 3: Final Selection
-- Combine all metrics and movie info per movie
-- =======================================================
select
    m.movie_id,
    m.title,
    m.genres,
    coalesce(r.total_ratings, 0) as total_ratings,
    coalesce(r.avg_rating, 0) as avg_rating
from movies m
left join ratings r on m.movie_id = r.movie_id
  
[0m08:25:04.653070 [debug] [Thread-4  ]: SQL status: OK in 4.420000076293945 seconds
[0m08:25:04.690306 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.movie_enriched (execute): 08:24:58.714183 => 08:25:04.689993
[0m08:25:04.691204 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: ROLLBACK
[0m08:25:04.691928 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:04.692624 [debug] [Thread-4  ]: On model.my_dbt_project.movie_enriched: Close
[0m08:25:04.818925 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f0d37e54-dc1a-475b-a2da-c4cb0919dbb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d48059d00>]}
[0m08:25:04.820299 [info ] [Thread-4  ]: 1 of 2 OK created sql table model movielens_volume.movie_enriched .............. [[32mOK[0m in 6.13s]
[0m08:25:04.821470 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.movie_enriched
[0m08:25:04.822279 [debug] [Thread-4  ]: Began running node model.my_dbt_project.user_preferences
[0m08:25:04.823134 [info ] [Thread-4  ]: 2 of 2 START sql incremental model movielens_volume.user_preferences ........... [RUN]
[0m08:25:04.824561 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.my_dbt_project.movie_enriched, now model.my_dbt_project.user_preferences)
[0m08:25:04.825439 [debug] [Thread-4  ]: Began compiling node model.my_dbt_project.user_preferences
[0m08:25:04.840485 [debug] [Thread-4  ]: Writing injected SQL for node "model.my_dbt_project.user_preferences"
[0m08:25:04.849445 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (compile): 08:25:04.826100 => 08:25:04.849002
[0m08:25:04.850255 [debug] [Thread-4  ]: Began executing node model.my_dbt_project.user_preferences
[0m08:25:04.900059 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:04.900854 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:25:04.901400 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m08:25:04.902011 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m08:25:06.119729 [debug] [Thread-4  ]: SQL status: OK in 1.2200000286102295 seconds
[0m08:25:06.132596 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:25:06.133677 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

  
    create temporary view `user_preferences__dbt_tmp` as
      

-- =======================================================
-- Step 1: Base Ratings
-- Load ratings and prepare timestamps for downstream aggregation
-- =======================================================
with ratings as (
    select
        user_id,
        movie_id,
        round(rating, 2) as rating,
        cast(rating_date as date) as rating_timestamp
    from `workspace`.`movielens_volume`.`silver_ratings`
),

-- =======================================================
-- Step 2: Movies Base
-- Load movie genres for user-genre analysis
-- =======================================================
movies as (
    select
        movie_id,
        genres
    from `workspace`.`movielens_volume`.`silver_movies`
),

-- =======================================================
-- Step 3: Per-user aggregated rating behavior
-- Calculate total ratings, average rating, first and last activity dates
-- =======================================================
user_overall as (
    select
        r.user_id,
        count(*) as user_rating_count,
        round(avg(r.rating), 2) as user_avg_rating,
        min(r.rating_timestamp) as first_activity_date,
        max(r.rating_timestamp) as last_activity_date
    from ratings r
    group by r.user_id
),

-- =======================================================
-- Step 4: User’s genre distribution
-- Calculate average rating and rating count per genre for each user
-- =======================================================
user_genres as (
    select
        r.user_id,
        m.genres,
        round(avg(r.rating), 2) as avg_rating,
        count(*) as genre_count
    from ratings r
    join movies m on r.movie_id = m.movie_id
    group by r.user_id, m.genres
),

-- =======================================================
-- Step 5: Pick the user’s top genre
-- Select the genre with highest rating count (tie-breaker = highest avg rating)
-- =======================================================
top_genre as (
    select user_id, genres as top_genre
    from (
        select
            ug.*,
            row_number() over (partition by ug.user_id order by ug.genre_count desc, ug.avg_rating desc) as rn
        from user_genres ug
    ) ranked
    where rn = 1
)

-- =======================================================
-- Step 6: Final Preferences Table
-- Combine overall user activity, influence, and top genre
-- =======================================================
select
    u.user_id,
    u.user_rating_count,
    u.user_avg_rating,
    u.first_activity_date,
    u.last_activity_date,
    u.user_rating_count as influence_score, 
    tg.top_genre
from user_overall u
left join top_genre tg on u.user_id = tg.user_id


  -- Incremental filter: only include users with new activity since last run
  where u.last_activity_date > (
    select coalesce(max(last_activity_date), date('1970-01-01')) from `workspace`.`movielens_volume`.`user_preferences`
  )

  
[0m08:25:07.007121 [debug] [Thread-4  ]: SQL status: OK in 0.8700000047683716 seconds
[0m08:25:07.035838 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:25:07.036655 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */

      describe extended `workspace`.`movielens_volume`.`user_preferences`
  
[0m08:25:07.367718 [debug] [Thread-4  ]: SQL status: OK in 0.33000001311302185 seconds
[0m08:25:07.380492 [debug] [Thread-4  ]: Writing runtime sql for node "model.my_dbt_project.user_preferences"
[0m08:25:07.390023 [debug] [Thread-4  ]: Using databricks connection "model.my_dbt_project.user_preferences"
[0m08:25:07.390969 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "model.my_dbt_project.user_preferences"} */
-- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into `workspace`.`movielens_volume`.`user_preferences` as DBT_INTERNAL_DEST
      using `user_preferences__dbt_tmp` as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
          

      when matched then update set
         * 

      when not matched then insert *

[0m08:25:17.855392 [debug] [Thread-4  ]: SQL status: OK in 10.460000038146973 seconds
[0m08:25:18.025304 [debug] [Thread-4  ]: Timing info for model.my_dbt_project.user_preferences (execute): 08:25:04.850795 => 08:25:18.024994
[0m08:25:18.026204 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: ROLLBACK
[0m08:25:18.027142 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:18.027819 [debug] [Thread-4  ]: On model.my_dbt_project.user_preferences: Close
[0m08:25:18.154217 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f0d37e54-dc1a-475b-a2da-c4cb0919dbb1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d4822e5b0>]}
[0m08:25:18.155428 [info ] [Thread-4  ]: 2 of 2 OK created sql incremental model movielens_volume.user_preferences ...... [[32mOK[0m in 13.33s]
[0m08:25:18.156835 [debug] [Thread-4  ]: Finished running node model.my_dbt_project.user_preferences
[0m08:25:18.159372 [debug] [MainThread]: On master: ROLLBACK
[0m08:25:18.160010 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:25:19.126342 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:25:19.127316 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:19.128073 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:25:19.128725 [debug] [MainThread]: On master: ROLLBACK
[0m08:25:19.129347 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:25:19.129918 [debug] [MainThread]: On master: Close
[0m08:25:19.245433 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:25:19.246323 [debug] [MainThread]: Connection 'model.my_dbt_project.user_preferences' was properly closed.
[0m08:25:19.249041 [info ] [MainThread]: 
[0m08:25:19.250941 [info ] [MainThread]: Finished running 1 table model, 1 incremental model in 0 hours 0 minutes and 21.95 seconds (21.95s).
[0m08:25:19.252486 [debug] [MainThread]: Command end result
[0m08:25:19.283876 [info ] [MainThread]: 
[0m08:25:19.285049 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:25:19.286102 [info ] [MainThread]: 
[0m08:25:19.287110 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m08:25:19.288255 [debug] [MainThread]: Command `dbt run` succeeded at 08:25:19.288109 after 25.76 seconds
[0m08:25:19.288907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d71083610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d482358e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d48235640>]}
[0m08:25:19.289559 [debug] [MainThread]: Flushing usage events
[0m08:25:23.461299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43cf5f6520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43cdc2f100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43cdc2f820>]}


============================== 08:25:23.469507 | 69e40dd4-cdf0-4c1b-b958-fabf25a7d9b9 ==============================
[0m08:25:23.469507 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:25:23.470528 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/opt/airflow/dbt', 'log_path': '/opt/airflow/dbt/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:25:24.497896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '69e40dd4-cdf0-4c1b-b958-fabf25a7d9b9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43cdc2f0d0>]}
[0m08:25:24.529954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '69e40dd4-cdf0-4c1b-b958-fabf25a7d9b9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43a288c250>]}
[0m08:25:24.531082 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:25:24.578662 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:25:25.882692 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:25:25.883423 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:25:25.895315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '69e40dd4-cdf0-4c1b-b958-fabf25a7d9b9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43a2844280>]}
[0m08:25:25.931225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '69e40dd4-cdf0-4c1b-b958-fabf25a7d9b9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43a27722b0>]}
[0m08:25:25.932344 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:25:25.933295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69e40dd4-cdf0-4c1b-b958-fabf25a7d9b9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43a2772310>]}
[0m08:25:25.937012 [info ] [MainThread]: 
[0m08:25:25.938482 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:25:25.941022 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m08:25:25.947892 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:25:25.948616 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:25:25.949272 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:25:25.605050 [debug] [ThreadPool]: SQL status: OK in -0.3400000035762787 seconds
[0m08:25:25.608498 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:25:25.736706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69e40dd4-cdf0-4c1b-b958-fabf25a7d9b9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43a28db610>]}
[0m08:25:25.737883 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:25.738625 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:25:25.739715 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:25:25.740531 [info ] [MainThread]: 
[0m08:25:25.763248 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:25:25.764165 [info ] [Thread-2  ]: 1 of 22 START test accepted_rating_range_silver_ratings_rating__5__0 ........... [RUN]
[0m08:25:25.765425 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff)
[0m08:25:25.766063 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:25:25.778602 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m08:25:25.788800 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (compile): 08:25:25.766630 => 08:25:25.788363
[0m08:25:25.789568 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:25:25.821248 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m08:25:25.831729 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:25.832472 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"
[0m08:25:25.833189 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`silver_ratings`
where rating < 0
   or rating > 5


      
    ) dbt_internal_test
[0m08:25:25.833881 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:27.327479 [debug] [Thread-2  ]: SQL status: OK in 1.4900000095367432 seconds
[0m08:25:27.336156 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff (execute): 08:25:25.790041 => 08:25:27.335738
[0m08:25:27.336964 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: ROLLBACK
[0m08:25:27.337756 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:27.338685 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff: Close
[0m08:25:27.465725 [info ] [Thread-2  ]: 1 of 22 PASS accepted_rating_range_silver_ratings_rating__5__0 ................. [[32mPASS[0m in 1.70s]
[0m08:25:27.467492 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff
[0m08:25:27.468436 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:25:27.469253 [info ] [Thread-2  ]: 2 of 22 START test not_null_silver_links_imdb_id ............................... [RUN]
[0m08:25:27.470628 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_silver_ratings_rating__5__0.f2e909bdff, now test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51)
[0m08:25:27.471296 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:25:27.481619 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m08:25:27.492075 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (compile): 08:25:27.471842 => 08:25:27.491537
[0m08:25:27.492861 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:25:27.497061 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m08:25:27.511925 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:27.512652 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"
[0m08:25:27.513235 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select imdb_id
from `workspace`.`movielens_volume`.`silver_links`
where imdb_id is null



      
    ) dbt_internal_test
[0m08:25:27.513866 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:28.601184 [debug] [Thread-2  ]: SQL status: OK in 1.090000033378601 seconds
[0m08:25:28.605631 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51 (execute): 08:25:27.493308 => 08:25:28.605241
[0m08:25:28.606584 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: ROLLBACK
[0m08:25:28.607306 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:28.607956 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51: Close
[0m08:25:28.731388 [info ] [Thread-2  ]: 2 of 22 PASS not_null_silver_links_imdb_id ..................................... [[32mPASS[0m in 1.26s]
[0m08:25:28.732955 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51
[0m08:25:28.733958 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:25:28.734944 [info ] [Thread-2  ]: 3 of 22 START test not_null_silver_links_movie_id .............................. [RUN]
[0m08:25:28.736391 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_imdb_id.01f1097d51, now test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d)
[0m08:25:28.737285 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:25:28.747077 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m08:25:28.757567 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (compile): 08:25:28.738031 => 08:25:28.757143
[0m08:25:28.758339 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:25:28.762797 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m08:25:28.773287 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:28.774102 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"
[0m08:25:28.774760 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_links`
where movie_id is null



      
    ) dbt_internal_test
[0m08:25:28.775373 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:29.716031 [debug] [Thread-2  ]: SQL status: OK in 0.9399999976158142 seconds
[0m08:25:29.720567 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d (execute): 08:25:28.758781 => 08:25:29.720184
[0m08:25:29.721400 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: ROLLBACK
[0m08:25:29.722055 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:29.722725 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d: Close
[0m08:25:29.852588 [info ] [Thread-2  ]: 3 of 22 PASS not_null_silver_links_movie_id .................................... [[32mPASS[0m in 1.12s]
[0m08:25:29.853966 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d
[0m08:25:29.854871 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:25:29.855723 [info ] [Thread-2  ]: 4 of 22 START test not_null_silver_links_tmdb_id ............................... [RUN]
[0m08:25:29.857211 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_movie_id.09b11d0c2d, now test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f)
[0m08:25:29.857997 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:25:29.870840 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m08:25:29.885976 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (compile): 08:25:29.858503 => 08:25:29.885474
[0m08:25:29.886791 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:25:29.891858 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m08:25:29.902227 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:29.902942 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"
[0m08:25:29.903563 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tmdb_id
from `workspace`.`movielens_volume`.`silver_links`
where tmdb_id is null



      
    ) dbt_internal_test
[0m08:25:29.904309 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:30.870196 [debug] [Thread-2  ]: SQL status: OK in 0.9700000286102295 seconds
[0m08:25:30.874865 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f (execute): 08:25:29.887334 => 08:25:30.874389
[0m08:25:30.875975 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: ROLLBACK
[0m08:25:30.876817 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:30.877562 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f: Close
[0m08:25:31.002580 [info ] [Thread-2  ]: 4 of 22 PASS not_null_silver_links_tmdb_id ..................................... [[32mPASS[0m in 1.15s]
[0m08:25:31.004034 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f
[0m08:25:31.004894 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:25:31.005708 [info ] [Thread-2  ]: 5 of 22 START test not_null_silver_movies_genres ............................... [RUN]
[0m08:25:31.007137 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_links_tmdb_id.90670d550f, now test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63)
[0m08:25:31.007866 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:25:31.017144 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m08:25:31.027513 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (compile): 08:25:31.008380 => 08:25:31.027049
[0m08:25:31.028393 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:25:31.032437 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m08:25:31.042821 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:31.043507 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"
[0m08:25:31.044161 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`silver_movies`
where genres is null



      
    ) dbt_internal_test
[0m08:25:31.044873 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:31.958425 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m08:25:31.963222 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63 (execute): 08:25:31.028904 => 08:25:31.962753
[0m08:25:31.964134 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: ROLLBACK
[0m08:25:31.964858 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:31.965600 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63: Close
[0m08:25:32.082596 [info ] [Thread-2  ]: 5 of 22 PASS not_null_silver_movies_genres ..................................... [[32mPASS[0m in 1.08s]
[0m08:25:32.084247 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63
[0m08:25:32.085166 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:25:32.086037 [info ] [Thread-2  ]: 6 of 22 START test not_null_silver_movies_movie_id ............................. [RUN]
[0m08:25:32.087380 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_genres.2ab8f64a63, now test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e)
[0m08:25:32.088004 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:25:32.094662 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m08:25:32.105680 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (compile): 08:25:32.088464 => 08:25:32.104970
[0m08:25:32.106773 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:25:32.111144 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m08:25:32.122244 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:32.122943 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"
[0m08:25:32.123522 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is null



      
    ) dbt_internal_test
[0m08:25:32.124146 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:33.039449 [debug] [Thread-2  ]: SQL status: OK in 0.9200000166893005 seconds
[0m08:25:33.043976 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e (execute): 08:25:32.107436 => 08:25:33.043610
[0m08:25:33.044766 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: ROLLBACK
[0m08:25:33.045562 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:33.046327 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e: Close
[0m08:25:33.163092 [info ] [Thread-2  ]: 6 of 22 PASS not_null_silver_movies_movie_id ................................... [[32mPASS[0m in 1.08s]
[0m08:25:33.164401 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e
[0m08:25:33.165146 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:25:33.165840 [info ] [Thread-2  ]: 7 of 22 START test not_null_silver_movies_title ................................ [RUN]
[0m08:25:33.167210 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_movie_id.a903205d5e, now test.my_dbt_project.not_null_silver_movies_title.3a779bd2de)
[0m08:25:33.167914 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:25:33.181184 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m08:25:33.192341 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (compile): 08:25:33.168401 => 08:25:33.191681
[0m08:25:33.193264 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:25:33.197653 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m08:25:33.208822 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:33.209600 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"
[0m08:25:33.210169 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_movies_title.3a779bd2de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`silver_movies`
where title is null



      
    ) dbt_internal_test
[0m08:25:33.210826 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:34.125349 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m08:25:34.129825 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_movies_title.3a779bd2de (execute): 08:25:33.193774 => 08:25:34.129471
[0m08:25:34.130625 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: ROLLBACK
[0m08:25:34.131231 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:34.131828 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_movies_title.3a779bd2de: Close
[0m08:25:34.253548 [info ] [Thread-2  ]: 7 of 22 PASS not_null_silver_movies_title ...................................... [[32mPASS[0m in 1.09s]
[0m08:25:34.254835 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_movies_title.3a779bd2de
[0m08:25:34.255708 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:25:34.256648 [info ] [Thread-2  ]: 8 of 22 START test not_null_silver_ratings_movie_id ............................ [RUN]
[0m08:25:34.257947 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_movies_title.3a779bd2de, now test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36)
[0m08:25:34.258567 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:25:34.265296 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m08:25:34.276430 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (compile): 08:25:34.259090 => 08:25:34.275983
[0m08:25:34.277543 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:25:34.282018 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m08:25:34.293496 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:34.294223 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"
[0m08:25:34.294975 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_ratings`
where movie_id is null



      
    ) dbt_internal_test
[0m08:25:34.295647 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:35.196102 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m08:25:35.200645 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36 (execute): 08:25:34.278150 => 08:25:35.200288
[0m08:25:35.201522 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: ROLLBACK
[0m08:25:35.202233 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:35.202890 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36: Close
[0m08:25:35.318185 [info ] [Thread-2  ]: 8 of 22 PASS not_null_silver_ratings_movie_id .................................. [[32mPASS[0m in 1.06s]
[0m08:25:35.319492 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36
[0m08:25:35.320454 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:25:35.321254 [info ] [Thread-2  ]: 9 of 22 START test not_null_silver_ratings_rating .............................. [RUN]
[0m08:25:35.322428 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_movie_id.63d982fd36, now test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a)
[0m08:25:35.323116 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:25:35.329636 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m08:25:35.339879 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (compile): 08:25:35.323590 => 08:25:35.339422
[0m08:25:35.340755 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:25:35.345364 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m08:25:35.357107 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:35.357878 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"
[0m08:25:35.358500 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating
from `workspace`.`movielens_volume`.`silver_ratings`
where rating is null



      
    ) dbt_internal_test
[0m08:25:35.359185 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:36.304350 [debug] [Thread-2  ]: SQL status: OK in 0.949999988079071 seconds
[0m08:25:36.309144 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a (execute): 08:25:35.341448 => 08:25:36.308799
[0m08:25:36.309937 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: ROLLBACK
[0m08:25:36.310534 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:36.311167 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a: Close
[0m08:25:36.437735 [info ] [Thread-2  ]: 9 of 22 PASS not_null_silver_ratings_rating .................................... [[32mPASS[0m in 1.12s]
[0m08:25:36.439088 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a
[0m08:25:36.439901 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:25:36.440662 [info ] [Thread-2  ]: 10 of 22 START test not_null_silver_ratings_rating_date ........................ [RUN]
[0m08:25:36.442177 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating.cd9852f72a, now test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f)
[0m08:25:36.442974 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:25:36.458531 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m08:25:36.469530 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (compile): 08:25:36.443539 => 08:25:36.469019
[0m08:25:36.470411 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:25:36.474867 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m08:25:36.485635 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:36.486550 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"
[0m08:25:36.487233 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select rating_date
from `workspace`.`movielens_volume`.`silver_ratings`
where rating_date is null



      
    ) dbt_internal_test
[0m08:25:36.487916 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:37.467046 [debug] [Thread-2  ]: SQL status: OK in 0.9800000190734863 seconds
[0m08:25:37.471317 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f (execute): 08:25:36.471067 => 08:25:37.470945
[0m08:25:37.472236 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: ROLLBACK
[0m08:25:37.473066 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:37.473790 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f: Close
[0m08:25:37.593221 [info ] [Thread-2  ]: 10 of 22 PASS not_null_silver_ratings_rating_date .............................. [[32mPASS[0m in 1.15s]
[0m08:25:37.594773 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f
[0m08:25:37.595565 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:25:37.596236 [info ] [Thread-2  ]: 11 of 22 START test not_null_silver_ratings_user_id ............................ [RUN]
[0m08:25:37.597456 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_rating_date.8636bef66f, now test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3)
[0m08:25:37.598038 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:25:37.607323 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m08:25:37.619221 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (compile): 08:25:37.598500 => 08:25:37.618767
[0m08:25:37.620054 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:25:37.625705 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m08:25:37.636923 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:37.637750 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"
[0m08:25:37.638410 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_ratings`
where user_id is null



      
    ) dbt_internal_test
[0m08:25:37.639123 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:38.537046 [debug] [Thread-2  ]: SQL status: OK in 0.8999999761581421 seconds
[0m08:25:38.541256 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3 (execute): 08:25:37.620702 => 08:25:38.540840
[0m08:25:38.542215 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: ROLLBACK
[0m08:25:38.543120 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:38.544007 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3: Close
[0m08:25:38.665118 [info ] [Thread-2  ]: 11 of 22 PASS not_null_silver_ratings_user_id .................................. [[32mPASS[0m in 1.07s]
[0m08:25:38.666710 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3
[0m08:25:38.667629 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:25:38.668453 [info ] [Thread-2  ]: 12 of 22 START test not_null_silver_tags_movie_id .............................. [RUN]
[0m08:25:38.669606 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_ratings_user_id.dc2f082ec3, now test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3)
[0m08:25:38.670221 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:25:38.680453 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m08:25:38.691301 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (compile): 08:25:38.670709 => 08:25:38.690658
[0m08:25:38.692240 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:25:38.697337 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m08:25:38.712281 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:38.713092 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"
[0m08:25:38.714011 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`silver_tags`
where movie_id is null



      
    ) dbt_internal_test
[0m08:25:38.714962 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:39.770612 [debug] [Thread-2  ]: SQL status: OK in 1.059999942779541 seconds
[0m08:25:39.774896 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3 (execute): 08:25:38.692945 => 08:25:39.774477
[0m08:25:39.775795 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: ROLLBACK
[0m08:25:39.776530 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:39.777206 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3: Close
[0m08:25:39.896621 [info ] [Thread-2  ]: 12 of 22 PASS not_null_silver_tags_movie_id .................................... [[32mPASS[0m in 1.23s]
[0m08:25:39.898044 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3
[0m08:25:39.898819 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:25:39.899501 [info ] [Thread-2  ]: 13 of 22 START test not_null_silver_tags_tag ................................... [RUN]
[0m08:25:39.901137 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_movie_id.b315f57ec3, now test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8)
[0m08:25:39.902079 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:25:39.914305 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m08:25:39.925416 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (compile): 08:25:39.902592 => 08:25:39.924939
[0m08:25:39.926274 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:25:39.930814 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m08:25:39.945023 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:39.945809 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"
[0m08:25:39.946478 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tag
from `workspace`.`movielens_volume`.`silver_tags`
where tag is null



      
    ) dbt_internal_test
[0m08:25:39.947247 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:40.940662 [debug] [Thread-2  ]: SQL status: OK in 0.9900000095367432 seconds
[0m08:25:40.945056 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8 (execute): 08:25:39.926886 => 08:25:40.944677
[0m08:25:40.946025 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: ROLLBACK
[0m08:25:40.946788 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:40.947515 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8: Close
[0m08:25:41.061538 [info ] [Thread-2  ]: 13 of 22 PASS not_null_silver_tags_tag ......................................... [[32mPASS[0m in 1.16s]
[0m08:25:41.063464 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8
[0m08:25:41.064828 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:25:41.066548 [info ] [Thread-2  ]: 14 of 22 START test not_null_silver_tags_tagged_date ........................... [RUN]
[0m08:25:41.069393 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tag.16a7959ec8, now test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86)
[0m08:25:41.070835 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:25:41.082370 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m08:25:41.102647 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (compile): 08:25:41.071750 => 08:25:41.101897
[0m08:25:41.104659 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:25:41.114425 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m08:25:41.131105 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:41.132331 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"
[0m08:25:41.133661 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tagged_date
from `workspace`.`movielens_volume`.`silver_tags`
where tagged_date is null



      
    ) dbt_internal_test
[0m08:25:41.134969 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:42.059372 [debug] [Thread-2  ]: SQL status: OK in 0.9200000166893005 seconds
[0m08:25:42.063922 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86 (execute): 08:25:41.107195 => 08:25:42.063583
[0m08:25:42.064757 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: ROLLBACK
[0m08:25:42.065632 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:42.066361 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86: Close
[0m08:25:42.185360 [info ] [Thread-2  ]: 14 of 22 PASS not_null_silver_tags_tagged_date ................................. [[32mPASS[0m in 1.12s]
[0m08:25:42.186877 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86
[0m08:25:42.187784 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:25:42.188588 [info ] [Thread-2  ]: 15 of 22 START test not_null_silver_tags_user_id ............................... [RUN]
[0m08:25:42.189916 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_tagged_date.385229eb86, now test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3)
[0m08:25:42.190617 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:25:42.197124 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m08:25:42.207415 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (compile): 08:25:42.191123 => 08:25:42.206964
[0m08:25:42.208138 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:25:42.212791 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m08:25:42.223046 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:42.223993 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"
[0m08:25:42.224676 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from `workspace`.`movielens_volume`.`silver_tags`
where user_id is null



      
    ) dbt_internal_test
[0m08:25:42.225317 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:43.167946 [debug] [Thread-2  ]: SQL status: OK in 0.9399999976158142 seconds
[0m08:25:43.172349 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3 (execute): 08:25:42.208688 => 08:25:43.172013
[0m08:25:43.173395 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: ROLLBACK
[0m08:25:43.174455 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:43.175175 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3: Close
[0m08:25:43.294400 [info ] [Thread-2  ]: 15 of 22 PASS not_null_silver_tags_user_id ..................................... [[32mPASS[0m in 1.10s]
[0m08:25:43.295962 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3
[0m08:25:43.296763 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:25:43.297469 [info ] [Thread-2  ]: 16 of 22 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:25:43.298669 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_silver_tags_user_id.9c692bc4d3, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m08:25:43.299376 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:25:43.320213 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:25:43.331293 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 08:25:43.299927 => 08:25:43.330624
[0m08:25:43.332247 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:25:43.336482 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:25:43.347312 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:43.348015 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:25:43.348719 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:25:43.349405 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:44.788250 [debug] [Thread-2  ]: SQL status: OK in 1.440000057220459 seconds
[0m08:25:44.793088 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 08:25:43.332856 => 08:25:44.792696
[0m08:25:44.794046 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m08:25:44.794976 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:44.795848 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m08:25:44.923332 [info ] [Thread-2  ]: 16 of 22 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.62s]
[0m08:25:44.924705 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:25:44.925543 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:25:44.926395 [info ] [Thread-2  ]: 17 of 22 START test relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:25:44.928201 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d)
[0m08:25:44.928940 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:25:44.938276 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:25:44.950451 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (compile): 08:25:44.929476 => 08:25:44.949861
[0m08:25:44.951275 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:25:44.955378 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:25:44.967481 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:44.976965 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"
[0m08:25:44.978053 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_performance`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:25:44.978915 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:47.245297 [debug] [Thread-2  ]: SQL status: OK in 2.2699999809265137 seconds
[0m08:25:47.249814 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d (execute): 08:25:44.951757 => 08:25:47.249471
[0m08:25:47.250752 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: ROLLBACK
[0m08:25:47.251596 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:47.252267 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d: Close
[0m08:25:47.371838 [info ] [Thread-2  ]: 17 of 22 PASS relationships_movie_performance_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 2.44s]
[0m08:25:47.373847 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d
[0m08:25:47.374827 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:25:47.375630 [info ] [Thread-2  ]: 18 of 22 START test relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:25:47.377143 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_performance_movie_id__movie_id__ref_silver_movies_.9609ad5a4d, now test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32)
[0m08:25:47.377956 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:25:47.387014 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m08:25:47.399350 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (compile): 08:25:47.378587 => 08:25:47.398834
[0m08:25:47.400323 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:25:47.405274 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m08:25:47.415910 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:47.416790 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"
[0m08:25:47.417512 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_links`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:25:47.418222 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:48.872342 [debug] [Thread-2  ]: SQL status: OK in 1.4500000476837158 seconds
[0m08:25:48.876803 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32 (execute): 08:25:47.401162 => 08:25:48.876356
[0m08:25:48.877706 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: ROLLBACK
[0m08:25:48.878583 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:48.879338 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32: Close
[0m08:25:49.001338 [info ] [Thread-2  ]: 18 of 22 PASS relationships_silver_links_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.62s]
[0m08:25:49.002797 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32
[0m08:25:49.003685 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:25:49.004771 [info ] [Thread-2  ]: 19 of 22 START test relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:25:49.006306 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_links_movie_id__movie_id__ref_silver_movies_.2947395c32, now test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd)
[0m08:25:49.007154 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:25:49.025537 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m08:25:49.036327 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (compile): 08:25:49.007812 => 08:25:49.035795
[0m08:25:49.037193 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:25:49.041409 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m08:25:49.051675 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:49.052407 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"
[0m08:25:49.053121 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_ratings`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:25:49.053923 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:50.585047 [debug] [Thread-2  ]: SQL status: OK in 1.5299999713897705 seconds
[0m08:25:50.589633 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd (execute): 08:25:49.037746 => 08:25:50.589253
[0m08:25:50.590462 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: ROLLBACK
[0m08:25:50.591113 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:50.591827 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd: Close
[0m08:25:50.712300 [info ] [Thread-2  ]: 19 of 22 PASS relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 1.71s]
[0m08:25:50.713807 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd
[0m08:25:50.714619 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:25:50.715412 [info ] [Thread-2  ]: 20 of 22 START test relationships_silver_tags_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:25:50.717477 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_ratings_movie_id__movie_id__ref_silver_movies_.3932905bcd, now test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719)
[0m08:25:50.718413 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:25:50.727460 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m08:25:50.738511 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (compile): 08:25:50.719043 => 08:25:50.737829
[0m08:25:50.739494 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:25:50.743724 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m08:25:50.754909 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:50.755686 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"
[0m08:25:50.756269 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`silver_tags`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:25:50.756930 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:52.416535 [debug] [Thread-2  ]: SQL status: OK in 1.659999966621399 seconds
[0m08:25:52.421417 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719 (execute): 08:25:50.740149 => 08:25:52.420836
[0m08:25:52.422946 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: ROLLBACK
[0m08:25:52.423701 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:52.424371 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719: Close
[0m08:25:52.544434 [info ] [Thread-2  ]: 20 of 22 PASS relationships_silver_tags_movie_id__movie_id__ref_silver_movies_ . [[32mPASS[0m in 1.83s]
[0m08:25:52.546020 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719
[0m08:25:52.546951 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:25:52.547793 [info ] [Thread-2  ]: 21 of 22 START test unique_silver_links_movie_id ............................... [RUN]
[0m08:25:52.549144 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_silver_tags_movie_id__movie_id__ref_silver_movies_.1f88c0c719, now test.my_dbt_project.unique_silver_links_movie_id.c422609054)
[0m08:25:52.549842 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:25:52.559726 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m08:25:52.569767 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (compile): 08:25:52.550484 => 08:25:52.569314
[0m08:25:52.570479 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:25:52.574605 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m08:25:52.585087 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:52.586078 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_links_movie_id.c422609054"
[0m08:25:52.587072 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_links_movie_id.c422609054"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_links`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:25:52.587934 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:53.555117 [debug] [Thread-2  ]: SQL status: OK in 0.9700000286102295 seconds
[0m08:25:53.559735 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_links_movie_id.c422609054 (execute): 08:25:52.570914 => 08:25:53.559316
[0m08:25:53.560649 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: ROLLBACK
[0m08:25:53.561585 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:53.562319 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_links_movie_id.c422609054: Close
[0m08:25:53.693939 [info ] [Thread-2  ]: 21 of 22 PASS unique_silver_links_movie_id ..................................... [[32mPASS[0m in 1.14s]
[0m08:25:53.695406 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_links_movie_id.c422609054
[0m08:25:53.696166 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:25:53.696892 [info ] [Thread-2  ]: 22 of 22 START test unique_silver_movies_movie_id .............................. [RUN]
[0m08:25:53.698072 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.unique_silver_links_movie_id.c422609054, now test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085)
[0m08:25:53.698673 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:25:53.704588 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m08:25:53.716624 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (compile): 08:25:53.699124 => 08:25:53.715989
[0m08:25:53.717721 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:25:53.737124 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m08:25:53.749029 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:53.749747 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"
[0m08:25:53.750353 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`silver_movies`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:25:53.751143 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:25:54.675747 [debug] [Thread-2  ]: SQL status: OK in 0.9200000166893005 seconds
[0m08:25:54.680236 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085 (execute): 08:25:53.718418 => 08:25:54.679863
[0m08:25:54.681193 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: ROLLBACK
[0m08:25:54.682051 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:25:54.682926 [debug] [Thread-2  ]: On test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085: Close
[0m08:25:54.824141 [info ] [Thread-2  ]: 22 of 22 PASS unique_silver_movies_movie_id .................................... [[32mPASS[0m in 1.13s]
[0m08:25:54.825602 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085
[0m08:25:54.828183 [debug] [MainThread]: On master: ROLLBACK
[0m08:25:54.828843 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:25:55.203674 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:25:55.204839 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:25:55.205526 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:25:55.206227 [debug] [MainThread]: On master: ROLLBACK
[0m08:25:55.206943 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:25:55.207577 [debug] [MainThread]: On master: Close
[0m08:25:55.327744 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:25:55.328561 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_silver_movies_movie_id.1e3141e085' was properly closed.
[0m08:25:55.331047 [info ] [MainThread]: 
[0m08:25:55.332428 [info ] [MainThread]: Finished running 22 tests in 0 hours 0 minutes and 29.39 seconds (29.39s).
[0m08:25:55.337998 [debug] [MainThread]: Command end result
[0m08:25:55.370368 [info ] [MainThread]: 
[0m08:25:55.371495 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:25:55.372338 [info ] [MainThread]: 
[0m08:25:55.373095 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=0 SKIP=0 TOTAL=22
[0m08:25:55.374326 [debug] [MainThread]: Command `dbt test` succeeded at 08:25:55.374156 after 32.97 seconds
[0m08:25:55.375031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43cf5f6520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43a288c250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43a144fa90>]}
[0m08:25:55.375716 [debug] [MainThread]: Flushing usage events
[0m08:25:58.931368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2a2c07550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2a11b01c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2a11b08e0>]}


============================== 08:25:58.939193 | 5bde9200-1a52-4ff2-9b18-337cf8055cd8 ==============================
[0m08:25:58.939193 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:25:58.940375 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/opt/airflow/dbt/logs', 'profiles_dir': '/opt/airflow/dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:25:59.979937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5bde9200-1a52-4ff2-9b18-337cf8055cd8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2a11b0160>]}
[0m08:26:00.012868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5bde9200-1a52-4ff2-9b18-337cf8055cd8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe275ed4850>]}
[0m08:26:00.014019 [info ] [MainThread]: Registered adapter: databricks=1.5.2
[0m08:26:00.060448 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:26:01.418620 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:26:01.419341 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:26:01.431543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5bde9200-1a52-4ff2-9b18-337cf8055cd8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe275cb5340>]}
[0m08:26:01.467644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5bde9200-1a52-4ff2-9b18-337cf8055cd8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe275dbb370>]}
[0m08:26:01.468602 [info ] [MainThread]: Found 9 models, 55 tests, 0 snapshots, 0 analyses, 529 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics, 0 groups
[0m08:26:01.469373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5bde9200-1a52-4ff2-9b18-337cf8055cd8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe275dbb3d0>]}
[0m08:26:01.472271 [info ] [MainThread]: 
[0m08:26:01.473835 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:26:01.476525 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_movielens_volume'
[0m08:26:01.483414 [debug] [ThreadPool]: Using databricks connection "list_workspace_movielens_volume"
[0m08:26:01.484154 [debug] [ThreadPool]: On list_workspace_movielens_volume: GetTables(database=workspace, schema=movielens_volume, identifier=None)
[0m08:26:01.484785 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:26:02.151189 [debug] [ThreadPool]: SQL status: OK in 0.6700000166893005 seconds
[0m08:26:02.154706 [debug] [ThreadPool]: On list_workspace_movielens_volume: Close
[0m08:26:02.275637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5bde9200-1a52-4ff2-9b18-337cf8055cd8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe275dcc7f0>]}
[0m08:26:02.276771 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:26:02.277420 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:26:02.278732 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:26:02.279610 [info ] [MainThread]: 
[0m08:26:02.297051 [debug] [Thread-2  ]: Began running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:26:02.298109 [info ] [Thread-2  ]: 1 of 8 START test accepted_rating_range_movie_enriched_avg_rating__5__0 ........ [RUN]
[0m08:26:02.299337 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_workspace_movielens_volume, now test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8)
[0m08:26:02.300213 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:26:02.313581 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m08:26:02.324806 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (compile): 08:26:02.300926 => 08:26:02.324269
[0m08:26:02.325629 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:26:02.356677 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m08:26:02.367548 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:26:02.368315 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"
[0m08:26:02.369035 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      

select *
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating < 0
   or avg_rating > 5


      
    ) dbt_internal_test
[0m08:26:02.369778 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:26:03.374810 [debug] [Thread-2  ]: SQL status: OK in 1.0099999904632568 seconds
[0m08:26:03.383452 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8 (execute): 08:26:02.326168 => 08:26:03.382996
[0m08:26:03.384411 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: ROLLBACK
[0m08:26:03.385376 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:26:03.386272 [debug] [Thread-2  ]: On test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8: Close
[0m08:26:03.518506 [info ] [Thread-2  ]: 1 of 8 PASS accepted_rating_range_movie_enriched_avg_rating__5__0 .............. [[32mPASS[0m in 1.22s]
[0m08:26:03.520018 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8
[0m08:26:03.520836 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:26:03.521548 [info ] [Thread-2  ]: 2 of 8 START test not_null_movie_enriched_avg_rating ........................... [RUN]
[0m08:26:03.523257 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.accepted_rating_range_movie_enriched_avg_rating__5__0.a9d15718a8, now test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd)
[0m08:26:03.524369 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:26:03.535351 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m08:26:03.548209 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (compile): 08:26:03.524861 => 08:26:03.547757
[0m08:26:03.549089 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:26:03.554025 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m08:26:03.565626 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:26:03.566538 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"
[0m08:26:03.567226 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select avg_rating
from `workspace`.`movielens_volume`.`movie_enriched`
where avg_rating is null



      
    ) dbt_internal_test
[0m08:26:03.568167 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:26:04.443515 [debug] [Thread-2  ]: SQL status: OK in 0.8799999952316284 seconds
[0m08:26:04.448172 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd (execute): 08:26:03.549655 => 08:26:04.447825
[0m08:26:04.449022 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: ROLLBACK
[0m08:26:04.450030 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:26:04.450838 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd: Close
[0m08:26:04.573967 [info ] [Thread-2  ]: 2 of 8 PASS not_null_movie_enriched_avg_rating ................................. [[32mPASS[0m in 1.05s]
[0m08:26:04.575461 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd
[0m08:26:04.576402 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:26:04.577232 [info ] [Thread-2  ]: 3 of 8 START test not_null_movie_enriched_genres ............................... [RUN]
[0m08:26:04.578500 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_avg_rating.b4efaaf8dd, now test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27)
[0m08:26:04.579263 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:26:04.589647 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m08:26:04.600428 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (compile): 08:26:04.580167 => 08:26:04.599920
[0m08:26:04.601343 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:26:04.605517 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m08:26:04.616249 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:26:04.617020 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"
[0m08:26:04.617718 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select genres
from `workspace`.`movielens_volume`.`movie_enriched`
where genres is null



      
    ) dbt_internal_test
[0m08:26:04.618342 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:26:05.482196 [debug] [Thread-2  ]: SQL status: OK in 0.8600000143051147 seconds
[0m08:26:05.486905 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27 (execute): 08:26:04.601869 => 08:26:05.486524
[0m08:26:05.487946 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: ROLLBACK
[0m08:26:05.488668 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:26:05.489255 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27: Close
[0m08:26:05.607051 [info ] [Thread-2  ]: 3 of 8 PASS not_null_movie_enriched_genres ..................................... [[32mPASS[0m in 1.03s]
[0m08:26:05.608659 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27
[0m08:26:05.609644 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:26:05.610384 [info ] [Thread-2  ]: 4 of 8 START test not_null_movie_enriched_movie_id ............................. [RUN]
[0m08:26:05.611724 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_genres.d6c7620a27, now test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86)
[0m08:26:05.612450 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:26:05.624993 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m08:26:05.635810 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (compile): 08:26:05.612940 => 08:26:05.635382
[0m08:26:05.636757 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:26:05.641029 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m08:26:05.651433 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:26:05.652185 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"
[0m08:26:05.652823 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select movie_id
from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is null



      
    ) dbt_internal_test
[0m08:26:05.653782 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:26:06.495144 [debug] [Thread-2  ]: SQL status: OK in 0.8399999737739563 seconds
[0m08:26:06.499444 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86 (execute): 08:26:05.637392 => 08:26:06.499095
[0m08:26:06.500322 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: ROLLBACK
[0m08:26:06.501135 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:26:06.501845 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86: Close
[0m08:26:06.622904 [info ] [Thread-2  ]: 4 of 8 PASS not_null_movie_enriched_movie_id ................................... [[32mPASS[0m in 1.01s]
[0m08:26:06.624446 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86
[0m08:26:06.625358 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:26:06.626193 [info ] [Thread-2  ]: 5 of 8 START test not_null_movie_enriched_title ................................ [RUN]
[0m08:26:06.627714 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_movie_id.6ddfe19f86, now test.my_dbt_project.not_null_movie_enriched_title.d20f00c292)
[0m08:26:06.628336 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:26:06.637070 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m08:26:06.647428 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (compile): 08:26:06.628813 => 08:26:06.646954
[0m08:26:06.648224 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:26:06.652511 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m08:26:06.662587 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:26:06.663296 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"
[0m08:26:06.663951 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_title.d20f00c292"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select title
from `workspace`.`movielens_volume`.`movie_enriched`
where title is null



      
    ) dbt_internal_test
[0m08:26:06.664670 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:26:07.570348 [debug] [Thread-2  ]: SQL status: OK in 0.9100000262260437 seconds
[0m08:26:07.575301 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_title.d20f00c292 (execute): 08:26:06.648792 => 08:26:07.574808
[0m08:26:07.576261 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: ROLLBACK
[0m08:26:07.576955 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:26:07.577751 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_title.d20f00c292: Close
[0m08:26:07.709637 [info ] [Thread-2  ]: 5 of 8 PASS not_null_movie_enriched_title ...................................... [[32mPASS[0m in 1.08s]
[0m08:26:07.711139 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_title.d20f00c292
[0m08:26:07.711961 [debug] [Thread-2  ]: Began running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:26:07.712861 [info ] [Thread-2  ]: 6 of 8 START test not_null_movie_enriched_total_ratings ........................ [RUN]
[0m08:26:07.714286 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_title.d20f00c292, now test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04)
[0m08:26:07.715044 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:26:07.722294 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m08:26:07.732771 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (compile): 08:26:07.715864 => 08:26:07.732303
[0m08:26:07.733586 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:26:07.737972 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m08:26:07.748126 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:26:07.748906 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"
[0m08:26:07.749527 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select total_ratings
from `workspace`.`movielens_volume`.`movie_enriched`
where total_ratings is null



      
    ) dbt_internal_test
[0m08:26:07.750198 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:26:08.814735 [debug] [Thread-2  ]: SQL status: OK in 1.059999942779541 seconds
[0m08:26:08.819593 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04 (execute): 08:26:07.734140 => 08:26:08.819168
[0m08:26:08.820552 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: ROLLBACK
[0m08:26:08.821282 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:26:08.822131 [debug] [Thread-2  ]: On test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04: Close
[0m08:26:08.943624 [info ] [Thread-2  ]: 6 of 8 PASS not_null_movie_enriched_total_ratings .............................. [[32mPASS[0m in 1.23s]
[0m08:26:08.945093 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04
[0m08:26:08.945954 [debug] [Thread-2  ]: Began running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:26:08.946765 [info ] [Thread-2  ]: 7 of 8 START test relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [RUN]
[0m08:26:08.948303 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.not_null_movie_enriched_total_ratings.2edde44e04, now test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1)
[0m08:26:08.949038 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:26:08.968056 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:26:08.978573 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (compile): 08:26:08.949589 => 08:26:08.978092
[0m08:26:08.979369 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:26:08.983565 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:26:08.993625 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:26:08.994324 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"
[0m08:26:08.994937 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select movie_id as from_field
    from `workspace`.`movielens_volume`.`movie_enriched`
    where movie_id is not null
),

parent as (
    select movie_id as to_field
    from `workspace`.`movielens_volume`.`silver_movies`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m08:26:08.995588 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:26:09.636491 [debug] [Thread-2  ]: SQL status: OK in 0.6399999856948853 seconds
[0m08:26:09.641232 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1 (execute): 08:26:08.979910 => 08:26:09.640868
[0m08:26:09.642092 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: ROLLBACK
[0m08:26:09.642833 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:26:09.643475 [debug] [Thread-2  ]: On test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1: Close
[0m08:26:09.759864 [info ] [Thread-2  ]: 7 of 8 PASS relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_  [[32mPASS[0m in 0.81s]
[0m08:26:09.761358 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1
[0m08:26:09.762358 [debug] [Thread-2  ]: Began running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:26:09.763092 [info ] [Thread-2  ]: 8 of 8 START test unique_movie_enriched_movie_id ............................... [RUN]
[0m08:26:09.764387 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly test.my_dbt_project.relationships_movie_enriched_movie_id__movie_id__ref_silver_movies_.633cd9acf1, now test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a)
[0m08:26:09.765134 [debug] [Thread-2  ]: Began compiling node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:26:09.775139 [debug] [Thread-2  ]: Writing injected SQL for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m08:26:09.785390 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (compile): 08:26:09.765656 => 08:26:09.784909
[0m08:26:09.786220 [debug] [Thread-2  ]: Began executing node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:26:09.790667 [debug] [Thread-2  ]: Writing runtime sql for node "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m08:26:09.801006 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m08:26:09.801760 [debug] [Thread-2  ]: Using databricks connection "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"
[0m08:26:09.802466 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.2", "databricks_sql_connector_version": "4.1.3", "profile_name": "my_dbt_project", "target_name": "dev", "node_id": "test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from `workspace`.`movielens_volume`.`movie_enriched`
where movie_id is not null
group by movie_id
having count(*) > 1



      
    ) dbt_internal_test
[0m08:26:09.803430 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m08:26:10.725342 [debug] [Thread-2  ]: SQL status: OK in 0.9200000166893005 seconds
[0m08:26:10.729719 [debug] [Thread-2  ]: Timing info for test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a (execute): 08:26:09.786801 => 08:26:10.729362
[0m08:26:10.730697 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: ROLLBACK
[0m08:26:10.731820 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m08:26:10.732514 [debug] [Thread-2  ]: On test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a: Close
[0m08:26:10.847346 [info ] [Thread-2  ]: 8 of 8 PASS unique_movie_enriched_movie_id ..................................... [[32mPASS[0m in 1.08s]
[0m08:26:10.848772 [debug] [Thread-2  ]: Finished running node test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a
[0m08:26:10.851174 [debug] [MainThread]: On master: ROLLBACK
[0m08:26:10.851801 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:26:11.236016 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:26:11.236868 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:26:11.237568 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:26:11.238286 [debug] [MainThread]: On master: ROLLBACK
[0m08:26:11.239138 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:26:11.239826 [debug] [MainThread]: On master: Close
[0m08:26:11.364243 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:26:11.365082 [debug] [MainThread]: Connection 'test.my_dbt_project.unique_movie_enriched_movie_id.8f605fb07a' was properly closed.
[0m08:26:11.367773 [info ] [MainThread]: 
[0m08:26:11.369183 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 9.89 seconds (9.89s).
[0m08:26:11.372226 [debug] [MainThread]: Command end result
[0m08:26:11.404871 [info ] [MainThread]: 
[0m08:26:11.405795 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:26:11.406523 [info ] [MainThread]: 
[0m08:26:11.407390 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8
[0m08:26:11.408543 [debug] [MainThread]: Command `dbt test` succeeded at 08:26:11.408394 after 12.50 seconds
[0m08:26:11.409171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2a2c07550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe275ed4850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe274b4ec10>]}
[0m08:26:11.409801 [debug] [MainThread]: Flushing usage events
